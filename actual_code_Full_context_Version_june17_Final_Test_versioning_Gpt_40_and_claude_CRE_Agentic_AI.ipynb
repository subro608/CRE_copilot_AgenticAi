{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IX7KhzaJbF8-",
        "outputId": "6f4c7310-fc23-4da0-affd-3c8d0ec577a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.54.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.54.0-py3-none-any.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.54.0\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.65)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.4.8 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "!pip install anthropic\n",
        "!pip install langgraph langchain langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgmo4iqobHNJ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import glob\n",
        "import os\n",
        "from anthropic import Anthropic\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import traceback\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "import numpy as np\n",
        "import logging\n",
        "import json\n",
        "from typing import List, Optional, Dict, Any, Union\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xJgdVBugREU"
      },
      "outputs": [],
      "source": [
        "def smart_dataframe_compression(df, max_tokens=8000, max_rows=None):\n",
        "    \"\"\"\n",
        "    🚀 SMART COMPRESSION: Convert dataframe to string, remove empty parts, and trim intelligently\n",
        "\n",
        "    Args:\n",
        "        df: Pandas DataFrame\n",
        "        max_tokens: Maximum tokens to aim for (default 8000)\n",
        "        max_rows: Maximum rows to include (auto-calculated if None)\n",
        "\n",
        "    Returns:\n",
        "        dict with compressed data and metadata\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return {\n",
        "            \"compressed_data\": \"Empty dataframe\",\n",
        "            \"metadata\": {\"original_shape\": (0, 0), \"compression_ratio\": 0, \"method\": \"empty\"}\n",
        "        }\n",
        "\n",
        "    original_shape = df.shape\n",
        "\n",
        "    # Step 1: Remove completely empty rows and columns\n",
        "    cleaned_df = df.dropna(how='all').dropna(axis=1, how='all')\n",
        "\n",
        "    # Step 2: Fill NaN values with empty string for better compression\n",
        "    cleaned_df = cleaned_df.fillna('')\n",
        "\n",
        "    # Step 3: Remove columns that are mostly empty (>80% empty)\n",
        "    mostly_empty_threshold = 0.8\n",
        "    for col in cleaned_df.columns:\n",
        "        empty_ratio = (cleaned_df[col] == '').sum() / len(cleaned_df)\n",
        "        if empty_ratio > mostly_empty_threshold:\n",
        "            cleaned_df = cleaned_df.drop(columns=[col])\n",
        "\n",
        "    # Step 4: Truncate long cell values to prevent individual cells from bloating\n",
        "    def truncate_cell_value(val, max_chars=50):\n",
        "        if isinstance(val, str) and len(val) > max_chars:\n",
        "            return val[:max_chars] + \"...\"\n",
        "        return val\n",
        "\n",
        "    # Apply truncation to string columns\n",
        "    for col in cleaned_df.columns:\n",
        "        if cleaned_df[col].dtype == 'object':\n",
        "            cleaned_df[col] = cleaned_df[col].apply(lambda x: truncate_cell_value(str(x)))\n",
        "\n",
        "    # Step 5: Determine optimal number of rows based on token limit\n",
        "    if max_rows is None:\n",
        "        # Estimate tokens per row and calculate max rows\n",
        "        sample_rows = min(3, len(cleaned_df))\n",
        "        if sample_rows > 0:\n",
        "            sample_string = cleaned_df.head(sample_rows).to_string(index=False)\n",
        "            tokens_per_row = len(sample_string) // 4 // sample_rows  # Rough estimate\n",
        "            max_rows = min(len(cleaned_df), max_tokens // max(tokens_per_row, 10))\n",
        "        else:\n",
        "            max_rows = 0\n",
        "\n",
        "    # Step 6: Select representative rows (not just first rows)\n",
        "    if len(cleaned_df) <= max_rows:\n",
        "        final_df = cleaned_df\n",
        "        selection_method = \"all_rows\"\n",
        "    else:\n",
        "        # Smart row selection: first rows + some middle rows + last rows\n",
        "        first_rows = max(1, max_rows // 3)\n",
        "        last_rows = max(1, max_rows // 3)\n",
        "        middle_rows = max_rows - first_rows - last_rows\n",
        "\n",
        "        rows_selected = []\n",
        "\n",
        "        # First rows\n",
        "        rows_selected.extend(list(range(first_rows)))\n",
        "\n",
        "        # Middle rows (evenly distributed)\n",
        "        if middle_rows > 0 and len(cleaned_df) > first_rows + last_rows:\n",
        "            middle_start = first_rows\n",
        "            middle_end = len(cleaned_df) - last_rows\n",
        "            if middle_end > middle_start:\n",
        "                middle_indices = np.linspace(middle_start, middle_end-1, middle_rows, dtype=int)\n",
        "                rows_selected.extend(middle_indices)\n",
        "\n",
        "        # Last rows\n",
        "        if last_rows > 0:\n",
        "            last_indices = list(range(len(cleaned_df) - last_rows, len(cleaned_df)))\n",
        "            rows_selected.extend(last_indices)\n",
        "\n",
        "        # Remove duplicates and sort\n",
        "        rows_selected = sorted(list(set(rows_selected)))\n",
        "        final_df = cleaned_df.iloc[rows_selected]\n",
        "        selection_method = f\"smart_selection_{len(rows_selected)}_rows\"\n",
        "\n",
        "    # Step 7: Convert to string and clean up formatting\n",
        "    df_string = final_df.to_string(index=False, max_cols=None)\n",
        "\n",
        "    # Step 8: Clean up the string - remove excessive whitespace\n",
        "    # Remove multiple spaces\n",
        "    df_string = re.sub(r' +', ' ', df_string)\n",
        "\n",
        "    # Remove empty lines\n",
        "    lines = [line.strip() for line in df_string.split('\\n') if line.strip()]\n",
        "    df_string = '\\n'.join(lines)\n",
        "\n",
        "    # Step 9: Final trimming if still too long\n",
        "    estimated_tokens = len(df_string) // 4\n",
        "    if estimated_tokens > max_tokens:\n",
        "        # Trim to fit token limit\n",
        "        char_limit = max_tokens * 4\n",
        "        df_string = df_string[:char_limit] + \"\\n... [truncated to fit context limit]\"\n",
        "        final_method = selection_method + \"_and_truncated\"\n",
        "    else:\n",
        "        final_method = selection_method\n",
        "\n",
        "    # Calculate compression ratio\n",
        "    original_size = len(df.to_string()) if not df.empty else 1\n",
        "    compressed_size = len(df_string)\n",
        "    compression_ratio = compressed_size / original_size\n",
        "\n",
        "    return {\n",
        "        \"compressed_data\": df_string,\n",
        "        \"metadata\": {\n",
        "            \"original_shape\": original_shape,\n",
        "            \"cleaned_shape\": cleaned_df.shape,\n",
        "            \"final_shape\": final_df.shape,\n",
        "            \"compression_ratio\": compression_ratio,\n",
        "            \"estimated_tokens\": len(df_string) // 4,\n",
        "            \"method\": final_method,\n",
        "            \"rows_selected\": len(final_df),\n",
        "            \"columns_kept\": len(final_df.columns),\n",
        "            \"empty_rows_removed\": original_shape[0] - cleaned_df.shape[0],\n",
        "            \"empty_columns_removed\": original_shape[1] - cleaned_df.shape[1]\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfT99HtjwLXf"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "class ConversationHistoryManager:\n",
        "    def __init__(self, max_context_tokens=20000, compression_threshold=8000):  # ✅ FIXED: Increased thresholds\n",
        "        self.max_context_tokens = max_context_tokens\n",
        "        self.compression_threshold = compression_threshold\n",
        "        self.encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "    def estimate_tokens(self, text):\n",
        "        \"\"\"Estimate token count for text\"\"\"\n",
        "        return len(self.encoding.encode(str(text)))\n",
        "\n",
        "    def get_conversation_size(self, messages):\n",
        "        \"\"\"Calculate total tokens in conversation history\"\"\"\n",
        "        total_tokens = 0\n",
        "        for msg in messages:\n",
        "            total_tokens += self.estimate_tokens(msg.get('content', ''))\n",
        "        return total_tokens\n",
        "\n",
        "    def compress_conversation_chunk(self, messages_chunk, openai_client):\n",
        "        \"\"\"Use GPT-4 to compress a chunk of conversation history\"\"\"\n",
        "        conversation_text = \"\"\n",
        "        for msg in messages_chunk:\n",
        "            role = msg.get('role', 'unknown')\n",
        "            content = msg.get('content', '')\n",
        "            conversation_text += f\"\\n[{role.upper()}]: {content}\\n\"\n",
        "\n",
        "        compression_prompt = f\"\"\"\n",
        "        You are an expert at summarizing technical conversations about commercial real estate rent roll analysis.\n",
        "\n",
        "        Please compress the following conversation history into a concise paragraph that preserves:\n",
        "        1. Key decisions made by the AI system\n",
        "        2. Important data analysis findings\n",
        "        3. User requests and their outcomes\n",
        "        4. Any dataframe modifications or versions created\n",
        "        5. Business insights discovered\n",
        "        6. Technical approaches used (GPT-4 vs Claude, etc.)\n",
        "\n",
        "        Original conversation ({self.estimate_tokens(conversation_text)} tokens):\n",
        "        {conversation_text}\n",
        "\n",
        "        Compress this into a single, information-dense paragraph of approximately 50-80 words that captures the essence and key outcomes.\n",
        "\n",
        "        Compressed summary:\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",  # ✅ Using gpt-4o instead of gpt-4.1 to avoid rate limits\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at compressing technical conversations while preserving critical information.\"},\n",
        "                    {\"role\": \"user\", \"content\": compression_prompt}\n",
        "                ],\n",
        "                max_tokens=100,\n",
        "                temperature=0.2\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Compression failed: {str(e)}\")\n",
        "            return f\"[Compression failed: {str(e)}] Conversation chunk with {len(messages_chunk)} messages covering rent roll analysis.\"\n",
        "\n",
        "    def compress_history_if_needed(self, messages, openai_client):\n",
        "        \"\"\"Compress conversation history if it exceeds limits\"\"\"\n",
        "        current_size = self.get_conversation_size(messages)\n",
        "        print(f\"🔧 Conversation size: {current_size} tokens\")\n",
        "\n",
        "        # ✅ FIXED: Emergency truncation for very large conversations\n",
        "        if current_size > 28000:  # Near API limit\n",
        "            print(f\"🚨 Emergency truncation: {current_size} -> keeping last 8 messages\")\n",
        "            return messages[-8:] if len(messages) > 8 else messages\n",
        "\n",
        "        # ✅ FIXED: Simple truncation if over compression threshold\n",
        "        if current_size > self.compression_threshold:\n",
        "            print(f\"🔄 Applying compression/truncation...\")\n",
        "\n",
        "            # Try compression first\n",
        "            try:\n",
        "                compressed = self._compress_with_fallback(messages, openai_client)\n",
        "                final_size = self.get_conversation_size(compressed)\n",
        "                print(f\"✅ Compressed: {current_size} -> {final_size} tokens\")\n",
        "                return compressed\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Compression failed: {e}, using truncation\")\n",
        "                # Fallback to simple truncation\n",
        "                keep_messages = max(6, int(len(messages) * 0.4))  # Keep 40% of messages\n",
        "                truncated = messages[-keep_messages:]\n",
        "                final_size = self.get_conversation_size(truncated)\n",
        "                print(f\"✂️ Truncated: {current_size} -> {final_size} tokens\")\n",
        "                return truncated\n",
        "\n",
        "        # No compression needed\n",
        "        return messages\n",
        "\n",
        "    def _compress_with_fallback(self, messages, openai_client):\n",
        "        \"\"\"Helper method for compression with multiple fallback strategies\"\"\"\n",
        "        current_size = self.get_conversation_size(messages)\n",
        "\n",
        "        # Strategy 1: Keep recent messages, compress older ones\n",
        "        if len(messages) > 10:\n",
        "            recent_count = min(8, len(messages) // 2)  # Keep half, but at least 8\n",
        "            recent_messages = messages[-recent_count:]\n",
        "            older_messages = messages[:-recent_count]\n",
        "\n",
        "            compressed_older = []\n",
        "            if older_messages:\n",
        "                # Compress in chunks\n",
        "                chunk_size = max(3, len(older_messages) // 3)  # Smaller chunks for better compression\n",
        "                for i in range(0, len(older_messages), chunk_size):\n",
        "                    chunk = older_messages[i:i+chunk_size]\n",
        "                    try:\n",
        "                        compressed_summary = self.compress_conversation_chunk(chunk, openai_client)\n",
        "                        summary_message = {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": f\"[COMPRESSED HISTORY]: {compressed_summary}\"\n",
        "                        }\n",
        "                        compressed_older.append(summary_message)\n",
        "                    except Exception as e:\n",
        "                        print(f\"⚠️ Chunk compression failed: {e}\")\n",
        "                        # Skip this chunk if compression fails\n",
        "                        continue\n",
        "\n",
        "            result = compressed_older + recent_messages\n",
        "\n",
        "            # Check if compression was effective\n",
        "            new_size = self.get_conversation_size(result)\n",
        "            if new_size < current_size * 0.8:  # At least 20% reduction\n",
        "                return result\n",
        "\n",
        "        # Strategy 2: Aggressive truncation if compression didn't work well\n",
        "        keep_messages = max(5, len(messages) // 3)  # Keep 1/3 of messages\n",
        "        return messages[-keep_messages:]\n",
        "\n",
        "# ✅ FIXED: Updated global conversation manager with better thresholds\n",
        "conversation_manager = ConversationHistoryManager(\n",
        "    max_context_tokens=20000,     # Increased from 4000\n",
        "    compression_threshold=8000    # Increased from 2000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMymkJcBcBB5"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('rent_roll_analyzer')\n",
        "\n",
        "# Global variables and API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yes16m2AcCzI"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Optional, Union, Dict, Any\n",
        "\n",
        "# Define the state as a TypedDict\n",
        "class AgentState(TypedDict, total=False):\n",
        "    messages: List[Dict[str, str]]\n",
        "    df: Optional[pd.DataFrame]\n",
        "    issues: List[str]\n",
        "    execution_plan: Optional[str]\n",
        "    needs_clarification: bool\n",
        "    clarification_question: Optional[str]\n",
        "    generate_code: bool\n",
        "    code_execution_results: Optional[str]\n",
        "    final_response: Optional[str]\n",
        "    anthropic_client: Optional[Any]  # For Claude API\n",
        "    openai_client: Optional[Any]     # For OpenAI API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlFG6YSrcEUU"
      },
      "outputs": [],
      "source": [
        "def read_rent_roll_simple(file_path):\n",
        "    \"\"\"\n",
        "    Improved function to read rent roll Excel files that handles special formatting\n",
        "    commonly found in commercial real estate rent roll sheets.\n",
        "    \"\"\"\n",
        "    # Read the raw Excel file with no header\n",
        "    df = pd.read_excel(file_path, header=None)\n",
        "\n",
        "    # Find the row containing the column headers\n",
        "    header_row = None\n",
        "    for i, row in df.iterrows():\n",
        "        if row.iloc[0] == \"Current\":\n",
        "            header_row = i + 1  # Headers are in the row after \"Current\"\n",
        "            break\n",
        "\n",
        "    if header_row is None:\n",
        "        logger.warning(\"Could not find header row with 'Current' marker. Falling back to standard loading.\")\n",
        "        return pd.read_excel(file_path)\n",
        "\n",
        "    # Get the headers\n",
        "    headers = []\n",
        "    for val in df.iloc[header_row]:\n",
        "        if pd.isna(val):\n",
        "            headers.append(\"NaN\")  # Use \"NaN\" for empty header cells\n",
        "        else:\n",
        "            headers.append(str(val))\n",
        "\n",
        "    # Create a new dataframe starting after the header row\n",
        "    data_rows = df.iloc[(header_row+1):].values\n",
        "\n",
        "    # Create a new dataframe with the extracted headers\n",
        "    result_df = pd.DataFrame(data_rows, columns=headers)\n",
        "\n",
        "    logger.info(f\"Successfully loaded rent roll with {len(result_df)} rows using specialized loader\")\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwQXfnG3cGOW"
      },
      "outputs": [],
      "source": [
        "def analyze_rent_roll_gpt(file_path, api_key):\n",
        "    \"\"\"\n",
        "    Analyzes a CRE rent roll Excel file by sending the data rows to GPT-4.\n",
        "    \"\"\"\n",
        "    # Load the rent roll\n",
        "    try:\n",
        "        df = read_rent_roll_simple(file_path)\n",
        "        logger.info(\"File loaded successfully for GPT analysis.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {e}\")\n",
        "        return []\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Convert the DataFrame to CSV string format\n",
        "    csv_data = df.to_csv(index=False)\n",
        "    logger.info(f\"Converted DataFrame to CSV with {len(df)} rows and {len(df.columns)} columns\")\n",
        "\n",
        "    # Enhance the system prompt to focus on general rent roll issues\n",
        "    system_prompt = \"\"\"\n",
        "    You are a Commercial Real Estate rent roll expert specializing in identifying data quality, formatting, and consistency issues.\n",
        "\n",
        "    When analyzing any CRE rent roll, rigorously check for these common categories of issues:\n",
        "\n",
        "    1. DUPLICATE OR REDUNDANT ENTRIES: Look for any repeated charges, fees, or line items\n",
        "    2. INCONSISTENT TERMINOLOGY: Identify any unclear, non-standard, or ambiguous descriptions\n",
        "    3. DATE ANOMALIES: Flag any suspicious or illogical date patterns across move-in, lease start/end\n",
        "    4. RENT DISCREPANCIES: Identify deviations between market rent values and actual charged amounts\n",
        "    5. CALCULATION INCONSISTENCIES: Check if component charges properly sum to totals\n",
        "    6. EXCEL ARTIFACTS: Identify any visible formulas, function calls, or spreadsheet mechanics\n",
        "    7. FORMATTING IRREGULARITIES: Notice inconsistent data entry patterns or splitting of information\n",
        "    8. BALANCE ANOMALIES: Identify unusual balances, especially negative values\n",
        "    9. OCCUPANCY MISMATCHES: Look for occupied units with zero rent or vacant units with charges\n",
        "    10. UNIT IDENTIFICATION PATTERNS: Check for inconsistencies in unit numbering or identification\n",
        "\n",
        "    Be extremely thorough and specific in your analysis. Report ALL issues you find, regardless of how minor they may seem.\n",
        "    DO NOT return \"No issues detected\" unless you've comprehensively analyzed the data for each category above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use a simplified prompt focused on analyzing the raw CSV data\n",
        "    prompt = (\n",
        "        f\"Please analyze this Commercial Real Estate rent roll data in CSV format and identify ALL potential issues \"\n",
        "        f\"that could affect data quality, accuracy, or decision-making.\\n\\n{csv_data}\\n\\n\"\n",
        "\n",
        "        f\"Based on your expertise in CRE rent rolls, provide a numbered list of ALL issues you can identify, including but not limited to:\\n\\n\"\n",
        "\n",
        "        f\"- Any duplicate or redundant charges\\n\"\n",
        "        f\"- Unclear, non-standard, or inconsistent descriptions\\n\"\n",
        "        f\"- Suspicious or illogical date patterns\\n\"\n",
        "        f\"- Inconsistencies between market rent and actual rent values\\n\"\n",
        "        f\"- Calculation errors where components don't match totals\\n\"\n",
        "        f\"- Spreadsheet artifacts like visible formulas\\n\"\n",
        "        f\"- Inconsistent data entry patterns\\n\"\n",
        "        f\"- Unusual balance values\\n\"\n",
        "        f\"- Occupancy status mismatches\\n\"\n",
        "        f\"- Inconsistent unit numbering or identification\\n\\n\"\n",
        "\n",
        "        f\"IMPORTANT: For each issue found, please reference the specific unit(s) affected and explain why it's problematic. \"\n",
        "        f\"Be comprehensive - rent roll accuracy is critical for CRE investment and property management decisions.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Sending request to GPT-4 for analysis...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=2000,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        response_text = response.choices[0].message.content\n",
        "        logger.info(\"Received response from GPT-4.\")\n",
        "\n",
        "        # Simple parsing of the response - split by numbered items\n",
        "        lines = response_text.split('\\n')\n",
        "        issues = []\n",
        "        current_issue = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            # If it's a new numbered item\n",
        "            if line.strip() and line[0].isdigit() and '. ' in line[:5]:\n",
        "                # If we were building a previous issue, add it\n",
        "                if current_issue:\n",
        "                    issues.append(current_issue.strip())\n",
        "                current_issue = line.strip()\n",
        "            elif line.strip() and current_issue:\n",
        "                # Continue building the current issue\n",
        "                current_issue += \" \" + line.strip()\n",
        "\n",
        "        # Add the last issue if there is one\n",
        "        if current_issue:\n",
        "            issues.append(current_issue.strip())\n",
        "\n",
        "        if not issues:\n",
        "            issues.append(\"No issues detected by GPT-4.\")\n",
        "\n",
        "        logger.info(f\"Identified {len(issues)} issues in the rent roll\")\n",
        "        return issues\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error calling GPT-4 for analysis: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return [\"Failed to analyze rent roll due to API error.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05TftxjAcKhx"
      },
      "outputs": [],
      "source": [
        "def determine_action(state):\n",
        "    \"\"\"Decide whether to answer directly, ask for clarification, or generate code.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    user_message = messages[-1][\"content\"] if messages[-1][\"role\"] == \"user\" else \"\"\n",
        "    df = state[\"df\"]\n",
        "\n",
        "    # Create OpenAI client for this function call\n",
        "    client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "    # Get column information for context\n",
        "    if df is not None:\n",
        "        try:\n",
        "            # Safer way to get column data types\n",
        "            column_info = []\n",
        "            for col in df.columns:\n",
        "                try:\n",
        "                    dtype_str = str(df[col].dtype)  # Convert dtype to string directly\n",
        "                    column_info.append(f\"- {col}: {dtype_str}\")\n",
        "                except:\n",
        "                    column_info.append(f\"- {col}: unknown type\")\n",
        "            column_info_str = \"\\n\".join(column_info)\n",
        "            df_preview = df.head(3).to_string()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting column info: {e}\")\n",
        "            column_info_str = \"Error retrieving column information\"\n",
        "            df_preview = \"Error retrieving data preview\"\n",
        "    else:\n",
        "        column_info_str = \"No dataframe loaded\"\n",
        "        df_preview = \"No data available\"\n",
        "\n",
        "    # Use GPT-4 to analyze the query and determine the best action\n",
        "    prompt = f\"\"\"\n",
        "    User query: {user_message}\n",
        "\n",
        "    Dataframe information:\n",
        "    - Rows: {len(df) if df is not None else 'No data loaded'}\n",
        "    - Columns: {column_info_str}\n",
        "\n",
        "    Data preview:\n",
        "    {df_preview}\n",
        "\n",
        "    Analyze the user query and determine the most appropriate action:\n",
        "    1. If the query is ambiguous or lacks specificity, choose \"ask_clarification\"\n",
        "    2. If the query can be answered with a simple explanation without analysis, choose \"text_response\"\n",
        "    3. If the query requires data analysis, calculations, or visualizations, choose \"generate_code\"\n",
        "\n",
        "    Respond with a JSON object containing:\n",
        "    {{\"action\": \"ask_clarification\" | \"text_response\" | \"generate_code\", \"reason\": \"brief explanation\"}}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a decision-making agent for a rent roll analysis system. Output ONLY a JSON object with the determined action and reason.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        response_text = response.choices[0].message.content\n",
        "\n",
        "        # Extract JSON from the response\n",
        "        json_match = re.search(r'{.*}', response_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            action_data = json.loads(json_match.group(0))\n",
        "            action = action_data.get(\"action\", \"text_response\")\n",
        "        else:\n",
        "            # Default to text response if parsing fails\n",
        "            action = \"text_response\"\n",
        "\n",
        "        logger.info(f\"Determined action using GPT-4: {action}\")\n",
        "\n",
        "        # Create a new state dict with updated values\n",
        "        new_state = dict(state)  # Create a copy\n",
        "        new_state[\"needs_clarification\"] = action == \"ask_clarification\"\n",
        "        new_state[\"generate_code\"] = action == \"generate_code\"\n",
        "\n",
        "        return new_state\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in determine_action: {e}\")\n",
        "        # Default to text response on error\n",
        "        new_state = dict(state)\n",
        "        new_state[\"needs_clarification\"] = False\n",
        "        new_state[\"generate_code\"] = False\n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IDnMakrcOA4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ask_clarification(state: AgentState) -> Dict:\n",
        "    \"\"\"Generate a clarification question for the user using GPT-4.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    user_message = messages[-1][\"content\"] if messages[-1][\"role\"] == \"user\" else \"\"\n",
        "\n",
        "    # Create OpenAI client for this function call\n",
        "    client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"\"\"You are a commercial real estate rent roll analyst.\n",
        "                Generate a clear, specific clarification question to better understand\n",
        "                what the user is asking about their rent roll data.\"\"\"},\n",
        "                {\"role\": \"user\", \"content\": f\"My question is: {user_message}\"}\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        clarification_question = response.choices[0].message.content\n",
        "\n",
        "        # Create a new state dict with updated values\n",
        "        new_state = dict(state)\n",
        "        new_state[\"clarification_question\"] = clarification_question\n",
        "        new_state[\"final_response\"] = clarification_question\n",
        "\n",
        "        # Add the clarification question to the messages\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": clarification_question})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        logger.info(f\"Generated clarification question using GPT-4: {clarification_question[:50]}...\")\n",
        "        return new_state\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in ask_clarification: {e}\")\n",
        "        # Fallback to a generic clarification question\n",
        "        generic_question = \"Could you please clarify what specific aspect of the rent roll you'd like me to analyze?\"\n",
        "\n",
        "        new_state = dict(state)\n",
        "        new_state[\"clarification_question\"] = generic_question\n",
        "        new_state[\"final_response\"] = generic_question\n",
        "\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": generic_question})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-C3FXmrcQ8J"
      },
      "outputs": [],
      "source": [
        "def generate_text_response(state):\n",
        "    \"\"\"Generate a simple text response to the user query using GPT-4.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    df = state[\"df\"]\n",
        "    issues = state[\"issues\"]\n",
        "\n",
        "    # Create OpenAI client for this function call\n",
        "    client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "    # Prepare context for GPT-4\n",
        "    issues_text = \"\\n\".join([f\"- {issue}\" for issue in issues])\n",
        "\n",
        "    # Get column and data preview for context\n",
        "    if df is not None:\n",
        "        column_info = \", \".join(df.columns)\n",
        "        data_stats = []\n",
        "        for col in df.columns[:10]:  # Limit to first 10 columns to avoid token limits\n",
        "            try:\n",
        "                if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                    stat = f\"- {col}: min={df[col].min()}, max={df[col].max()}, mean={df[col].mean():.2f}, null={df[col].isna().sum()}\"\n",
        "                else:\n",
        "                    unique_vals = df[col].nunique()\n",
        "                    stat = f\"- {col}: unique values={unique_vals}, null={df[col].isna().sum()}\"\n",
        "                data_stats.append(stat)\n",
        "            except:\n",
        "                data_stats.append(f\"- {col}: [error calculating stats]\")\n",
        "        data_stats_str = \"\\n\".join(data_stats)\n",
        "        df_preview = df.head(3).to_string()\n",
        "    else:\n",
        "        column_info = \"No columns available\"\n",
        "        data_stats_str = \"No data statistics available\"\n",
        "        df_preview = \"No data preview available\"\n",
        "\n",
        "    system_prompt = f\"\"\"You are a commercial real estate rent roll analyst.\n",
        "    The rent roll data has {len(df) if df is not None else 0} rows and\n",
        "    {len(df.columns) if df is not None else 0} columns.\n",
        "\n",
        "    Column information: {column_info}\n",
        "\n",
        "    Data statistics:\n",
        "    {data_stats_str}\n",
        "\n",
        "    Data preview:\n",
        "    {df_preview}\n",
        "\n",
        "    Identified issues:\n",
        "    {issues_text}\n",
        "\n",
        "    Provide a concise, informative answer to the user's question.\n",
        "    Focus on being helpful and direct, with only 1-2 paragraphs.\n",
        "    Do not include code or detailed analysis unless absolutely necessary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract system message and filter other messages\n",
        "    filtered_messages = []\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] != \"system\":\n",
        "            filtered_messages.append(msg)\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                *filtered_messages\n",
        "            ],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        text_response = response.choices[0].message.content\n",
        "\n",
        "        # Create a new state dict with updated values\n",
        "        new_state = dict(state)\n",
        "        new_state[\"final_response\"] = text_response\n",
        "\n",
        "        # Add the response to the messages\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": text_response})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        logger.info(f\"Generated text response using GPT-4: {text_response[:50]}...\")\n",
        "        return new_state\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in generate_text_response: {e}\")\n",
        "        # Fallback to a generic response\n",
        "        fallback_response = \"I'm sorry, I'm having trouble analyzing your rent roll data right now. Could you try rephrasing your question?\"\n",
        "\n",
        "        new_state = dict(state)\n",
        "        new_state[\"final_response\"] = fallback_response\n",
        "\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": fallback_response})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiwl_X7iDPVC"
      },
      "outputs": [],
      "source": [
        "def trim_dataframe_output(output_text, max_rows=20, max_chars=None):\n",
        "    \"\"\"\n",
        "    Extremely simplified function that just returns the first 20 lines of output.\n",
        "\n",
        "    Args:\n",
        "        output_text: The text output\n",
        "        max_rows: Maximum number of rows to keep (default: 20)\n",
        "        max_chars: Not used, kept for compatibility\n",
        "\n",
        "    Returns:\n",
        "        Trimmed text showing only top rows\n",
        "    \"\"\"\n",
        "    lines = output_text.split('\\n')\n",
        "\n",
        "    if len(lines) <= max_rows:\n",
        "        return output_text\n",
        "\n",
        "    trimmed_lines = lines[:max_rows]\n",
        "    trimmed_lines.append(f\"... [output truncated, showing first {max_rows} lines only] ...\")\n",
        "\n",
        "    return '\\n'.join(trimmed_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeQetycQblZd"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datetime import datetime\n",
        "def save_dataframe_version(df, operation_description=\"\"):\n",
        "    \"\"\"Save the current state of the dataframe as both CSV and Excel files.\n",
        "\n",
        "    Args:\n",
        "        df: The dataframe to save\n",
        "        operation_description: A string describing what operation was performed\n",
        "\n",
        "    Returns:\n",
        "        version_name: The name of the version that was saved\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create versions directory if it doesn't exist\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "    os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "    # Generate version name with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    version_name = f\"v_{timestamp}\"\n",
        "\n",
        "    # Create filenames for both CSV and Excel\n",
        "    csv_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.csv\")\n",
        "    excel_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.xlsx\")\n",
        "\n",
        "    # Save as CSV\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    # Save as Excel\n",
        "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "\n",
        "    # Add version metadata to the registry\n",
        "    if 'app_state' in globals():\n",
        "        version_info = {\n",
        "            'name': version_name,\n",
        "            'description': operation_description,\n",
        "            'timestamp': timestamp,\n",
        "            'filename': csv_filename,  # Keep CSV as primary for backward compatibility\n",
        "            'excel_filename': excel_filename,  # Add Excel filename\n",
        "            'is_original': len(app_state[\"df_versions\"]) == 0  # First one is original\n",
        "        }\n",
        "        app_state[\"df_versions\"].append(version_info)\n",
        "\n",
        "    print(f\"✓ Saved dataframe version {version_name}: {operation_description}\")\n",
        "    print(f\"  - CSV: {csv_filename}\")\n",
        "    print(f\"  - Excel: {excel_filename}\")\n",
        "\n",
        "    # Return the version name for reference\n",
        "    return version_name\n",
        "\n",
        "def get_versions_info_for_prompt():\n",
        "    \"\"\"Generate version information for the Claude prompt.\"\"\"\n",
        "    if not app_state[\"df_versions\"]:\n",
        "        return \"No versions available yet.\"\n",
        "\n",
        "    # Find the original version\n",
        "    original = next((v for v in app_state[\"df_versions\"] if v.get('is_original')), app_state[\"df_versions\"][0])\n",
        "\n",
        "    # Get the latest version\n",
        "    latest = app_state[\"df_versions\"][-1]\n",
        "\n",
        "    # Format all versions\n",
        "    all_versions = []\n",
        "    for i, version in enumerate(app_state[\"df_versions\"]):\n",
        "        status = []\n",
        "        if version == original:\n",
        "            status.append(\"ORIGINAL\")\n",
        "        if version == latest:\n",
        "            status.append(\"LATEST\")\n",
        "\n",
        "        status_str = f\" ({', '.join(status)})\" if status else \"\"\n",
        "        all_versions.append(f\"{i+1}. {version['name']}{status_str}: {version['description']}\")\n",
        "\n",
        "    versions_text = \"\\n\".join(all_versions)\n",
        "\n",
        "    return f\"\"\"\n",
        "DATAFRAME VERSION HISTORY:\n",
        "{versions_text}\n",
        "\n",
        "Original version: {original['name']}\n",
        "Latest version: {latest['name']}\n",
        "Total versions: {len(app_state[\"df_versions\"])}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_9Jh6NHnONJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te_nwcN4xbwJ",
        "outputId": "56db4cfb-ea47-418b-fe1f-a348fe1cd4ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ NEW VALIDATION FUNCTION WITH LOGGING AND SAMPLING LOADED!\n",
            "📝 This function will create logs in 'validation_logs' directory\n",
            "🎯 Token usage should be dramatically reduced with strategic sampling\n",
            "🔧 Run 'check_and_read_validation_logs()' after validation to see logs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import traceback\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "import re\n",
        "def validate_dataframe_changes_with_gpt4(original_df, new_df, user_query, executed_code=None):\n",
        "    \"\"\"\n",
        "    Use GPT-4.1 to validate if the dataframe changes align with user intent\n",
        "    Using strategic sampling: first 5 rows + middle 10 rows + last 5 rows\n",
        "    All inputs and outputs are logged to validation_logs directory\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create logging directory\n",
        "    logs_dir = \"validation_logs\"\n",
        "    os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "    # Create unique log file with timestamp\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]\n",
        "    log_file = os.path.join(logs_dir, f\"validation_{timestamp}.log\")\n",
        "\n",
        "    def log_to_file(content, section_title=\"\"):\n",
        "        \"\"\"Log content to file with proper formatting\"\"\"\n",
        "        try:\n",
        "            with open(log_file, 'a', encoding='utf-8') as f:\n",
        "                if section_title:\n",
        "                    f.write(f\"\\n{'='*60}\\n\")\n",
        "                    f.write(f\"{section_title}\\n\")\n",
        "                    f.write(f\"{'='*60}\\n\")\n",
        "                f.write(str(content))\n",
        "                f.write(f\"\\n\")\n",
        "                f.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to write to log file: {e}\")\n",
        "\n",
        "    # Log initial information\n",
        "    log_to_file(f\"VALIDATION SESSION STARTED\", \"VALIDATION LOG\")\n",
        "    log_to_file(f\"Timestamp: {datetime.now()}\")\n",
        "    log_to_file(f\"Log file: {log_file}\")\n",
        "\n",
        "    # Log input parameters\n",
        "    log_to_file(f\"User Query: {user_query}\", \"INPUT PARAMETERS\")\n",
        "    log_to_file(f\"Original DataFrame Shape: {original_df.shape}\")\n",
        "    log_to_file(f\"New DataFrame Shape: {new_df.shape}\")\n",
        "    log_to_file(f\"Executed Code: {executed_code if executed_code else 'None'}\")\n",
        "\n",
        "    print(f\"📝 Logging validation session to: {log_file}\")\n",
        "\n",
        "    client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "    def get_strategic_sample(df, max_sample_rows=20):\n",
        "        \"\"\"Get strategic sample: first 5, middle 10, last 5 rows\"\"\"\n",
        "        total_rows = len(df)\n",
        "\n",
        "        if total_rows <= max_sample_rows:\n",
        "            # If dataframe is small, return it as-is\n",
        "            return df, \"complete\", total_rows\n",
        "\n",
        "        # Get strategic samples\n",
        "        first_5 = df.head(5)\n",
        "        last_5 = df.tail(5)\n",
        "\n",
        "        # Calculate middle section\n",
        "        if total_rows > 10:  # Need at least 10 rows to have a meaningful middle\n",
        "            middle_start = max(5, total_rows // 2 - 5)  # Don't overlap with first 5\n",
        "            middle_end = min(total_rows - 5, total_rows // 2 + 5)  # Don't overlap with last 5\n",
        "\n",
        "            if middle_end > middle_start:\n",
        "                middle_10 = df.iloc[middle_start:middle_end]\n",
        "                # Combine all samples\n",
        "                sampled_df = pd.concat([first_5, middle_10, last_5], ignore_index=False)\n",
        "            else:\n",
        "                # If overlap would occur, just use first and last\n",
        "                sampled_df = pd.concat([first_5, last_5], ignore_index=False)\n",
        "        else:\n",
        "            # Very small dataframe, just use first and last\n",
        "            sampled_df = pd.concat([first_5, last_5], ignore_index=False)\n",
        "\n",
        "        # Remove any duplicate indices\n",
        "        sampled_df = sampled_df[~sampled_df.index.duplicated(keep='first')]\n",
        "\n",
        "        return sampled_df, \"sampled\", total_rows\n",
        "\n",
        "    # Get strategic samples of both dataframes\n",
        "    original_sample, orig_sample_type, orig_total_rows = get_strategic_sample(original_df)\n",
        "    new_sample, new_sample_type, new_total_rows = get_strategic_sample(new_df)\n",
        "\n",
        "    # Log sampling information\n",
        "    log_to_file(f\"Original DataFrame Sampling:\", \"SAMPLING INFORMATION\")\n",
        "    log_to_file(f\"  Total rows: {orig_total_rows}\")\n",
        "    log_to_file(f\"  Sample type: {orig_sample_type}\")\n",
        "    log_to_file(f\"  Sample size: {len(original_sample)}\")\n",
        "    log_to_file(f\"  Sample indices: {list(original_sample.index)}\")\n",
        "\n",
        "    log_to_file(f\"New DataFrame Sampling:\")\n",
        "    log_to_file(f\"  Total rows: {new_total_rows}\")\n",
        "    log_to_file(f\"  Sample type: {new_sample_type}\")\n",
        "    log_to_file(f\"  Sample size: {len(new_sample)}\")\n",
        "    log_to_file(f\"  Sample indices: {list(new_sample.index)}\")\n",
        "\n",
        "    # Prepare dataframe information for GPT-4.1 with samples\n",
        "    original_info = {\n",
        "        \"shape\": original_df.shape,\n",
        "        \"total_rows\": orig_total_rows,\n",
        "        \"sample_type\": orig_sample_type,\n",
        "        \"sample_size\": len(original_sample),\n",
        "        \"columns\": list(original_df.columns),\n",
        "        \"dtypes\": dict(original_df.dtypes.astype(str)),\n",
        "        \"sampled_data\": original_sample.to_string(max_rows=None, max_cols=None),\n",
        "        \"null_counts\": dict(original_df.isnull().sum())\n",
        "    }\n",
        "\n",
        "    new_info = {\n",
        "        \"shape\": new_df.shape,\n",
        "        \"total_rows\": new_total_rows,\n",
        "        \"sample_type\": new_sample_type,\n",
        "        \"sample_size\": len(new_sample),\n",
        "        \"columns\": list(new_df.columns),\n",
        "        \"dtypes\": dict(new_df.dtypes.astype(str)),\n",
        "        \"sampled_data\": new_sample.to_string(max_rows=None, max_cols=None),\n",
        "        \"null_counts\": dict(new_df.isnull().sum())\n",
        "    }\n",
        "\n",
        "    # Log original dataframe details\n",
        "    log_to_file(\"Original DataFrame Info:\", \"ORIGINAL DATAFRAME\")\n",
        "    log_to_file(f\"Shape: {original_info['shape']}\")\n",
        "    log_to_file(f\"Columns: {original_info['columns']}\")\n",
        "    log_to_file(f\"Data Types: {original_info['dtypes']}\")\n",
        "    log_to_file(f\"Null Counts: {original_info['null_counts']}\")\n",
        "    log_to_file(\"Sampled Data:\")\n",
        "    log_to_file(original_info['sampled_data'])\n",
        "\n",
        "    # Log new dataframe details\n",
        "    log_to_file(\"New DataFrame Info:\", \"NEW DATAFRAME\")\n",
        "    log_to_file(f\"Shape: {new_info['shape']}\")\n",
        "    log_to_file(f\"Columns: {new_info['columns']}\")\n",
        "    log_to_file(f\"Data Types: {new_info['dtypes']}\")\n",
        "    log_to_file(f\"Null Counts: {new_info['null_counts']}\")\n",
        "    log_to_file(\"Sampled Data:\")\n",
        "    log_to_file(new_info['sampled_data'])\n",
        "\n",
        "    # Create comprehensive validation prompt with sampling info\n",
        "    validation_prompt = f\"\"\"\n",
        "    You are an expert data analyst validator. Your job is to verify if dataframe changes correctly fulfill the user's request.\n",
        "\n",
        "    USER'S ORIGINAL REQUEST:\n",
        "    \"{user_query}\"\n",
        "\n",
        "    ORIGINAL DATAFRAME:\n",
        "    Shape: {original_info['shape']} (Total: {orig_total_rows} rows)\n",
        "    Sample Type: {orig_sample_type} ({original_info['sample_size']} rows shown)\n",
        "    Columns: {original_info['columns']}\n",
        "    Data Types: {original_info['dtypes']}\n",
        "    Null Counts: {original_info['null_counts']}\n",
        "\n",
        "    Strategic Sample Data (First 5 + Middle 10 + Last 5 rows):\n",
        "    {original_info['sampled_data']}\n",
        "\n",
        "    NEW/MODIFIED DATAFRAME:\n",
        "    Shape: {new_info['shape']} (Total: {new_total_rows} rows)\n",
        "    Sample Type: {new_sample_type} ({new_info['sample_size']} rows shown)\n",
        "    Columns: {new_info['columns']}\n",
        "    Data Types: {new_info['dtypes']}\n",
        "    Null Counts: {new_info['null_counts']}\n",
        "\n",
        "    Strategic Sample Data (First 5 + Middle 10 + Last 5 rows):\n",
        "    {new_info['sampled_data']}\n",
        "\n",
        "    {f'''\n",
        "    CODE THAT WAS EXECUTED:\n",
        "    {executed_code}\n",
        "    ''' if executed_code else ''}\n",
        "\n",
        "    VALIDATION TASK:\n",
        "    Analyze the dataframe changes based on the strategic samples provided and determine:\n",
        "\n",
        "    1. INTENT ALIGNMENT: Did the changes accomplish what the user requested?\n",
        "    2. DATA INTEGRITY: Are there any data corruption, loss, or invalid values in the samples?\n",
        "    3. LOGICAL CONSISTENCY: Do the changes make logical sense for this type of data?\n",
        "    4. SCHEMA VALIDATION: Are column types and structures still appropriate?\n",
        "    5. SAMPLE ANALYSIS: Based on the strategic sampling, do the changes appear consistent?\n",
        "\n",
        "    NOTE: You are analyzing strategic samples (first 5 + middle ~10 + last 5 rows) of the full dataset.\n",
        "    The samples should be representative of the overall changes.\n",
        "\n",
        "    Provide your analysis in this exact JSON format:\n",
        "    {{\n",
        "        \"validation_result\": \"PASS\" | \"FAIL\" | \"WARNING\",\n",
        "        \"intent_fulfilled\": true/false,\n",
        "        \"data_integrity_intact\": true/false,\n",
        "        \"logical_consistency\": true/false,\n",
        "        \"schema_appropriate\": true/false,\n",
        "        \"issues_found\": [\n",
        "            \"List any specific issues discovered in the samples\"\n",
        "        ],\n",
        "        \"positive_changes\": [\n",
        "            \"List what was done correctly\"\n",
        "        ],\n",
        "        \"summary\": \"Brief overall assessment based on strategic sampling\",\n",
        "        \"confidence\": \"high\" | \"medium\" | \"low\",\n",
        "        \"recommendation\": \"PROCEED\" | \"RETRY\" | \"MANUAL_REVIEW\",\n",
        "        \"sampling_note\": \"Analysis based on strategic samples from {orig_total_rows} and {new_total_rows} total rows\"\n",
        "    }}\n",
        "\n",
        "    Be thorough in examining the sample data provided, keeping in mind this represents the broader dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Log the full prompt being sent to GPT-4\n",
        "    log_to_file(validation_prompt, \"GPT-4 PROMPT\")\n",
        "\n",
        "    # Calculate approximate token count for logging\n",
        "    estimated_tokens = len(validation_prompt.split()) * 1.3\n",
        "    log_to_file(f\"Estimated token count: {estimated_tokens:.0f}\", \"TOKEN ESTIMATION\")\n",
        "\n",
        "    try:\n",
        "        # Log API call attempt\n",
        "        log_to_file(\"Sending request to GPT-4...\", \"API CALL\")\n",
        "\n",
        "        # Call GPT-4.1 for validation\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",  # Using gpt-4o for better availability\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a meticulous data validation expert. Analyze dataframe changes based on strategic samples and provide detailed validation results in JSON format.\"},\n",
        "                {\"role\": \"user\", \"content\": validation_prompt}\n",
        "            ],\n",
        "            max_tokens=2000,\n",
        "            temperature=0.1  # Low temperature for consistent validation\n",
        "        )\n",
        "\n",
        "        validation_response = response.choices[0].message.content\n",
        "\n",
        "        # Log GPT-4 response\n",
        "        log_to_file(validation_response, \"GPT-4 RESPONSE\")\n",
        "        log_to_file(f\"Response length: {len(validation_response)} characters\")\n",
        "\n",
        "        # Log token usage if available\n",
        "        if hasattr(response, 'usage') and response.usage:\n",
        "            log_to_file(f\"Actual token usage:\", \"TOKEN USAGE\")\n",
        "            log_to_file(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
        "            log_to_file(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
        "            log_to_file(f\"  Total tokens: {response.usage.total_tokens}\")\n",
        "\n",
        "        # Extract JSON from response\n",
        "        import json\n",
        "        import re\n",
        "        json_match = re.search(r'{.*}', validation_response, re.DOTALL)\n",
        "\n",
        "        if json_match:\n",
        "            log_to_file(\"JSON found in response, attempting to parse...\", \"JSON PARSING\")\n",
        "            try:\n",
        "                validation_result = json.loads(json_match.group(0))\n",
        "                # Add sampling metadata\n",
        "                validation_result[\"sampling_method\"] = \"strategic\"\n",
        "                validation_result[\"original_sample_info\"] = {\n",
        "                    \"total_rows\": orig_total_rows,\n",
        "                    \"sample_type\": orig_sample_type,\n",
        "                    \"sample_size\": len(original_sample)\n",
        "                }\n",
        "                validation_result[\"new_sample_info\"] = {\n",
        "                    \"total_rows\": new_total_rows,\n",
        "                    \"sample_type\": new_sample_type,\n",
        "                    \"sample_size\": len(new_sample)\n",
        "                }\n",
        "                validation_result[\"log_file\"] = log_file\n",
        "\n",
        "                # Log successful parsing and final result\n",
        "                log_to_file(\"JSON parsing successful!\", \"PARSING SUCCESS\")\n",
        "                log_to_file(json.dumps(validation_result, indent=2), \"FINAL VALIDATION RESULT\")\n",
        "                log_to_file(\"Validation completed successfully!\", \"SESSION END\")\n",
        "\n",
        "                return validation_result\n",
        "            except json.JSONDecodeError as e:\n",
        "                log_to_file(f\"JSON parsing failed: {str(e)}\", \"PARSING ERROR\")\n",
        "                fallback_result = {\n",
        "                    \"validation_result\": \"WARNING\",\n",
        "                    \"intent_fulfilled\": None,\n",
        "                    \"data_integrity_intact\": None,\n",
        "                    \"logical_consistency\": None,\n",
        "                    \"schema_appropriate\": None,\n",
        "                    \"issues_found\": [\"JSON parsing failed\"],\n",
        "                    \"positive_changes\": [],\n",
        "                    \"summary\": \"Validation response could not be parsed\",\n",
        "                    \"confidence\": \"low\",\n",
        "                    \"recommendation\": \"MANUAL_REVIEW\",\n",
        "                    \"sampling_method\": \"strategic\",\n",
        "                    \"log_file\": log_file,\n",
        "                    \"raw_response\": validation_response\n",
        "                }\n",
        "                log_to_file(json.dumps(fallback_result, indent=2), \"FALLBACK RESULT\")\n",
        "                return fallback_result\n",
        "        else:\n",
        "            log_to_file(\"No JSON found in GPT-4 response\", \"PARSING ERROR\")\n",
        "            fallback_result = {\n",
        "                \"validation_result\": \"WARNING\",\n",
        "                \"intent_fulfilled\": None,\n",
        "                \"data_integrity_intact\": None,\n",
        "                \"logical_consistency\": None,\n",
        "                \"schema_appropriate\": None,\n",
        "                \"issues_found\": [\"No JSON found in validation response\"],\n",
        "                \"positive_changes\": [],\n",
        "                \"summary\": \"Validation failed to produce structured output\",\n",
        "                \"confidence\": \"low\",\n",
        "                \"recommendation\": \"MANUAL_REVIEW\",\n",
        "                \"sampling_method\": \"strategic\",\n",
        "                \"log_file\": log_file,\n",
        "                \"raw_response\": validation_response\n",
        "            }\n",
        "            log_to_file(json.dumps(fallback_result, indent=2), \"FALLBACK RESULT\")\n",
        "            return fallback_result\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log the exception details\n",
        "        log_to_file(f\"Exception occurred: {str(e)}\", \"EXCEPTION ERROR\")\n",
        "        log_to_file(f\"Exception type: {type(e).__name__}\")\n",
        "\n",
        "        import traceback\n",
        "        log_to_file(f\"Full traceback:\\n{traceback.format_exc()}\", \"FULL TRACEBACK\")\n",
        "\n",
        "        # Fallback validation result on API failure\n",
        "        fallback_result = {\n",
        "            \"validation_result\": \"WARNING\",\n",
        "            \"intent_fulfilled\": None,\n",
        "            \"data_integrity_intact\": None,\n",
        "            \"logical_consistency\": None,\n",
        "            \"schema_appropriate\": None,\n",
        "            \"issues_found\": [f\"Validation API failed: {str(e)}\"],\n",
        "            \"positive_changes\": [],\n",
        "            \"summary\": \"Could not perform validation due to API error\",\n",
        "            \"confidence\": \"low\",\n",
        "            \"recommendation\": \"MANUAL_REVIEW\",\n",
        "            \"sampling_method\": \"strategic\",\n",
        "            \"log_file\": log_file,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "        log_to_file(json.dumps(fallback_result, indent=2), \"ERROR FALLBACK RESULT\")\n",
        "        log_to_file(\"Validation session ended with error\", \"SESSION END\")\n",
        "\n",
        "        return fallback_result\n",
        "\n",
        "\n",
        "# Function to check and read validation logs\n",
        "def check_and_read_validation_logs():\n",
        "    \"\"\"Check for validation log files and display their contents\"\"\"\n",
        "    import os\n",
        "    import glob\n",
        "    from datetime import datetime\n",
        "\n",
        "    logs_dir = \"validation_logs\"\n",
        "\n",
        "    print(f\"🔍 Checking for validation logs in: {os.path.abspath(logs_dir)}\")\n",
        "\n",
        "    if not os.path.exists(logs_dir):\n",
        "        print(f\"❌ Logs directory does not exist: {logs_dir}\")\n",
        "        return []\n",
        "\n",
        "    # Find all validation log files\n",
        "    log_pattern = os.path.join(logs_dir, \"validation_*.log\")\n",
        "    log_files = glob.glob(log_pattern)\n",
        "\n",
        "    if not log_files:\n",
        "        print(f\"❌ No validation log files found in {logs_dir}\")\n",
        "        return []\n",
        "\n",
        "    # Sort by modification time (newest first)\n",
        "    log_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
        "\n",
        "    print(f\"✅ Found {len(log_files)} validation log files:\")\n",
        "\n",
        "    for i, log_file in enumerate(log_files):\n",
        "        try:\n",
        "            stat = os.stat(log_file)\n",
        "            size = stat.st_size\n",
        "            mtime = datetime.fromtimestamp(stat.st_mtime)\n",
        "            print(f\"  {i+1}. {os.path.basename(log_file)} ({size:,} bytes, {mtime.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
        "        except Exception as e:\n",
        "            print(f\"  {i+1}. {os.path.basename(log_file)} (error reading stats: {e})\")\n",
        "\n",
        "    # Show content of most recent log\n",
        "    if log_files:\n",
        "        latest_log = log_files[0]\n",
        "        print(f\"\\n📖 Content of most recent log file: {os.path.basename(latest_log)}\")\n",
        "        print(\"─\" * 80)\n",
        "\n",
        "        try:\n",
        "            with open(latest_log, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Show full content if reasonable size, otherwise show truncated\n",
        "            if len(content) > 5000:\n",
        "                print(content[:2500])\n",
        "                print(f\"\\n... [TRUNCATED - showing first 2500 of {len(content)} characters] ...\")\n",
        "                print(content[-2500:])\n",
        "            else:\n",
        "                print(content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Could not read log file: {e}\")\n",
        "\n",
        "        print(\"─\" * 80)\n",
        "\n",
        "    return log_files\n",
        "\n",
        "\n",
        "print(\"✅ NEW VALIDATION FUNCTION WITH LOGGING AND SAMPLING LOADED!\")\n",
        "print(\"📝 This function will create logs in 'validation_logs' directory\")\n",
        "print(\"🎯 Token usage should be dramatically reduced with strategic sampling\")\n",
        "print(\"🔧 Run 'check_and_read_validation_logs()' after validation to see logs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXlh7N8SyoJr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def format_validation_message(validation_result):\n",
        "    \"\"\"\n",
        "    Format validation result into a user-friendly message\n",
        "    \"\"\"\n",
        "    result = validation_result.get(\"validation_result\", \"UNKNOWN\")\n",
        "\n",
        "    if result == \"PASS\":\n",
        "        emoji = \"✅\"\n",
        "        status = \"VALIDATION PASSED\"\n",
        "    elif result == \"FAIL\":\n",
        "        emoji = \"❌\"\n",
        "        status = \"VALIDATION FAILED\"\n",
        "    elif result == \"WARNING\":\n",
        "        emoji = \"⚠️\"\n",
        "        status = \"VALIDATION WARNING\"\n",
        "    else:\n",
        "        emoji = \"❓\"\n",
        "        status = \"VALIDATION UNCLEAR\"\n",
        "\n",
        "    message = f\"\\n\\n{emoji} **{status}**\\n\"\n",
        "    message += f\"**Summary:** {validation_result.get('summary', 'No summary available')}\\n\"\n",
        "    message += f\"**Confidence:** {validation_result.get('confidence', 'unknown').title()}\\n\"\n",
        "    message += f\"**Recommendation:** {validation_result.get('recommendation', 'MANUAL_REVIEW')}\\n\\n\"\n",
        "\n",
        "    # Add specific validation checks\n",
        "    checks = [\n",
        "        (\"Intent Fulfilled\", validation_result.get(\"intent_fulfilled\")),\n",
        "        (\"Data Integrity\", validation_result.get(\"data_integrity_intact\")),\n",
        "        (\"Logical Consistency\", validation_result.get(\"logical_consistency\")),\n",
        "        (\"Schema Appropriate\", validation_result.get(\"schema_appropriate\"))\n",
        "    ]\n",
        "\n",
        "    message += \"**Validation Checks:**\\n\"\n",
        "    for check_name, check_result in checks:\n",
        "        if check_result is True:\n",
        "            message += f\"• ✅ {check_name}: Passed\\n\"\n",
        "        elif check_result is False:\n",
        "            message += f\"• ❌ {check_name}: Failed\\n\"\n",
        "        else:\n",
        "            message += f\"• ❓ {check_name}: Unknown\\n\"\n",
        "\n",
        "    # Add issues if any\n",
        "    issues = validation_result.get(\"issues_found\", [])\n",
        "    if issues:\n",
        "        message += f\"\\n**Issues Found ({len(issues)}):**\\n\"\n",
        "        for issue in issues:\n",
        "            message += f\"• ⚠️ {issue}\\n\"\n",
        "\n",
        "    # Add positive changes if any\n",
        "    positives = validation_result.get(\"positive_changes\", [])\n",
        "    if positives:\n",
        "        message += f\"\\n**Positive Changes ({len(positives)}):**\\n\"\n",
        "        for positive in positives:\n",
        "            message += f\"• ✅ {positive}\\n\"\n",
        "\n",
        "    return message\n",
        "\n",
        "\n",
        "def execute_code_blocks(code_blocks, globals_dict):\n",
        "    \"\"\"\n",
        "    Execute code blocks and return results\n",
        "    Separated from main function for better structure\n",
        "    \"\"\"\n",
        "    import io\n",
        "    import contextlib\n",
        "\n",
        "    execution_results = \"\"\n",
        "    success_count = 0\n",
        "    total_blocks = len(code_blocks)\n",
        "    failed_code = \"\"\n",
        "    error_msg = \"\"\n",
        "\n",
        "    for i, code_block in enumerate(code_blocks):\n",
        "        print(f\"\\n--- Executing Code Block {i+1}/{total_blocks} ---\")\n",
        "\n",
        "        # Capture stdout for this block\n",
        "        captured_output = io.StringIO()\n",
        "\n",
        "        try:\n",
        "            with contextlib.redirect_stdout(captured_output):\n",
        "                exec(code_block, globals_dict)\n",
        "\n",
        "            # Get the captured output\n",
        "            block_output = captured_output.getvalue()\n",
        "\n",
        "            # Print the output\n",
        "            if block_output.strip():\n",
        "                print(block_output)\n",
        "                execution_results += f\"\\n--- Output from Code Block {i+1} ---\\n{block_output}\\n\"\n",
        "            else:\n",
        "                print(\"Code executed successfully (no output)\")\n",
        "                execution_results += f\"\\n--- Code Block {i+1} executed successfully (no output) ---\\n\"\n",
        "\n",
        "            success_count += 1\n",
        "            print(f\"✓ Code Block {i+1} executed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            failed_code = code_block\n",
        "            print(f\"✗ Code Block {i+1} failed with error: {error_msg}\")\n",
        "            execution_results += f\"\\n--- Code Block {i+1} FAILED ---\\nError: {error_msg}\\n\"\n",
        "            break  # Stop executing remaining blocks on first error\n",
        "\n",
        "    all_executed_successfully = (success_count == total_blocks)\n",
        "\n",
        "    return execution_results, all_executed_successfully, failed_code, error_msg\n",
        "\n",
        "def generate_code_and_execute(state: AgentState) -> Dict:\n",
        "    \"\"\"\n",
        "    Enhanced version: Generate and execute code using a two-step AI approach with GPT-4.1 validation\n",
        "    and support for version-specific dataframes with conversation history management\n",
        "    \"\"\"\n",
        "    print(\"Starting code generation with version support...\")\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    df = state[\"df\"]  # This will be version-specific if user requested a specific version\n",
        "    selected_context = state.get(\"selected_context\")\n",
        "    version_context = state.get(\"version_context\", \"\")\n",
        "    using_specific_version = state.get(\"using_specific_version\", False)\n",
        "    requested_version = state.get(\"requested_version\")\n",
        "\n",
        "    # Store original dataframe for validation\n",
        "    original_df = df.copy()\n",
        "\n",
        "    # Get OpenAI client from state or create new one\n",
        "    openai_client = state.get(\"openai_client\") or OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "    # Get Anthropic client from state or create new one\n",
        "    anthropic_client = state.get(\"anthropic_client\") or Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "    # Get column information for context\n",
        "    column_info = \", \".join(df.columns) if df is not None else \"No columns available\"\n",
        "\n",
        "    # Create SAMPLE dataframe content for GPT-4.1 (first 5 rows instead of full dataset)\n",
        "    if df is not None:\n",
        "        # Get first 5 rows for GPT-4.1 context\n",
        "        sample_df = df.head(5)\n",
        "\n",
        "        # Use smart compression for the sample\n",
        "        compression_result = smart_dataframe_compression(sample_df, max_tokens=500)\n",
        "        df_sample_content = compression_result['compressed_data']\n",
        "\n",
        "        # Prepare comprehensive data summary with sample data and version info\n",
        "        df_summary = f\"\"\"\n",
        "DATAFRAME CONTENT {'(SPECIFIC VERSION)' if using_specific_version else '(CURRENT VERSION)'}:\n",
        "{version_context if version_context else ''}\n",
        "\n",
        "SAMPLE DATAFRAME CONTENT (First 5 rows):\n",
        "{df_sample_content}\n",
        "\n",
        "FULL DATAFRAME STATISTICS:\n",
        "- Shape: {df.shape}\n",
        "- Columns: {list(df.columns)}\n",
        "- Data types: {dict(df.dtypes)}\n",
        "- Memory usage: {df.memory_usage(deep=True).sum()} bytes\n",
        "- Null values per column: {dict(df.isnull().sum())}\n",
        "\n",
        "{'VERSION-SPECIFIC ANALYSIS CONTEXT:' if using_specific_version else ''}\n",
        "{f'Working with version: {requested_version}' if using_specific_version else 'Working with current/latest version'}\n",
        "\n",
        "NOTE: This is a sample of the first 5 rows. The complete dataframe has {len(df)} rows.\n",
        "\"\"\"\n",
        "    else:\n",
        "        df_summary = \"No data available\"\n",
        "\n",
        "    # Create versions directory if it doesn't exist\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "    os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "    # Print initial state for debugging\n",
        "    print(f\"\\n==== STARTING CODE GENERATION WITH VERSION SUPPORT ====\")\n",
        "    print(f\"User query: {messages[-1]['content'] if messages[-1]['role'] == 'user' else 'No user query found'}\")\n",
        "    print(f\"Dataframe has {len(df) if df is not None else 0} rows and {len(df.columns) if df is not None else 0} columns\")\n",
        "    print(f\"Using specific version: {using_specific_version}\")\n",
        "    if using_specific_version:\n",
        "        print(f\"Requested version: {requested_version}\")\n",
        "    print(f\"Sending FIRST 5 ROWS to GPT-4.1 (sample instead of full dataset)\")\n",
        "    if selected_context:\n",
        "        print(f\"Selected message context: {selected_context[:100]}...\")\n",
        "\n",
        "    try:\n",
        "        # First, use GPT-4 to create the optimal prompt for Claude\n",
        "        print(\"\\n==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH VERSION CONTEXT) ====\")\n",
        "        versions_info = get_versions_info_for_prompt()\n",
        "\n",
        "        # Enhanced system prompt for GPT-4 to create a Claude prompt with version awareness\n",
        "        gpt_system_prompt = f\"\"\"You are an expert at creating prompts for Claude AI to generate code.\n",
        "        Your task is to analyze the user query history and convert it into an optimal prompt for Claude to generate Python code that analyzes a rent roll dataframe.\n",
        "\n",
        "        CRITICAL INFORMATION: The dataframe is ALREADY LOADED and available as 'df'.\n",
        "        {'This is a SPECIFIC VERSION of the dataframe as requested by the user.' if using_specific_version else 'This is the current/latest version of the dataframe.'}\n",
        "\n",
        "        {f'''\n",
        "        VERSION-SPECIFIC CONTEXT:\n",
        "        ========================\n",
        "        The user specifically requested version: {requested_version}\n",
        "        This version has been loaded and is available as 'df'.\n",
        "\n",
        "        {version_context}\n",
        "\n",
        "        IMPORTANT: All analysis should be performed on this specific version, not the current/latest version.\n",
        "        Make sure Claude understands it's working with the requested version.\n",
        "        ''' if using_specific_version else ''}\n",
        "\n",
        "        HERE IS A SAMPLE OF THE DATAFRAME CONTENT (FIRST 5 ROWS OUT OF {len(df)} TOTAL ROWS):\n",
        "        {df_summary}\n",
        "\n",
        "        IMPORTANT: This is only a sample of the first 5 rows to give you context about the data structure and content.\n",
        "        The actual dataframe that Claude will work with contains ALL {len(df)} rows.\n",
        "\n",
        "        {f'''\n",
        "        SELECTED MESSAGE CONTEXT:\n",
        "        ========================\n",
        "        The user has selected a previous assistant message as context for this request. This means they want to build upon or reference previous analysis:\n",
        "\n",
        "        SELECTED CONTEXT: {selected_context}\n",
        "\n",
        "        When creating the Claude prompt, make sure to:\n",
        "        1. Reference this previous context appropriately\n",
        "        2. Build upon the analysis mentioned in the context\n",
        "        3. Ensure continuity between the previous analysis and the current request\n",
        "        4. Help Claude understand what was previously discussed or analyzed\n",
        "        ''' if selected_context else ''}\n",
        "\n",
        "        # IMPORTANT: DATAFRAME VERSION MANAGEMENT\n",
        "        {versions_info}\n",
        "\n",
        "        Some important guidelines to include in your prompt to Claude:\n",
        "        1. The variable 'df' is ALREADY DEFINED and CONTAINS ALL {len(df)} ROWS OF DATA from {'version ' + requested_version if using_specific_version else 'the current version'}\n",
        "        2. Claude should explain its approach step by step before showing code\n",
        "        3. Code must be wrapped in ```python and ``` blocks\n",
        "        4. Code MUST display ALL rows in the output when showing tables (no limiting rows)\n",
        "        5. Claude should not attempt to clean data unless specifically requested\n",
        "        6. Code should include proper error handling\n",
        "        7. IMPORTANT: After performing any analysis or showing results, Claude should ALWAYS call the save_dataframe_version() function to maintain version history\n",
        "        8. CRITICAL: Claude should NOT use try-except blocks in its code. Any errors should be allowed to propagate naturally\n",
        "        {f'''\n",
        "        9. VERSION AWARENESS: Claude is working with version {requested_version}, not the current/latest version\n",
        "        10. VERSION CONTEXT: Include reference to the version context when appropriate\n",
        "        ''' if using_specific_version else ''}\n",
        "        {f'''\n",
        "        9. CONTEXT AWARENESS: Reference and build upon the selected previous message context when appropriate\n",
        "        10. CONTINUITY: Ensure the analysis flows logically from the previous context\n",
        "        ''' if selected_context and not using_specific_version else ''}\n",
        "\n",
        "        Your output will be directly sent to Claude, so format it as a complete system prompt.\n",
        "        Include any table formatting functions that might be useful.\n",
        "\n",
        "        Make sure to include these helper functions in your prompt:\n",
        "\n",
        "        ```python\n",
        "        # For tabular display with proper formatting (PREFERRED METHOD):\n",
        "        def print_formatted_table(df, title=None):\n",
        "            if title:\n",
        "                print(f\"\\\\n{{title}}\")\n",
        "                print(\"=\" * 80)\n",
        "\n",
        "            # Create a display copy (doesn't change original df)\n",
        "            display_df = df.copy()\n",
        "\n",
        "            # Set pandas display options for better readability\n",
        "            # Show ALL rows - no limits\n",
        "            pd.set_option('display.max_rows', None)\n",
        "            pd.set_option('display.max_columns', None)\n",
        "            pd.set_option('display.width', 1000)\n",
        "            pd.set_option('display.colheader_justify', 'left')\n",
        "            pd.set_option('display.precision', 2)\n",
        "\n",
        "            # Display the dataframe - ALL rows will be shown\n",
        "            print(display_df)\n",
        "\n",
        "            # Reset display options to default\n",
        "            pd.reset_option('display.max_rows')\n",
        "            pd.reset_option('display.max_columns')\n",
        "            pd.reset_option('display.width')\n",
        "            pd.reset_option('display.colheader_justify')\n",
        "            pd.reset_option('display.precision')\n",
        "        ```\n",
        "\n",
        "        ```python\n",
        "        # Function to save dataframe versions\n",
        "        def save_dataframe_version(df, operation_description=\"\"):\n",
        "            import os\n",
        "            from datetime import datetime\n",
        "\n",
        "            # Create versions directory if it doesn't exist\n",
        "            versions_dir = \"rent_roll_versions\"\n",
        "            os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "            # Generate version name with timestamp\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            version_name = f\"v_{{timestamp}}\"\n",
        "\n",
        "            # Create filenames for both CSV and Excel\n",
        "            csv_filename = os.path.join(versions_dir, f\"rent_roll_{{version_name}}.csv\")\n",
        "            excel_filename = os.path.join(versions_dir, f\"rent_roll_{{version_name}}.xlsx\")\n",
        "\n",
        "            # Save as CSV\n",
        "            df.to_csv(csv_filename, index=False)\n",
        "\n",
        "            # Save as Excel\n",
        "            df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "\n",
        "            print(f\"✓ Saved dataframe version {{version_name}}: {{operation_description}}\")\n",
        "            print(f\"  - CSV: {{csv_filename}}\")\n",
        "            print(f\"  - Excel: {{excel_filename}}\")\n",
        "\n",
        "            # Return the version name for reference\n",
        "            return version_name\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        # Filter out system messages and apply conversation history management\n",
        "        filtered_messages = []\n",
        "        for msg in messages:\n",
        "            if msg[\"role\"] != \"system\":\n",
        "                filtered_messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
        "\n",
        "        # ✅ APPLY CONVERSATION HISTORY MANAGEMENT HERE\n",
        "        print(f\"Original conversation size: {len(filtered_messages)} messages\")\n",
        "\n",
        "        # Check if conversation manager exists and apply compression\n",
        "        if 'conversation_manager' in globals() and conversation_manager is not None:\n",
        "            try:\n",
        "                print(\"🔧 Applying conversation history compression...\")\n",
        "                initial_size = conversation_manager.get_conversation_size(filtered_messages)\n",
        "                print(f\"Initial token count: {initial_size}\")\n",
        "\n",
        "                optimized_messages = conversation_manager.compress_history_if_needed(\n",
        "                    filtered_messages, openai_client\n",
        "                )\n",
        "\n",
        "                final_size = conversation_manager.get_conversation_size(optimized_messages)\n",
        "                print(f\"Compressed token count: {final_size}\")\n",
        "                print(f\"Compression ratio: {final_size/initial_size:.2%} of original size\")\n",
        "                print(f\"Messages after compression: {len(optimized_messages)}\")\n",
        "\n",
        "                filtered_messages = optimized_messages\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Conversation compression failed: {str(e)}\")\n",
        "                print(\"Falling back to recent message truncation...\")\n",
        "                # Fallback: Keep only recent messages\n",
        "                filtered_messages = filtered_messages[-10:] if len(filtered_messages) > 10 else filtered_messages\n",
        "        else:\n",
        "            print(\"⚠️ No conversation manager found, using simple truncation...\")\n",
        "            # Simple fallback: Keep only recent messages to avoid token limits\n",
        "            if len(filtered_messages) > 12:\n",
        "                filtered_messages = filtered_messages[-12:]\n",
        "                print(f\"Truncated to recent {len(filtered_messages)} messages\")\n",
        "\n",
        "        # Convert the messages to the format expected by OpenAI\n",
        "        gpt_messages = [{\"role\": \"system\", \"content\": gpt_system_prompt}]\n",
        "        for msg in filtered_messages:\n",
        "            gpt_messages.append(msg)\n",
        "\n",
        "        # Add a final message explaining the task clearly with version awareness\n",
        "        final_instruction = f\"\"\"Based on this conversation history and the {'VERSION-SPECIFIC' if using_specific_version else ''} SAMPLE dataframe content (first 5 rows) provided above, create the optimal Claude prompt to generate Python code for rent roll analysis.\n",
        "\n",
        "        {'CRITICAL: The user requested analysis of version ' + requested_version + '. ' if using_specific_version else ''}The prompt should emphasize that:\n",
        "        - The dataframe already exists and is loaded as 'df' with ALL {len(df)} rows\n",
        "        - {'This is version ' + requested_version + ', not the current/latest version' if using_specific_version else 'This is the current version of the data'}\n",
        "        - ALL rows should be displayed when requested\n",
        "        - Versions should be saved with save_dataframe_version() function\n",
        "        - You have access to a representative sample of the data structure\n",
        "\n",
        "{f'''\n",
        "IMPORTANT: The user has selected a previous assistant message as context for this request. Make sure the Claude prompt acknowledges this context and builds upon the previous analysis mentioned in the selected message.\n",
        "\n",
        "SELECTED CONTEXT TO REFERENCE: {selected_context[:500]}...\n",
        "''' if selected_context else ''}\n",
        "\n",
        "{f'''\n",
        "VERSION CONTEXT TO INCLUDE: The user specifically requested analysis of version {requested_version}. Make sure Claude understands this and references it appropriately.\n",
        "''' if using_specific_version else ''}\"\"\"\n",
        "\n",
        "        gpt_messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": final_instruction\n",
        "        })\n",
        "\n",
        "        # Estimate final token count before sending\n",
        "        if 'conversation_manager' in globals() and conversation_manager is not None:\n",
        "            final_token_estimate = conversation_manager.get_conversation_size(gpt_messages)\n",
        "            print(f\"Final GPT-4.1 request token estimate: {final_token_estimate}\")\n",
        "\n",
        "            if final_token_estimate > 28000:  # Safety margin\n",
        "                print(\"⚠️ Still approaching token limit, applying emergency truncation...\")\n",
        "                # Emergency fallback\n",
        "                gpt_messages = gpt_messages[:3] + gpt_messages[-5:]  # Keep system + recent messages\n",
        "\n",
        "        # Get the optimized prompt from GPT-4\n",
        "        gpt_response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=gpt_messages,\n",
        "            max_tokens=4000,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        claude_system_prompt = gpt_response.choices[0].message.content\n",
        "\n",
        "        # Print the generated prompt for debugging\n",
        "        print(\"\\n==== GPT-4 GENERATED PROMPT FOR CLAUDE (WITH VERSION CONTEXT) ====\")\n",
        "        print(claude_system_prompt[:500] + \"...\" if len(claude_system_prompt) > 500 else claude_system_prompt)\n",
        "        print(\"==== END OF PROMPT (TRUNCATED) ====\\n\")\n",
        "\n",
        "        logger.info(f\"Generated optimized prompt for Claude using GPT-4 with {'version-specific' if using_specific_version else 'current'} dataframe\")\n",
        "\n",
        "        # Now use the GPT-4 generated prompt to ask Claude for code\n",
        "        print(f\"\\n==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION ({'VERSION-SPECIFIC' if using_specific_version else 'CURRENT'}) ====\")\n",
        "        logger.info(f\"Sending optimized prompt to Claude for {'version-specific' if using_specific_version else 'current'} code generation\")\n",
        "\n",
        "        # Prepare messages for Claude with enhanced context\n",
        "        claude_messages = filtered_messages.copy()\n",
        "\n",
        "        # Enhanced sample data message with version and context awareness\n",
        "        sample_data_content = f\"\"\"Here is a SAMPLE of the dataframe that's already loaded as 'df' (showing first 5 rows out of {len(df)} total rows):\n",
        "\n",
        "{df_summary}\n",
        "\n",
        "{'VERSION NOTICE: You are working with version ' + requested_version + ', not the current/latest version. ' if using_specific_version else ''}Please process my request using this FULL dataset of {len(df)} rows and remember to save versions with save_dataframe_version().\n",
        "\n",
        "{f'IMPORTANT CONTEXT: Please reference and build upon this previous analysis: {selected_context[:300]}...' if selected_context else ''}\"\"\"\n",
        "\n",
        "        sample_data_message = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": sample_data_content\n",
        "        }\n",
        "        claude_messages.append(sample_data_message)\n",
        "\n",
        "        # Try to get code from Claude\n",
        "        claude_response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            system=claude_system_prompt,\n",
        "            messages=claude_messages,\n",
        "            max_tokens=4000,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the response text from Claude\n",
        "        response_text = claude_response.content[0].text\n",
        "\n",
        "        # Print Claude's response for debugging\n",
        "        print(f\"\\n==== CLAUDE'S RESPONSE ({'VERSION ' + requested_version if using_specific_version else 'CURRENT VERSION'}) ====\")\n",
        "        print(response_text[:500] + \"...\" if len(response_text) > 500 else response_text)\n",
        "        print(\"==== END OF CLAUDE RESPONSE (TRUNCATED) ====\\n\")\n",
        "\n",
        "        # Extract code blocks\n",
        "        code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "\n",
        "        # Print extracted code blocks for debugging\n",
        "        print(f\"\\n==== EXTRACTED {len(code_blocks)} CODE BLOCKS ====\")\n",
        "        for i, block in enumerate(code_blocks):\n",
        "            print(f\"\\n-- Code Block {i+1} --\")\n",
        "            print(block[:200] + \"...\" if len(block) > 200 else block)\n",
        "\n",
        "        # If no code blocks are found, add emergency code\n",
        "        if len(code_blocks) == 0:\n",
        "            emergency_code = f\"\"\"\n",
        "# Emergency code to display the {'version-specific' if using_specific_version else 'current'} dataframe\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"\\\\n=== RENT ROLL DATA {'(VERSION ' + requested_version + ')' if using_specific_version else '(CURRENT VERSION)'} ===\\\\n\")\n",
        "print(f\"Displaying all {{len(df)}} rows and {{len(df.columns)}} columns\\\\n\")\n",
        "\n",
        "# Print the entire dataframe\n",
        "print(df)\n",
        "\n",
        "# Save a version of the dataframe\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Create versions directory if it doesn't exist\n",
        "versions_dir = \"rent_roll_versions\"\n",
        "os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "# Generate version name with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "version_name = f\"v_{{timestamp}}\"\n",
        "\n",
        "# Create filename\n",
        "filename = os.path.join(versions_dir, f\"rent_roll_{{version_name}}.csv\")\n",
        "\n",
        "# Save dataframe\n",
        "df.to_csv(filename, index=False)\n",
        "\n",
        "print(f\"✓ Saved dataframe version {{version_name}}: Emergency display of {'version ' + requested_version if using_specific_version else 'current'} data\")\n",
        "\"\"\"\n",
        "            code_blocks.append(emergency_code)\n",
        "            print(\"\\n-- Added Emergency Code Block --\")\n",
        "            print(f\"Emergency code added since Claude didn't generate code for {'version ' + requested_version if using_specific_version else 'current version'}\")\n",
        "\n",
        "        # Define helper functions\n",
        "        def print_formatted_table(df, title=None):\n",
        "            if title:\n",
        "                print(f\"\\n{title}\")\n",
        "                print(\"=\" * 80)\n",
        "            display_df = df.copy()\n",
        "            pd.set_option('display.max_rows', None)\n",
        "            pd.set_option('display.max_columns', None)\n",
        "            pd.set_option('display.width', 1000)\n",
        "            pd.set_option('display.colheader_justify', 'left')\n",
        "            pd.set_option('display.precision', 2)\n",
        "            print(display_df)\n",
        "            pd.reset_option('display.max_rows')\n",
        "            pd.reset_option('display.max_columns')\n",
        "            pd.reset_option('display.width')\n",
        "            pd.reset_option('display.colheader_justify')\n",
        "            pd.reset_option('display.precision')\n",
        "\n",
        "        def print_bordered_table(df, title=None):\n",
        "            if title:\n",
        "                print(f\"\\n{title}\")\n",
        "                print(\"=\" * 80)\n",
        "            if len(df) == 0:\n",
        "                print(\"No data available\")\n",
        "                return\n",
        "            display_df = df.copy()\n",
        "            col_widths = {}\n",
        "            for col in display_df.columns:\n",
        "                col_values = display_df[col].astype(str)\n",
        "                max_data_width = col_values.str.len().max()\n",
        "                col_widths[col] = max(len(str(col)), max_data_width) + 2\n",
        "            header = \"| \" + \" | \".join(str(col).ljust(col_widths[col]) for col in display_df.columns) + \" |\"\n",
        "            separator = \"+\" + \"+\".join(\"-\" * (col_widths[col] + 2) for col in display_df.columns) + \"+\"\n",
        "            print(separator)\n",
        "            print(header)\n",
        "            print(separator)\n",
        "            for i in range(len(display_df)):\n",
        "                row = display_df.iloc[i]\n",
        "                row_str = \"| \" + \" | \".join(str(val).ljust(col_widths[col]) for col, val in row.items()) + \" |\"\n",
        "                print(row_str)\n",
        "            print(separator)\n",
        "            print(f\"Total rows: {len(display_df)}\")\n",
        "\n",
        "        # Add to globals_dict before executing code\n",
        "        globals_dict = {\n",
        "            \"df\": df,  # This is now the version-specific dataframe if requested\n",
        "            \"pd\": pd,\n",
        "            \"np\": np,\n",
        "            \"os\": os,\n",
        "            \"datetime\": datetime,\n",
        "            \"versions_dir\": versions_dir,\n",
        "            \"print_formatted_table\": print_formatted_table,\n",
        "            \"print_bordered_table\": print_bordered_table,\n",
        "            \"save_dataframe_version\": save_dataframe_version\n",
        "        }\n",
        "\n",
        "        print(f\"\\n==== STEP 3: EXECUTING CODE ({'VERSION ' + requested_version if using_specific_version else 'CURRENT VERSION'}) ====\")\n",
        "\n",
        "        # Execute initial attempt first\n",
        "        execution_results, all_executed_successfully, failed_code, error_msg = execute_code_blocks(code_blocks, globals_dict)\n",
        "\n",
        "        # Handle retries if needed\n",
        "        max_retries = 3\n",
        "        retry_count = 0\n",
        "\n",
        "        while not all_executed_successfully and retry_count < max_retries:\n",
        "            retry_count += 1\n",
        "            print(f\"\\n==== RETRY ATTEMPT {retry_count}/{max_retries} ====\")\n",
        "\n",
        "            # Create a retry message with more details each time\n",
        "            retry_message = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"The code you provided failed with this error: {error_msg}\n",
        "\n",
        "                Here is the code that failed:\n",
        "                ```python\n",
        "                {failed_code}\n",
        "                ```\n",
        "\n",
        "                This is retry attempt {retry_count} of {max_retries}.\n",
        "\n",
        "                {'REMEMBER: You are working with version ' + requested_version + ', not the current version.' if using_specific_version else ''}\n",
        "\n",
        "                {\"After multiple attempts, please try a completely different approach.\" if retry_count >= 2 else \"Please fix this specific error.\"}\n",
        "                IMPORTANT: DO NOT use try-except blocks in your code. Allow any errors to propagate naturally so our system can detect them.\n",
        "                Please fix this code to handle the specific error while maintaining the requirement to show ALL rows in the output and saving a version with save_dataframe_version().\n",
        "                Return the corrected code wrapped in ```python and ``` blocks.\"\"\"\n",
        "            }\n",
        "\n",
        "            # Add this feedback to the messages\n",
        "            fix_messages = claude_messages.copy()\n",
        "            fix_messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "            fix_messages.append(retry_message)\n",
        "\n",
        "            # Get Claude's fixed code\n",
        "            retry_response = anthropic_client.messages.create(\n",
        "                model=\"claude-3-7-sonnet-20250219\",\n",
        "                system=claude_system_prompt,\n",
        "                messages=fix_messages,\n",
        "                max_tokens=3500,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            retry_text = retry_response.content[0].text\n",
        "            print(f\"\\n==== CLAUDE'S FIX SUGGESTION (ATTEMPT {retry_count}) ====\")\n",
        "            print(retry_text[:500] + \"...\" if len(retry_text) > 500 else retry_text)\n",
        "            print(\"==== END OF FIX SUGGESTION ====\\n\")\n",
        "\n",
        "            # Extract new code blocks from the retry response\n",
        "            new_code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', retry_text, re.DOTALL)\n",
        "\n",
        "            if new_code_blocks:\n",
        "                code_blocks = new_code_blocks\n",
        "                response_text = retry_text  # Update the response text\n",
        "                print(f\"Found {len(new_code_blocks)} new code blocks in retry response\")\n",
        "            else:\n",
        "                print(\"No code blocks found in retry response, keeping original code\")\n",
        "\n",
        "            # Execute the new code blocks\n",
        "            retry_results, all_executed_successfully, failed_code, error_msg = execute_code_blocks(code_blocks, globals_dict)\n",
        "            execution_results += f\"\\n\\n--- RETRY ATTEMPT {retry_count} RESULTS ---\\n{retry_results}\"\n",
        "\n",
        "            if all_executed_successfully:\n",
        "                print(f\"\\n✅ RETRY {retry_count} SUCCESSFUL!\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"\\n⚠️ Retry {retry_count} failed. {'Trying again...' if retry_count < max_retries else 'Maximum retries reached.'}\")\n",
        "\n",
        "        # Update the dataframe in state with any changes\n",
        "        if \"df\" in globals_dict:\n",
        "            # Important: Only update app_state if we're working with current version\n",
        "            if not using_specific_version:\n",
        "                state[\"df\"] = globals_dict[\"df\"]\n",
        "            else:\n",
        "                # For version-specific analysis, keep the changes in the state but don't update global app_state\n",
        "                state[\"df\"] = globals_dict[\"df\"]\n",
        "\n",
        "        # Enhanced validation with version awareness\n",
        "        print(f\"\\n==== STEP 4: VALIDATING CHANGES ({'VERSION ' + requested_version if using_specific_version else 'CURRENT VERSION'}) ====\")\n",
        "\n",
        "        if execution_results and all_executed_successfully:\n",
        "            # Get the new dataframe (after changes)\n",
        "            new_df = globals_dict[\"df\"]\n",
        "\n",
        "            # Get the user's original message\n",
        "            user_message = messages[-1][\"content\"] if messages[-1][\"role\"] == \"user\" else \"\"\n",
        "\n",
        "            # Get the executed code for context\n",
        "            executed_code = \"\\n\\n# --- Next Code Block ---\\n\\n\".join(code_blocks)\n",
        "\n",
        "            # Perform validation\n",
        "            print(\"🔍 Validating changes with GPT-4.1...\")\n",
        "            validation_result = validate_dataframe_changes_with_gpt4(\n",
        "                original_df=original_df,\n",
        "                new_df=new_df,\n",
        "                user_query=user_message,\n",
        "                executed_code=executed_code\n",
        "            )\n",
        "\n",
        "            # Format validation message with version awareness\n",
        "            validation_message = format_validation_message(validation_result)\n",
        "\n",
        "            if using_specific_version:\n",
        "                validation_message += f\"\\n\\n📋 **Version Analysis Note**: This validation was performed on version `{requested_version}`. Changes to this version do not affect the current/latest dataframe unless explicitly saved as a new version.\"\n",
        "\n",
        "            # Add validation to the response\n",
        "            final_response = response_text + execution_results + validation_message\n",
        "            print(f\"🔍 Validation: {validation_result.get('validation_result', 'UNKNOWN')}\")\n",
        "\n",
        "        else:\n",
        "            # No validation if execution failed\n",
        "            validation_message = f\"\\n\\n🔍 **Validation Skipped** ❌\\n*Code execution failed - no changes to validate*\"\n",
        "            if using_specific_version:\n",
        "                validation_message += f\"\\n*Note: Analysis was attempted on version {requested_version}*\"\n",
        "            final_response = response_text + execution_results + validation_message\n",
        "            validation_result = None\n",
        "\n",
        "        # Add version and hybrid note\n",
        "        version_note = f\"\\n\\n📋 **Version Info**: Analysis performed on {'version ' + requested_version if using_specific_version else 'current/latest version'}\"\n",
        "        hybrid_note = f\"\\n\\n---\\n*This response combines GPT-4 for prompt optimization and Claude for code generation, with automated error handling ({retry_count} retries used).*\"\n",
        "        final_response += version_note + hybrid_note\n",
        "\n",
        "        # Create complete state return\n",
        "        new_state = dict(state)\n",
        "        new_state.update({\n",
        "            \"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": final_response}],\n",
        "            \"df\": globals_dict[\"df\"],  # Updated dataframe (version-specific or current)\n",
        "            \"final_response\": final_response,\n",
        "            \"code_execution_results\": execution_results,\n",
        "            \"validation_result\": validation_result\n",
        "        })\n",
        "\n",
        "        logger.info(f\"Code generation and execution complete with {'version-specific' if using_specific_version else 'current'} validation\")\n",
        "        print(f\"\\n==== CODE GENERATION COMPLETE ({'VERSION ' + requested_version if using_specific_version else 'CURRENT VERSION'}) ====\")\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"An unexpected error occurred: {str(e)}\"\n",
        "        if using_specific_version:\n",
        "            error_msg += f\" (while analyzing version {requested_version})\"\n",
        "\n",
        "        logger.error(f\"Error in generate_code_and_execute: {error_msg}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        # Return complete error state\n",
        "        new_state = dict(state)\n",
        "        new_state.update({\n",
        "            \"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"content\": error_msg}],\n",
        "            \"df\": state[\"df\"],\n",
        "            \"final_response\": error_msg,\n",
        "            \"code_execution_results\": \"\",\n",
        "            \"validation_result\": None\n",
        "        })\n",
        "\n",
        "        return new_state\n",
        "\n",
        "def save_dataframe_version(df, operation_description=\"\"):\n",
        "    \"\"\"\n",
        "    FIXED: Save the current state of the dataframe as both CSV and Excel files.\n",
        "\n",
        "    Args:\n",
        "        df: The dataframe to save\n",
        "        operation_description: A string describing what operation was performed\n",
        "\n",
        "    Returns:\n",
        "        version_name: The name of the version that was saved\n",
        "    \"\"\"\n",
        "    global app_state\n",
        "\n",
        "    # Create versions directory if it doesn't exist\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "    os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "    # Generate version name with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    version_name = f\"v_{timestamp}\"\n",
        "\n",
        "    # Create filenames for both CSV and Excel\n",
        "    csv_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.csv\")\n",
        "    excel_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.xlsx\")\n",
        "\n",
        "    try:\n",
        "        # Save as CSV\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        logger.info(f\"Saved CSV: {csv_filename}\")\n",
        "\n",
        "        # Save as Excel\n",
        "        df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "        logger.info(f\"Saved Excel: {excel_filename}\")\n",
        "\n",
        "        # FIXED: Ensure app_state and df_versions exist\n",
        "        if app_state is None:\n",
        "            logger.warning(\"app_state is None, initializing basic structure\")\n",
        "            app_state = {\"df_versions\": []}\n",
        "\n",
        "        if \"df_versions\" not in app_state:\n",
        "            app_state[\"df_versions\"] = []\n",
        "\n",
        "        # Add version metadata to the registry\n",
        "        version_info = {\n",
        "            'name': version_name,\n",
        "            'description': operation_description,\n",
        "            'timestamp': timestamp,\n",
        "            'csv_filename': csv_filename,  # FIXED: Always use csv_filename\n",
        "            'excel_filename': excel_filename,\n",
        "            'shape': list(df.shape),\n",
        "            'columns': list(df.columns),\n",
        "            'is_original': len(app_state[\"df_versions\"]) == 0  # First one is original\n",
        "        }\n",
        "\n",
        "        app_state[\"df_versions\"].append(version_info)\n",
        "\n",
        "        print(f\"✓ Saved dataframe version {version_name}: {operation_description}\")\n",
        "        print(f\"  - CSV: {csv_filename}\")\n",
        "        print(f\"  - Excel: {excel_filename}\")\n",
        "        print(f\"  - Shape: {df.shape}\")\n",
        "        print(f\"  - Registry updated: {len(app_state['df_versions'])} total versions\")\n",
        "\n",
        "        logger.info(f\"Version {version_name} saved successfully. Total versions: {len(app_state['df_versions'])}\")\n",
        "\n",
        "        return version_name\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error saving dataframe version: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "        print(f\"❌ {error_msg}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def validate_simple_query_execution(original_df, result_df, user_query, execution_output):\n",
        "    \"\"\"\n",
        "    Lightweight validation for queries that don't modify data\n",
        "    \"\"\"\n",
        "    # Check if dataframes are identical\n",
        "    if original_df.equals(result_df):\n",
        "        return {\n",
        "            \"validation_result\": \"PASS\",\n",
        "            \"intent_fulfilled\": True,\n",
        "            \"data_integrity_intact\": True,\n",
        "            \"logical_consistency\": True,\n",
        "            \"schema_appropriate\": True,\n",
        "            \"issues_found\": [],\n",
        "            \"positive_changes\": [\"Query executed successfully without data modifications\"],\n",
        "            \"summary\": f\"Read-only analysis completed successfully. Query '{user_query[:50]}...' provided insights without altering the data.\",\n",
        "            \"confidence\": \"high\",\n",
        "            \"recommendation\": \"PROCEED\"\n",
        "        }\n",
        "    else:\n",
        "        # Data changed, use full GPT-4 validation\n",
        "        return validate_dataframe_changes_with_gpt4(original_df, result_df, user_query)\n",
        "\n",
        "\n",
        "# Additional helper functions for enhanced validation\n",
        "\n",
        "def get_versions_info_for_prompt():\n",
        "    \"\"\"Generate version information for the Claude prompt.\"\"\"\n",
        "    global app_state\n",
        "\n",
        "    if not app_state or not app_state.get(\"df_versions\"):\n",
        "        return \"No versions available yet.\"\n",
        "\n",
        "    # Find the original version\n",
        "    original = next((v for v in app_state[\"df_versions\"] if v.get('is_original')), app_state[\"df_versions\"][0])\n",
        "\n",
        "    # Get the latest version\n",
        "    latest = app_state[\"df_versions\"][-1]\n",
        "\n",
        "    # Format all versions\n",
        "    all_versions = []\n",
        "    for i, version in enumerate(app_state[\"df_versions\"]):\n",
        "        status = []\n",
        "        if version == original:\n",
        "            status.append(\"ORIGINAL\")\n",
        "        if version == latest:\n",
        "            status.append(\"LATEST\")\n",
        "\n",
        "        status_str = f\" ({', '.join(status)})\" if status else \"\"\n",
        "        all_versions.append(f\"{i+1}. {version['name']}{status_str}: {version['description']}\")\n",
        "\n",
        "    versions_text = \"\\n\".join(all_versions)\n",
        "\n",
        "    return f\"\"\"\n",
        "DATAFRAME VERSION HISTORY:\n",
        "{versions_text}\n",
        "\n",
        "Original version: {original['name']}\n",
        "Latest version: {latest['name']}\n",
        "Total versions: {len(app_state[\"df_versions\"])}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def trim_dataframe_output(output_text, max_rows=20, max_chars=None):\n",
        "    \"\"\"\n",
        "    Trim dataframe output to prevent overwhelming responses\n",
        "    \"\"\"\n",
        "    lines = output_text.split('\\n')\n",
        "\n",
        "    if len(lines) <= max_rows:\n",
        "        return output_text\n",
        "\n",
        "    trimmed_lines = lines[:max_rows]\n",
        "    trimmed_lines.append(f\"... [output truncated, showing first {max_rows} lines only] ...\")\n",
        "\n",
        "    return '\\n'.join(trimmed_lines)\n",
        "\n",
        "\n",
        "# Enhanced logging and debugging functions\n",
        "\n",
        "def log_validation_attempt(original_df, new_df, user_query, validation_result):\n",
        "    \"\"\"\n",
        "    Log validation attempts for debugging and improvement\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    logs_dir = \"validation_logs\"\n",
        "    os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_file = os.path.join(logs_dir, f\"validation_{timestamp}.log\")\n",
        "\n",
        "    with open(log_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"VALIDATION LOG - {timestamp}\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"User Query: {user_query}\\n\\n\")\n",
        "        f.write(f\"Original DF Shape: {original_df.shape}\\n\")\n",
        "        f.write(f\"New DF Shape: {new_df.shape}\\n\\n\")\n",
        "        f.write(f\"Validation Result: {validation_result.get('validation_result', 'UNKNOWN')}\\n\")\n",
        "        f.write(f\"Intent Fulfilled: {validation_result.get('intent_fulfilled', 'Unknown')}\\n\")\n",
        "        f.write(f\"Summary: {validation_result.get('summary', 'No summary')}\\n\\n\")\n",
        "        f.write(f\"Issues Found: {validation_result.get('issues_found', [])}\\n\")\n",
        "        f.write(f\"Positive Changes: {validation_result.get('positive_changes', [])}\\n\")\n",
        "\n",
        "    print(f\"📝 Validation logged to: {log_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXr64Vkq_L9v"
      },
      "outputs": [],
      "source": [
        "def format_validation_message_for_chat(validation_result):\n",
        "    \"\"\"\n",
        "    Format validation result for always-visible chat display\n",
        "    Optimized for transparency while remaining readable\n",
        "    \"\"\"\n",
        "    result = validation_result.get(\"validation_result\", \"UNKNOWN\")\n",
        "    confidence = validation_result.get(\"confidence\", \"unknown\")\n",
        "    summary = validation_result.get(\"summary\", \"No summary available\")\n",
        "\n",
        "    # Status emoji and header\n",
        "    if result == \"PASS\":\n",
        "        emoji = \"✅\"\n",
        "        status_color = \"🟢\"\n",
        "    elif result == \"FAIL\":\n",
        "        emoji = \"❌\"\n",
        "        status_color = \"🔴\"\n",
        "    elif result == \"WARNING\":\n",
        "        emoji = \"⚠️\"\n",
        "        status_color = \"🟡\"\n",
        "    else:\n",
        "        emoji = \"❓\"\n",
        "        status_color = \"⚪\"\n",
        "\n",
        "    # Build the always-visible validation section\n",
        "    message = f\"\"\"\n",
        "\n",
        "🔍 **AI VALIDATION RESULTS** {emoji}\n",
        "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
        "\n",
        "{status_color} **Status:** {result} | **Confidence:** {confidence.title()} | **Recommendation:** {validation_result.get('recommendation', 'MANUAL_REVIEW')}\n",
        "\n",
        "📝 **Summary:** {summary}\n",
        "\n",
        "**Detailed Validation Checks:**\"\"\"\n",
        "\n",
        "    # Always show all validation checks for transparency\n",
        "    checks = [\n",
        "        (\"Intent Fulfilled\", validation_result.get(\"intent_fulfilled\")),\n",
        "        (\"Data Integrity\", validation_result.get(\"data_integrity_intact\")),\n",
        "        (\"Logical Consistency\", validation_result.get(\"logical_consistency\")),\n",
        "        (\"Schema Appropriate\", validation_result.get(\"schema_appropriate\"))\n",
        "    ]\n",
        "\n",
        "    for check_name, check_result in checks:\n",
        "        if check_result is True:\n",
        "            message += f\"\\n• ✅ **{check_name}:** Passed\"\n",
        "        elif check_result is False:\n",
        "            message += f\"\\n• ❌ **{check_name}:** Failed\"\n",
        "        else:\n",
        "            message += f\"\\n• ❓ **{check_name}:** Unknown\"\n",
        "\n",
        "    # Always show issues if any (critical for learning)\n",
        "    issues = validation_result.get(\"issues_found\", [])\n",
        "    if issues:\n",
        "        message += f\"\\n\\n**⚠️ Issues Identified ({len(issues)}):**\"\n",
        "        for i, issue in enumerate(issues, 1):\n",
        "            message += f\"\\n{i}. {issue}\"\n",
        "\n",
        "    # Always show positive changes (reinforces learning)\n",
        "    positives = validation_result.get(\"positive_changes\", [])\n",
        "    if positives:\n",
        "        message += f\"\\n\\n**✅ Positive Changes ({len(positives)}):**\"\n",
        "        for i, positive in enumerate(positives, 1):\n",
        "            message += f\"\\n{i}. {positive}\"\n",
        "\n",
        "    # Always include recommendation explanation\n",
        "    recommendation = validation_result.get(\"recommendation\", \"MANUAL_REVIEW\")\n",
        "    if recommendation == \"PROCEED\":\n",
        "        message += f\"\\n\\n💚 **Recommendation:** Safe to proceed with these changes.\"\n",
        "    elif recommendation == \"RETRY\":\n",
        "        message += f\"\\n\\n🔄 **Recommendation:** Consider refining the approach and trying again.\"\n",
        "    elif recommendation == \"MANUAL_REVIEW\":\n",
        "        message += f\"\\n\\n👀 **Recommendation:** Manual review recommended before proceeding.\"\n",
        "\n",
        "    message += f\"\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\n",
        "\n",
        "    return message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvJUbuAa_kRw"
      },
      "outputs": [],
      "source": [
        "def validate_simple_query_execution(original_df, result_df, user_query, execution_output):\n",
        "    \"\"\"\n",
        "    Lightweight validation for queries that don't modify data\n",
        "    \"\"\"\n",
        "    # Check if dataframes are identical\n",
        "    if original_df.equals(result_df):\n",
        "        return {\n",
        "            \"validation_result\": \"PASS\",\n",
        "            \"intent_fulfilled\": True,\n",
        "            \"data_integrity_intact\": True,\n",
        "            \"logical_consistency\": True,\n",
        "            \"schema_appropriate\": True,\n",
        "            \"issues_found\": [],\n",
        "            \"positive_changes\": [\"Query executed successfully without data modifications\"],\n",
        "            \"summary\": f\"Read-only analysis completed successfully. Query '{user_query[:50]}...' provided insights without altering the data.\",\n",
        "            \"confidence\": \"high\",\n",
        "            \"recommendation\": \"PROCEED\"\n",
        "        }\n",
        "    else:\n",
        "        # Data changed, use full GPT-4 validation\n",
        "        return validate_dataframe_changes_with_gpt4(original_df, result_df, user_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-bzkcxC_mwI"
      },
      "outputs": [],
      "source": [
        "def add_validation_context_to_response(response_text, validation_result):\n",
        "    \"\"\"\n",
        "    Add contextual information to help users understand validation\n",
        "    \"\"\"\n",
        "    learning_note = \"\"\"\n",
        "\n",
        "💡 **Why AI Validation?**\n",
        "This validation helps ensure that:\n",
        "- The AI understood your request correctly\n",
        "- No unintended data changes occurred\n",
        "- Business logic remains sound\n",
        "- You can trust the results for decision-making\n",
        "\n",
        "*This validation runs automatically after every code execution for your protection and learning.*\"\"\"\n",
        "\n",
        "    return response_text + learning_note"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bMY3yPyjw6z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q7RJJWhjgO7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFZKl40KcZaV"
      },
      "outputs": [],
      "source": [
        "# Build the LangGraph workflow\n",
        "def create_agentic_rent_roll_analyzer():\n",
        "    \"\"\"Create and return the agentic rent roll analyzer workflow.\"\"\"\n",
        "\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    workflow.add_node(\"determine_action\", determine_action)\n",
        "    workflow.add_node(\"ask_clarification\", ask_clarification)\n",
        "    workflow.add_node(\"generate_text_response\", generate_text_response)\n",
        "    workflow.add_node(\"generate_code_and_execute\", generate_code_and_execute)\n",
        "\n",
        "    # Set the entry point\n",
        "    workflow.set_entry_point(\"determine_action\")\n",
        "\n",
        "    # Define conditional edges based on dictionary state values\n",
        "    workflow.add_conditional_edges(\n",
        "        \"determine_action\",\n",
        "        lambda state: \"ask_clarification\" if state.get(\"needs_clarification\") else\n",
        "                      \"generate_code_and_execute\" if state.get(\"generate_code\") else\n",
        "                      \"generate_text_response\"\n",
        "    )\n",
        "\n",
        "    # Add edges to END\n",
        "    workflow.add_edge(\"ask_clarification\", END)\n",
        "    workflow.add_edge(\"generate_text_response\", END)\n",
        "    workflow.add_edge(\"generate_code_and_execute\", END)\n",
        "\n",
        "    # Compile the graph\n",
        "    agentic_analyzer = workflow.compile()\n",
        "\n",
        "    return agentic_analyzer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qADS9o8CTiwq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mKF1WukcceN"
      },
      "outputs": [],
      "source": [
        "def upload_rent_roll(file, anthropic_api_key, openai_api_key, auto_analyze):\n",
        "    \"\"\"\n",
        "    FIXED: Process the uploaded rent roll file and initialize the chat with proper versioning\n",
        "    \"\"\"\n",
        "    global app_state\n",
        "\n",
        "    logger.info(\"Starting rent roll upload and processing with fixed versioning\")\n",
        "\n",
        "    # Initialize version system first\n",
        "    ensure_version_system_initialized()\n",
        "\n",
        "    # Use the default API keys if none are provided\n",
        "    anthropic_key = anthropic_api_key if anthropic_api_key else DEFAULT_ANTHROPIC_API_KEY\n",
        "    openai_key = openai_api_key if openai_api_key else DEFAULT_OPENAI_API_KEY\n",
        "    logger.info(\"API keys configured\")\n",
        "\n",
        "    # Validate inputs\n",
        "    if not file:\n",
        "        logger.warning(\"No file uploaded\")\n",
        "        return \"Please upload a rent roll Excel file.\", None, gr.update(visible=False), gr.update(choices=[], value=None)\n",
        "\n",
        "    try:\n",
        "        # Save the uploaded file to a temporary location\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx')\n",
        "        temp_file.close()\n",
        "        file_path = temp_file.name\n",
        "        logger.info(f\"Created temporary file: {file_path}\")\n",
        "\n",
        "        # Copy the uploaded file to our temporary location\n",
        "        with open(file.name, 'rb') as src_file, open(file_path, 'wb') as dst_file:\n",
        "            dst_file.write(src_file.read())\n",
        "        logger.info(\"File copied to temporary location\")\n",
        "\n",
        "        # Use our improved rent roll loader\n",
        "        try:\n",
        "            logger.info(\"Loading rent roll with specialized loader...\")\n",
        "            rent_roll_df = read_rent_roll_simple(file_path)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error with specialized loader: {e}. Falling back to standard loading.\")\n",
        "            rent_roll_df = pd.read_excel(file_path)\n",
        "            logger.info(\"Fallback: Loaded rent roll with default pandas settings\")\n",
        "\n",
        "        logger.info(f\"Loaded rent roll data: {len(rent_roll_df)} rows, {len(rent_roll_df.columns)} columns\")\n",
        "\n",
        "        # Auto-analyze with GPT if selected\n",
        "        issues_list = []\n",
        "        if auto_analyze:\n",
        "            logger.info(\"Auto-analyze option selected. Calling GPT for analysis...\")\n",
        "            issues_list = analyze_rent_roll_gpt(file_path, openai_key)\n",
        "            logger.info(f\"GPT analysis complete. Found {len(issues_list)} issues.\")\n",
        "\n",
        "        # FIXED: Initialize the global app state with proper structure\n",
        "        app_state = {\n",
        "            \"df\": rent_roll_df,\n",
        "            \"issues\": issues_list,\n",
        "            \"anthropic_client\": Anthropic(api_key=anthropic_key),\n",
        "            \"openai_client\": OpenAI(api_key=openai_key),\n",
        "            \"system_message\": \"\",\n",
        "            \"df_versions\": [],  # FIXED: Always initialize as empty list\n",
        "            \"original_filename\": os.path.basename(file.name)  # Store original filename\n",
        "        }\n",
        "\n",
        "        # FIXED: Save the initial version with proper description\n",
        "        initial_version = save_dataframe_version(\n",
        "            rent_roll_df,\n",
        "            f\"Initial upload - original dataset from {os.path.basename(file.name)}\"\n",
        "        )\n",
        "\n",
        "        if initial_version:\n",
        "            logger.info(f\"Created initial dataframe version: {initial_version}\")\n",
        "        else:\n",
        "            logger.error(\"Failed to create initial version\")\n",
        "\n",
        "        # Create system message with data understanding\n",
        "        column_info = []\n",
        "        for col in rent_roll_df.columns:\n",
        "            try:\n",
        "                dtype_str = str(rent_roll_df[col].dtype)\n",
        "                column_info.append(f\"- {col}: {dtype_str}\")\n",
        "            except Exception as e:\n",
        "                column_info.append(f\"- {col}: [Error determining type: {str(e)}]\")\n",
        "        column_info_str = \"\\n\".join(column_info)\n",
        "\n",
        "        # Calculate basic stats about the data\n",
        "        data_stats = []\n",
        "        for col in rent_roll_df.columns:\n",
        "            try:\n",
        "                if pd.api.types.is_numeric_dtype(rent_roll_df[col]):\n",
        "                    stat = f\"- {col}: min={rent_roll_df[col].min()}, max={rent_roll_df[col].max()}, mean={rent_roll_df[col].mean():.2f}, null={rent_roll_df[col].isna().sum()}\"\n",
        "                else:\n",
        "                    unique_vals = rent_roll_df[col].nunique()\n",
        "                    stat = f\"- {col}: unique values={unique_vals}, null={rent_roll_df[col].isna().sum()}\"\n",
        "                data_stats.append(stat)\n",
        "            except:\n",
        "                data_stats.append(f\"- {col}: [error calculating stats]\")\n",
        "        data_stats_str = \"\\n\".join(data_stats)\n",
        "\n",
        "        # Format issues for display\n",
        "        issues_text = \"\\n\".join([f\"- {issue}\" for issue in issues_list])\n",
        "\n",
        "        system_message = f\"\"\"\n",
        "        You are a Commercial Real Estate rent roll assistant that has analyzed a rent roll and found the following issues:\n",
        "\n",
        "        {issues_text}\n",
        "\n",
        "        The rent roll data has {len(rent_roll_df)} rows and {len(rent_roll_df.columns)} columns.\n",
        "\n",
        "        Column information:\n",
        "        {column_info_str}\n",
        "\n",
        "        Data statistics:\n",
        "        {data_stats_str}\n",
        "\n",
        "        When helping the user, follow these critical guidelines:\n",
        "        1. DO NOT generate placeholder code with fake column names. Work ONLY with the actual columns from the dataframe.\n",
        "        2. NEVER assume column names that don't exist in the actual data.\n",
        "        3. Always start by examining the first few rows to understand the meaning of each column.\n",
        "        4. If you can't identify which columns contain certain information, clearly state this limitation.\n",
        "        5. DO NOT proceed with analysis using made-up column names that don't exist in the data.\n",
        "\n",
        "        The entire dataframe is available as 'df' in the execution environment.\n",
        "\n",
        "        Important instructions for code and calculations:\n",
        "        1. ALWAYS share your chain of thought reasoning in your responses. For each analysis:\n",
        "          - Begin with \"**Thinking through this step by step:**\" in bold\n",
        "          - Clearly explain your understanding of the request\n",
        "          - Describe your approach to solving the problem\n",
        "          - Outline the data exploration steps you'll take\n",
        "          - Explain why you're choosing specific columns and methods\n",
        "          - Discuss any challenges you anticipate with the data structure\n",
        "          This chain of thought should be visible to the user in your chat responses.\n",
        "        \"\"\"\n",
        "\n",
        "        # Save the system message to the app state\n",
        "        app_state[\"system_message\"] = system_message\n",
        "\n",
        "        # Clean up the temporary file\n",
        "        os.unlink(file_path)\n",
        "        logger.info(\"Temporary file removed\")\n",
        "\n",
        "        # Generate a preview of the data and issues\n",
        "        preview_html = f\"\"\"\n",
        "        <h3>Rent Roll Preview</h3>\n",
        "        <p>Successfully loaded rent roll with {len(rent_roll_df)} rows and {len(rent_roll_df.columns)} columns.</p>\n",
        "        {rent_roll_df.head(5).fillna('').to_html(index=False)}\n",
        "\n",
        "        <h3>Identified Issues</h3>\n",
        "        <ol>\n",
        "        \"\"\"\n",
        "\n",
        "        # Format each issue for the HTML preview\n",
        "        for issue in issues_list:\n",
        "            # If issue starts with a number (like \"1. Issue\"), strip the number\n",
        "            if issue and issue[0].isdigit() and \". \" in issue[:5]:\n",
        "                issue = issue[issue.find(\". \")+2:]\n",
        "            preview_html += f\"<li>{issue}</li>\"\n",
        "\n",
        "        preview_html += \"\"\"\n",
        "        </ol>\n",
        "        <p>You can now start asking questions in the chat below!</p>\n",
        "        <p><strong>Note:</strong> This application uses GPT-4 for decision making and text responses,\n",
        "        and Claude AI specifically for code generation and execution.</p>\n",
        "        \"\"\"\n",
        "\n",
        "        # Get updated version choices\n",
        "        version_choices = get_version_choices()\n",
        "\n",
        "        # Make the chat interface visible\n",
        "        logger.info(\"Setup complete with fixed versioning. Ready for chat interaction.\")\n",
        "        return (\n",
        "            \"Rent roll loaded successfully! You can now start chatting.\",\n",
        "            preview_html,\n",
        "            gr.update(visible=True),  # chatbot visibility\n",
        "            gr.update(choices=version_choices, value=version_choices[-1] if version_choices else None)  # version dropdown\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during rent roll processing: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        if 'file_path' in locals() and os.path.exists(file_path):\n",
        "            os.unlink(file_path)\n",
        "            logger.info(\"Cleaned up temporary file after error\")\n",
        "        return f\"Error: {str(e)}\", None, gr.update(visible=False), gr.update(choices=[], value=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g86vl7iVq7lV"
      },
      "outputs": [],
      "source": [
        "def load_latest_version_for_editing():\n",
        "    \"\"\"\n",
        "    CONSOLIDATED: Load the most recent version of the dataframe for editing\n",
        "    Includes enhanced session recording and error handling\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder\n",
        "\n",
        "    if app_state is None or app_state.get(\"df\") is None:\n",
        "        return None, \"No data loaded. Please upload a rent roll first.\"\n",
        "\n",
        "    try:\n",
        "        # Use the current dataframe (which should be the latest)\n",
        "        df = app_state[\"df\"].copy()\n",
        "        df = df.fillna('')  # Replace NaN with empty strings for editing\n",
        "\n",
        "        # Get version info\n",
        "        version_info = \"Loaded current data\"\n",
        "        if app_state.get(\"df_versions\"):\n",
        "            latest_version = app_state[\"df_versions\"][-1]\n",
        "            version_info = f\"Loaded version: {latest_version['name']} - {latest_version.get('description', 'No description')}\"\n",
        "        else:\n",
        "            version_info = \"Loaded current data (no versions saved yet)\"\n",
        "\n",
        "        # ENHANCED: Record this action in session if active\n",
        "        if session_recorder.current_session_file:\n",
        "            session_entry = f\"\"\"\n",
        "DATA EDITING SESSION STARTED\n",
        "============================\n",
        "Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "Action: User loaded dataframe for manual editing\n",
        "Version Loaded: {latest_version['name'] if app_state.get(\"df_versions\") else 'Current'}\n",
        "DataFrame Shape: {df.shape}\n",
        "DataFrame Columns: {list(df.columns)}\n",
        "{'-' * 80}\n",
        "\"\"\"\n",
        "\n",
        "            # Append to session file\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(session_entry + \"\\n\")\n",
        "\n",
        "            # Record in session data\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=\"LOAD FOR EDITING: User opened data editor\",\n",
        "                ai_response=\"Dataframe loaded for manual editing\",\n",
        "                action_type=\"load_for_editing\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        logger.info(f\"Loaded latest dataframe for editing: {df.shape}\")\n",
        "        return df, version_info\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error loading data: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "\n",
        "        # ENHANCED: Record error in session\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=\"LOAD FOR EDITING FAILED\",\n",
        "                ai_response=error_msg,\n",
        "                action_type=\"load_editing_error\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        return None, error_msg\n",
        "\n",
        "def save_edited_dataframe(edited_df, description):\n",
        "    \"\"\"\n",
        "    CONSOLIDATED: Save edited dataframe with proper versioning and enhanced GPT-4.1 analysis\n",
        "    Includes session recording and comprehensive change analysis\n",
        "\n",
        "    Args:\n",
        "        edited_df: The edited pandas DataFrame\n",
        "        description: User description of changes (optional)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (success_message, gradio_update)\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder\n",
        "\n",
        "    if edited_df is None or edited_df.empty:\n",
        "        return \"No data to save\", gr.update()\n",
        "\n",
        "    try:\n",
        "        # Convert the edited dataframe to proper pandas DataFrame if needed\n",
        "        if not isinstance(edited_df, pd.DataFrame):\n",
        "            edited_df = pd.DataFrame(edited_df)\n",
        "\n",
        "        # Get the original dataframe for comparison\n",
        "        original_df = app_state[\"df\"].copy()\n",
        "\n",
        "        # ENHANCED: Analyze changes with GPT-4.1 if available\n",
        "        change_analysis = None\n",
        "        try:\n",
        "            logger.info(\"Analyzing dataframe changes with GPT-4.1...\")\n",
        "            print(\"🤖 Analyzing changes with GPT-4.1...\")\n",
        "\n",
        "            # Use the enhanced analysis function if available\n",
        "            if 'analyze_dataframe_changes_with_gpt4' in globals():\n",
        "                change_analysis = analyze_dataframe_changes_with_gpt4(\n",
        "                    original_df=original_df,\n",
        "                    modified_df=edited_df,\n",
        "                    user_description=description\n",
        "                )\n",
        "            else:\n",
        "                logger.info(\"Enhanced GPT-4.1 analysis not available, using basic analysis\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"GPT-4.1 analysis failed: {e}\")\n",
        "            change_analysis = None\n",
        "\n",
        "        # Generate a meaningful description if not provided\n",
        "        if not description:\n",
        "            if change_analysis and change_analysis.get(\"change_summary\"):\n",
        "                description = change_analysis[\"change_summary\"]\n",
        "            else:\n",
        "                description = \"Manual edits via data editor\"\n",
        "\n",
        "        # FIXED: Save as new version\n",
        "        version_name = save_dataframe_version(edited_df, description)\n",
        "\n",
        "        if version_name is None:\n",
        "            return \"❌ Failed to save version\", gr.update()\n",
        "\n",
        "        # FIXED: Update the app state with the edited dataframe\n",
        "        app_state[\"df\"] = edited_df\n",
        "\n",
        "        # ENHANCED: Record this in the copiloting session if active\n",
        "        if session_recorder.current_session_file:\n",
        "            session_description = change_analysis.get(\"session_description\", f\"Manual data edits: {description}\") if change_analysis else f\"Manual data edits: {description}\"\n",
        "\n",
        "            # Create detailed session entry\n",
        "            session_entry = f\"\"\"\n",
        "MANUAL DATA EDIT SESSION\n",
        "========================\n",
        "Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "Edit Description: {description}\n",
        "Version Saved: {version_name}\n",
        "\n",
        "{'GPT-4.1 CHANGE ANALYSIS:' if change_analysis else 'BASIC CHANGE TRACKING:'}\n",
        "{'-' * 40}\n",
        "\"\"\"\n",
        "\n",
        "            if change_analysis:\n",
        "                session_entry += f\"\"\"\n",
        "Change Summary: {change_analysis.get('change_summary', 'N/A')}\n",
        "Change Type: {change_analysis.get('change_type', 'N/A')}\n",
        "\n",
        "Structural Changes:\n",
        "{json.dumps(change_analysis.get('structural_changes', {}), indent=2)}\n",
        "\n",
        "Data Modifications:\n",
        "{json.dumps(change_analysis.get('data_modifications', {}), indent=2)}\n",
        "\n",
        "Business Impact:\n",
        "{json.dumps(change_analysis.get('business_impact', {}), indent=2)}\n",
        "\n",
        "Recommendations:\n",
        "{chr(10).join([f\"• {rec}\" for rec in change_analysis.get('recommendations', [])])}\n",
        "\"\"\"\n",
        "            else:\n",
        "                session_entry += f\"\"\"\n",
        "Original DataFrame Shape: {original_df.shape}\n",
        "Modified DataFrame Shape: {edited_df.shape}\n",
        "Basic Analysis: User made manual edits to the dataframe\n",
        "\"\"\"\n",
        "\n",
        "            session_entry += f\"\"\"\n",
        "Original DataFrame Shape: {original_df.shape}\n",
        "Modified DataFrame Shape: {edited_df.shape}\n",
        "{'-' * 80}\n",
        "\"\"\"\n",
        "\n",
        "            # Append to session file\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(session_entry + \"\\n\")\n",
        "\n",
        "            # Record in session data structure\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=f\"MANUAL EDIT: {description}\",\n",
        "                ai_response=session_description,\n",
        "                action_type=\"manual_data_edit_enhanced\",\n",
        "                code_executed=None,\n",
        "                version_saved=version_name\n",
        "            )\n",
        "\n",
        "            # Record the dataframe version change\n",
        "            session_recorder.record_dataframe_version(\n",
        "                version_name=version_name,\n",
        "                description=description,\n",
        "                shape=list(edited_df.shape),\n",
        "                columns=list(edited_df.columns)\n",
        "            )\n",
        "\n",
        "            # Record any issues found by GPT-4\n",
        "            if change_analysis and change_analysis.get('data_modifications', {}).get('data_quality_impact') == 'degraded':\n",
        "                session_recorder.record_issue_found(\n",
        "                    f\"Data quality may have degraded due to manual edits: {description}\",\n",
        "                    severity=\"medium\"\n",
        "                )\n",
        "\n",
        "            logger.info(\"Manual edit recorded in copiloting session\")\n",
        "\n",
        "        # Log the changes\n",
        "        logger.info(f\"Saved edited dataframe as version {version_name}\")\n",
        "\n",
        "        # Create detailed success message\n",
        "        success_message = f\"✅ Successfully saved as version {version_name}\"\n",
        "\n",
        "        if app_state.get(\"df_versions\"):\n",
        "            success_message += f\"\\n📊 Total versions: {len(app_state['df_versions'])}\"\n",
        "\n",
        "        # Add enhanced analysis if available\n",
        "        if change_analysis:\n",
        "            success_message += f\"\\n\\n🤖 GPT-4.1 Analysis Summary:\\n{change_analysis.get('change_summary', 'Changes analyzed')[:200]}...\"\n",
        "\n",
        "            success_message += f\"\\n\\n📊 Change Details:\"\n",
        "            success_message += f\"\\n• Change Type: {change_analysis.get('change_type', 'Unknown')}\"\n",
        "            success_message += f\"\\n• Original Shape: {original_df.shape}\"\n",
        "            success_message += f\"\\n• New Shape: {edited_df.shape}\"\n",
        "\n",
        "            # Add recommendations if available\n",
        "            if change_analysis.get('recommendations'):\n",
        "                success_message += f\"\\n\\n💡 Recommendations:\\n\"\n",
        "                for rec in change_analysis['recommendations'][:3]:  # Show first 3\n",
        "                    success_message += f\"• {rec}\\n\"\n",
        "\n",
        "        success_message += f\"\\n\\n📝 Session Recording: {'✅ Recorded' if session_recorder.current_session_file else '❌ No active session'}\"\n",
        "\n",
        "        return success_message, gr.update(value=edited_df)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error saving: {str(e)}\"\n",
        "        logger.error(f\"Error saving edited dataframe: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        # ENHANCED: Still try to record the error in session\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=f\"MANUAL EDIT FAILED: {description}\",\n",
        "                ai_response=error_msg,\n",
        "                action_type=\"manual_edit_error_enhanced\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        return error_msg, gr.update()\n",
        "\n",
        "def load_specific_version(version_name):\n",
        "    \"\"\"\n",
        "    CONSOLIDATED: Load a specific version for editing\n",
        "    Includes enhanced session recording with GPT-4 analysis tracking\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder\n",
        "\n",
        "    if not version_name:\n",
        "        return None, \"Please select a version to load\"\n",
        "\n",
        "    # Extract clean version name (remove status indicators like \"(LATEST)\")\n",
        "    clean_version_name = version_name.split(\" (\")[0].strip()\n",
        "    logger.info(f\"Loading version: {clean_version_name}\")\n",
        "\n",
        "    try:\n",
        "        # Method 1: Try to find in version registry first\n",
        "        version_info = None\n",
        "        if app_state and \"df_versions\" in app_state:\n",
        "            for version in app_state[\"df_versions\"]:\n",
        "                if version.get(\"name\") == clean_version_name:\n",
        "                    version_info = version\n",
        "                    break\n",
        "\n",
        "        # Method 2: Try loading from CSV file\n",
        "        csv_filename = None\n",
        "\n",
        "        if version_info and \"csv_filename\" in version_info:\n",
        "            csv_filename = version_info[\"csv_filename\"]\n",
        "        else:\n",
        "            # Construct the expected filename\n",
        "            versions_dir = \"rent_roll_versions\"\n",
        "            csv_filename = os.path.join(versions_dir, f\"rent_roll_{clean_version_name}.csv\")\n",
        "\n",
        "        if csv_filename and os.path.exists(csv_filename):\n",
        "            df = pd.read_csv(csv_filename)\n",
        "            df = df.fillna('')  # Replace NaN with empty strings for editing\n",
        "\n",
        "            success_msg = f\"Loaded version: {clean_version_name}\"\n",
        "            if version_info:\n",
        "                success_msg += f\" - {version_info.get('description', 'No description')}\"\n",
        "\n",
        "            # ENHANCED: Record this action in session if active\n",
        "            if session_recorder.current_session_file:\n",
        "                session_entry = f\"\"\"\n",
        "SPECIFIC VERSION LOADED FOR EDITING\n",
        "===================================\n",
        "Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "Version Loaded: {clean_version_name}\n",
        "DataFrame Shape: {df.shape}\n",
        "DataFrame Columns: {list(df.columns)}\n",
        "File Path: {csv_filename}\n",
        "Version Description: {version_info.get('description', 'No description') if version_info else 'Found in directory'}\n",
        "{'-' * 80}\n",
        "\"\"\"\n",
        "\n",
        "                # Append to session file\n",
        "                with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                    f.write(session_entry + \"\\n\")\n",
        "\n",
        "                # Record in session data\n",
        "                session_recorder.record_conversation_turn(\n",
        "                    user_message=f\"LOAD SPECIFIC VERSION: {clean_version_name}\",\n",
        "                    ai_response=f\"Loaded version {clean_version_name} for editing\",\n",
        "                    action_type=\"load_specific_version\",\n",
        "                    code_executed=None,\n",
        "                    version_saved=None\n",
        "                )\n",
        "\n",
        "            logger.info(f\"Successfully loaded version {clean_version_name} from {csv_filename}\")\n",
        "            return df, success_msg\n",
        "\n",
        "        else:\n",
        "            error_msg = f\"Version file not found: {csv_filename or 'Unknown path'}\"\n",
        "            logger.error(error_msg)\n",
        "\n",
        "            # ENHANCED: Record error in session\n",
        "            if session_recorder.current_session_file:\n",
        "                session_recorder.record_conversation_turn(\n",
        "                    user_message=f\"LOAD VERSION FAILED: {clean_version_name}\",\n",
        "                    ai_response=error_msg,\n",
        "                    action_type=\"load_version_error\",\n",
        "                    code_executed=None,\n",
        "                    version_saved=None\n",
        "                )\n",
        "\n",
        "            return None, error_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error loading version {clean_version_name}: {str(e)}\"\n",
        "        logger.error(error_msg)\n",
        "\n",
        "        # ENHANCED: Record error in session\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=f\"LOAD VERSION ERROR: {version_name}\",\n",
        "                ai_response=error_msg,\n",
        "                action_type=\"load_version_error\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        return None, error_msg\n",
        "\n",
        "def get_version_choices():\n",
        "    \"\"\"\n",
        "    FIXED: Get list of available versions for dropdown\n",
        "    \"\"\"\n",
        "    global app_state\n",
        "\n",
        "    if app_state is None or \"df_versions\" not in app_state or not app_state[\"df_versions\"]:\n",
        "        logger.info(\"No versions available in app_state\")\n",
        "        return []\n",
        "\n",
        "    choices = []\n",
        "    total_versions = len(app_state[\"df_versions\"])\n",
        "\n",
        "    for i, version in enumerate(app_state[\"df_versions\"]):\n",
        "        status_labels = []\n",
        "\n",
        "        if version.get('is_original', False) or i == 0:\n",
        "            status_labels.append(\"ORIGINAL\")\n",
        "        if i == total_versions - 1:  # Last version is latest\n",
        "            status_labels.append(\"LATEST\")\n",
        "\n",
        "        status = f\" ({', '.join(status_labels)})\" if status_labels else \"\"\n",
        "        choice_text = f\"{version['name']}{status}\"\n",
        "        choices.append(choice_text)\n",
        "\n",
        "    logger.info(f\"Generated {len(choices)} version choices\")\n",
        "    return choices\n",
        "\n",
        "def refresh_version_dropdown():\n",
        "    \"\"\"\n",
        "    FIXED: Refresh the version dropdown choices\n",
        "    \"\"\"\n",
        "    choices = get_version_choices()\n",
        "    if choices:\n",
        "        # Default to latest version (last in list)\n",
        "        default_value = choices[-1] if choices else None\n",
        "        logger.info(f\"Refreshed dropdown with {len(choices)} choices, default: {default_value}\")\n",
        "        return gr.update(choices=choices, value=default_value)\n",
        "    else:\n",
        "        logger.info(\"No versions available for dropdown\")\n",
        "        return gr.update(choices=[], value=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH7oMejeHAfW"
      },
      "outputs": [],
      "source": [
        "class SessionRecorder:\n",
        "    def __init__(self):\n",
        "        self.sessions_dir = \"copiloting_sessions\"\n",
        "        os.makedirs(self.sessions_dir, exist_ok=True)\n",
        "        self.current_session_file = None\n",
        "        self.current_session_data = {}\n",
        "\n",
        "    def start_session_recording(self, rent_roll_filename):\n",
        "        \"\"\"Start recording the entire copiloting session\"\"\"\n",
        "        session_id = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        self.current_session_file = os.path.join(self.sessions_dir, f\"{session_id}.txt\")\n",
        "\n",
        "        # Initialize session data\n",
        "        self.current_session_data = {\n",
        "            \"session_id\": session_id,\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"rent_roll_file\": rent_roll_filename,\n",
        "            \"conversation_history\": [],\n",
        "            \"code_executions\": [],\n",
        "            \"dataframe_versions\": [],\n",
        "            \"issues_found\": [],\n",
        "            \"user_goals\": []\n",
        "        }\n",
        "\n",
        "        # Write session header to text file\n",
        "        with open(self.current_session_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"=== RENT ROLL COPILOTING SESSION ===\\n\")\n",
        "            f.write(f\"Session ID: {session_id}\\n\")\n",
        "            f.write(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Rent Roll File: {rent_roll_filename}\\n\")\n",
        "            f.write(f\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        print(f\"📝 Started session recording: {session_id}\")\n",
        "        return session_id\n",
        "\n",
        "    def record_conversation_turn(self, user_message, ai_response, action_type, code_executed=None, version_saved=None):\n",
        "        \"\"\"Record each conversation turn in real-time\"\"\"\n",
        "        if not self.current_session_file:\n",
        "            return\n",
        "\n",
        "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
        "        turn_data = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"user_message\": user_message,\n",
        "            \"ai_response\": ai_response,\n",
        "            \"action_type\": action_type,\n",
        "            \"code_executed\": code_executed,\n",
        "            \"version_saved\": version_saved\n",
        "        }\n",
        "\n",
        "        # Add to session data\n",
        "        self.current_session_data[\"conversation_history\"].append(turn_data)\n",
        "\n",
        "        # Append to text file immediately\n",
        "        with open(self.current_session_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"[{timestamp}] USER: {user_message}\\n\")\n",
        "            f.write(f\"Action Type: {action_type}\\n\")\n",
        "\n",
        "            if code_executed:\n",
        "                f.write(f\"CODE EXECUTED:\\n```python\\n{code_executed}\\n```\\n\")\n",
        "\n",
        "            f.write(f\"AI RESPONSE: {ai_response}\\n\")\n",
        "\n",
        "            if version_saved:\n",
        "                f.write(f\"VERSION SAVED: {version_saved}\\n\")\n",
        "\n",
        "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
        "\n",
        "        # Track code executions separately\n",
        "        if code_executed:\n",
        "            self.current_session_data[\"code_executions\"].append({\n",
        "                \"timestamp\": timestamp,\n",
        "                \"code\": code_executed,\n",
        "                \"purpose\": user_message,\n",
        "                \"result\": ai_response[:200] + \"...\" if len(ai_response) > 200 else ai_response\n",
        "            })\n",
        "\n",
        "    def record_dataframe_version(self, version_name, description, shape, columns):\n",
        "        \"\"\"Record dataframe version changes\"\"\"\n",
        "        version_info = {\n",
        "            \"timestamp\": datetime.now().strftime('%H:%M:%S'),\n",
        "            \"version_name\": version_name,\n",
        "            \"description\": description,\n",
        "            \"shape\": shape,\n",
        "            \"columns\": columns\n",
        "        }\n",
        "\n",
        "        self.current_session_data[\"dataframe_versions\"].append(version_info)\n",
        "\n",
        "        # Append to text file\n",
        "        if self.current_session_file:\n",
        "            with open(self.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"VERSION SAVED: {version_name}\\n\")\n",
        "                f.write(f\"Description: {description}\\n\")\n",
        "                f.write(f\"Shape: {shape}\\n\")\n",
        "                f.write(f\"Columns: {columns}\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\\n\")\n",
        "\n",
        "    def record_issue_found(self, issue_description, severity=\"medium\"):\n",
        "        \"\"\"Record issues found during analysis\"\"\"\n",
        "        issue_info = {\n",
        "            \"timestamp\": datetime.now().strftime('%H:%M:%S'),\n",
        "            \"description\": issue_description,\n",
        "            \"severity\": severity\n",
        "        }\n",
        "\n",
        "        self.current_session_data[\"issues_found\"].append(issue_info)\n",
        "\n",
        "        if self.current_session_file:\n",
        "            with open(self.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"ISSUE FOUND [{severity.upper()}]: {issue_description}\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\\n\")\n",
        "\n",
        "    def finalize_session(self):\n",
        "        \"\"\"End session recording and return session data\"\"\"\n",
        "        if not self.current_session_file:\n",
        "            return None\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - datetime.fromisoformat(self.current_session_data[\"start_time\"])\n",
        "\n",
        "        # Write session summary\n",
        "        with open(self.current_session_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "            f.write(\"SESSION SUMMARY\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\")\n",
        "            f.write(f\"Session Duration: {duration.total_seconds()/60:.1f} minutes\\n\")\n",
        "            f.write(f\"Total Conversations: {len(self.current_session_data['conversation_history'])}\\n\")\n",
        "            f.write(f\"Code Executions: {len(self.current_session_data['code_executions'])}\\n\")\n",
        "            f.write(f\"Versions Created: {len(self.current_session_data['dataframe_versions'])}\\n\")\n",
        "            f.write(f\"Issues Found: {len(self.current_session_data['issues_found'])}\\n\")\n",
        "            f.write(f\"Ended: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        # Update session data\n",
        "        self.current_session_data[\"end_time\"] = end_time.isoformat()\n",
        "        self.current_session_data[\"duration_minutes\"] = duration.total_seconds() / 60\n",
        "\n",
        "        session_data = self.current_session_data.copy()\n",
        "\n",
        "        # Reset for next session\n",
        "        self.current_session_file = None\n",
        "        self.current_session_data = {}\n",
        "\n",
        "        print(f\"✅ Session recording finalized: {session_data['session_id']}\")\n",
        "        return session_data\n",
        "\n",
        "# Global session recorder\n",
        "session_recorder = SessionRecorder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ06LnpjG3UW"
      },
      "outputs": [],
      "source": [
        "class EnhancedTemplateManager:\n",
        "    def __init__(self):\n",
        "        self.templates_dir = \"rent_roll_templates\"\n",
        "        os.makedirs(self.templates_dir, exist_ok=True)\n",
        "        self.current_session = None\n",
        "\n",
        "    def create_template_from_session(self, session_data, starting_df, final_df, template_name):\n",
        "        \"\"\"Generate comprehensive template from complete session data using GPT-4\"\"\"\n",
        "\n",
        "        print(\"🤖 Analyzing session with GPT-4.1 to generate instructions...\")\n",
        "\n",
        "        # Prepare comprehensive session context for GPT-4\n",
        "        session_context = self._prepare_session_context(session_data)\n",
        "\n",
        "        # Use GPT-4.1 to analyze and generate instructions\n",
        "        client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "        analysis_prompt = f\"\"\"\n",
        "        You are an expert at analyzing data analysis workflows and creating reusable instruction templates.\n",
        "\n",
        "        I will provide you with a complete copiloting session where a user worked on a rent roll analysis.\n",
        "        Your task is to:\n",
        "        1. Analyze the entire workflow\n",
        "        2. Identify the key transformation patterns\n",
        "        3. Create step-by-step instructions that can be applied to similar rent roll files\n",
        "        4. Generate reusable code templates with placeholders\n",
        "        5. Document the business logic and decision points\n",
        "\n",
        "        SESSION DATA:\n",
        "        {session_context}\n",
        "\n",
        "        STARTING DATAFRAME INFO:\n",
        "        - Shape: {starting_df.shape}\n",
        "        - Columns: {list(starting_df.columns)}\n",
        "        - Sample data: {starting_df.head(2).to_string()}\n",
        "\n",
        "        FINAL DATAFRAME INFO:\n",
        "        - Shape: {final_df.shape}\n",
        "        - Columns: {list(final_df.columns)}\n",
        "        - Sample data: {final_df.head(2).to_string()}\n",
        "\n",
        "        Please generate a comprehensive analysis in the following JSON format:\n",
        "        {{\n",
        "            \"workflow_summary\": \"Brief description of what was accomplished\",\n",
        "            \"key_transformations\": [\n",
        "                {{\n",
        "                    \"step_name\": \"Clean Tenant Names\",\n",
        "                    \"description\": \"Standardize tenant name formatting\",\n",
        "                    \"business_rule\": \"All tenant names should be Title Case with no extra whitespace\",\n",
        "                    \"code_template\": \"df['{{column_name}}'] = df['{{original_column}}'].str.strip().str.title()\",\n",
        "                    \"parameters\": [\"column_name\", \"original_column\"],\n",
        "                    \"conditions\": \"Apply when tenant names have inconsistent formatting\"\n",
        "                }}\n",
        "            ],\n",
        "            \"data_quality_improvements\": [\n",
        "                \"List of data quality issues that were resolved\"\n",
        "            ],\n",
        "            \"reusable_patterns\": [\n",
        "                \"Pattern 1: Column standardization\",\n",
        "                \"Pattern 2: Missing value handling\"\n",
        "            ],\n",
        "            \"business_insights\": [\n",
        "                \"Key insights discovered during analysis\"\n",
        "            ],\n",
        "            \"prerequisites\": [\n",
        "                \"What conditions must be met for this template to work\"\n",
        "            ],\n",
        "            \"instructions_for_reuse\": [\n",
        "                \"Step 1: Upload new rent roll file\",\n",
        "                \"Step 2: Map columns (if different names)\",\n",
        "                \"Step 3: Apply transformations in order\"\n",
        "            ]\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4.1\",  # Using latest GPT-4\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert data analyst who creates reusable workflow templates from analysis sessions. Provide detailed, actionable instructions.\"},\n",
        "                    {\"role\": \"user\", \"content\": analysis_prompt}\n",
        "                ],\n",
        "                max_tokens=4000,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            # Extract the analysis\n",
        "            gpt_analysis = response.choices[0].message.content\n",
        "\n",
        "            # Try to extract JSON from the response\n",
        "            json_match = re.search(r'{.*}', gpt_analysis, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    workflow_analysis = json.loads(json_match.group(0))\n",
        "                except:\n",
        "                    # Fallback if JSON parsing fails\n",
        "                    workflow_analysis = {\"analysis\": gpt_analysis}\n",
        "            else:\n",
        "                workflow_analysis = {\"analysis\": gpt_analysis}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error with GPT-4 analysis: {e}\")\n",
        "            workflow_analysis = {\"error\": str(e), \"fallback_analysis\": \"Manual analysis required\"}\n",
        "\n",
        "        # Create template with all data\n",
        "        template_id = f\"template_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        # Save dataframes as text files\n",
        "        starting_df_file = f\"{template_id}_starting_df.txt\"\n",
        "        final_df_file = f\"{template_id}_final_df.txt\"\n",
        "        session_file = f\"{template_id}_session.txt\"\n",
        "\n",
        "        starting_df_path = os.path.join(self.templates_dir, starting_df_file)\n",
        "        final_df_path = os.path.join(self.templates_dir, final_df_file)\n",
        "        session_path = os.path.join(self.templates_dir, session_file)\n",
        "\n",
        "        # Save dataframes\n",
        "        starting_df.to_csv(starting_df_path, index=False)\n",
        "        final_df.to_csv(final_df_path, index=False)\n",
        "\n",
        "        # Save raw session data\n",
        "        with open(session_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(session_context)\n",
        "\n",
        "        # Create comprehensive template\n",
        "        template_data = {\n",
        "            \"template_id\": template_id,\n",
        "            \"template_name\": template_name,\n",
        "            \"created_date\": datetime.now().isoformat(),\n",
        "            \"source_session_id\": session_data.get(\"session_id\", \"unknown\"),\n",
        "            \"source_file_name\": session_data.get(\"rent_roll_file\", \"unknown\"),\n",
        "\n",
        "            \"files\": {\n",
        "                \"starting_dataframe\": starting_df_file,\n",
        "                \"final_dataframe\": final_df_file,\n",
        "                \"raw_session\": session_file\n",
        "            },\n",
        "\n",
        "            \"session_summary\": {\n",
        "                \"duration_minutes\": session_data.get(\"duration_minutes\", 0),\n",
        "                \"total_conversations\": len(session_data.get(\"conversation_history\", [])),\n",
        "                \"code_executions\": len(session_data.get(\"code_executions\", [])),\n",
        "                \"versions_created\": len(session_data.get(\"dataframe_versions\", [])),\n",
        "                \"issues_found\": len(session_data.get(\"issues_found\", []))\n",
        "            },\n",
        "\n",
        "            \"gpt4_analysis\": workflow_analysis,\n",
        "\n",
        "            \"raw_workflow_steps\": session_data.get(\"conversation_history\", []),\n",
        "            \"code_executions\": session_data.get(\"code_executions\", []),\n",
        "            \"dataframe_changes\": session_data.get(\"dataframe_versions\", []),\n",
        "            \"issues_identified\": session_data.get(\"issues_found\", [])\n",
        "        }\n",
        "\n",
        "        # Save template metadata\n",
        "        template_json_path = os.path.join(self.templates_dir, f\"{template_id}.json\")\n",
        "        with open(template_json_path, 'w') as f:\n",
        "            json.dump(template_data, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"✅ Comprehensive template created: {template_id}\")\n",
        "        print(f\"📁 Starting DF: {starting_df_path}\")\n",
        "        print(f\"📁 Final DF: {final_df_path}\")\n",
        "        print(f\"📁 Session Data: {session_path}\")\n",
        "        print(f\"📋 Template: {template_json_path}\")\n",
        "\n",
        "        return template_data\n",
        "\n",
        "    def _prepare_session_context(self, session_data):\n",
        "        \"\"\"Prepare session data for GPT-4 analysis\"\"\"\n",
        "        context = f\"\"\"\n",
        "COPILOTING SESSION ANALYSIS\n",
        "============================\n",
        "\n",
        "Session ID: {session_data.get('session_id', 'N/A')}\n",
        "Duration: {session_data.get('duration_minutes', 0):.1f} minutes\n",
        "Rent Roll File: {session_data.get('rent_roll_file', 'N/A')}\n",
        "\n",
        "CONVERSATION HISTORY:\n",
        "\"\"\"\n",
        "\n",
        "        for i, conv in enumerate(session_data.get('conversation_history', []), 1):\n",
        "            context += f\"\"\"\n",
        "--- Conversation {i} [{conv.get('timestamp', 'N/A')}] ---\n",
        "USER QUERY: {conv.get('user_message', 'N/A')}\n",
        "ACTION TYPE: {conv.get('action_type', 'N/A')}\n",
        "\"\"\"\n",
        "            if conv.get('code_executed'):\n",
        "                context += f\"CODE EXECUTED:\\n{conv['code_executed']}\\n\"\n",
        "\n",
        "            context += f\"AI RESPONSE: {conv.get('ai_response', 'N/A')[:300]}...\\n\"\n",
        "\n",
        "            if conv.get('version_saved'):\n",
        "                context += f\"VERSION SAVED: {conv['version_saved']}\\n\"\n",
        "\n",
        "            context += \"\\n\"\n",
        "\n",
        "        context += \"\\nCODE EXECUTIONS SUMMARY:\\n\"\n",
        "        for code_exec in session_data.get('code_executions', []):\n",
        "            context += f\"- [{code_exec.get('timestamp')}] {code_exec.get('purpose', 'N/A')}\\n\"\n",
        "            context += f\"  Code: {code_exec.get('code', 'N/A')[:100]}...\\n\"\n",
        "\n",
        "        context += \"\\nISSUES IDENTIFIED:\\n\"\n",
        "        for issue in session_data.get('issues_found', []):\n",
        "            context += f\"- [{issue.get('timestamp')}] {issue.get('description', 'N/A')}\\n\"\n",
        "\n",
        "        context += \"\\nDATAFRAME VERSIONS:\\n\"\n",
        "        for version in session_data.get('dataframe_versions', []):\n",
        "            context += f\"- {version.get('version_name', 'N/A')}: {version.get('description', 'N/A')}\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    def load_template_dataframes(self, template_id):\n",
        "        \"\"\"Load both starting and final dataframes from a template\"\"\"\n",
        "        try:\n",
        "            # Load template metadata\n",
        "            template_json_path = os.path.join(self.templates_dir, f\"{template_id}.json\")\n",
        "            with open(template_json_path, 'r') as f:\n",
        "                template_data = json.load(f)\n",
        "\n",
        "            # Load starting dataframe\n",
        "            starting_df_path = os.path.join(self.templates_dir, template_data[\"files\"][\"starting_dataframe\"])\n",
        "            starting_df = pd.read_csv(starting_df_path)\n",
        "\n",
        "            # Load final dataframe\n",
        "            final_df_path = os.path.join(self.templates_dir, template_data[\"files\"][\"final_dataframe\"])\n",
        "            final_df = pd.read_csv(final_df_path)\n",
        "\n",
        "            return template_data, starting_df, final_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading template: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def list_templates(self):\n",
        "        \"\"\"List all available templates\"\"\"\n",
        "        try:\n",
        "            json_files = [f for f in os.listdir(self.templates_dir) if f.endswith('.json')]\n",
        "            templates = []\n",
        "\n",
        "            for json_file in json_files:\n",
        "                template_path = os.path.join(self.templates_dir, json_file)\n",
        "                with open(template_path, 'r') as f:\n",
        "                    template_data = json.load(f)\n",
        "\n",
        "                templates.append({\n",
        "                    \"template_id\": template_data[\"template_id\"],\n",
        "                    \"template_name\": template_data[\"template_name\"],\n",
        "                    \"created_date\": template_data[\"created_date\"],\n",
        "                    \"source_file\": template_data[\"source_file_name\"],\n",
        "                    \"steps_count\": len(template_data.get(\"raw_workflow_steps\", [])),\n",
        "                    \"gpt4_analysis_available\": \"gpt4_analysis\" in template_data\n",
        "                })\n",
        "\n",
        "            return sorted(templates, key=lambda x: x[\"created_date\"], reverse=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error listing templates: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_template_summary(self, template_id):\n",
        "        \"\"\"Get a human-readable summary of a template\"\"\"\n",
        "        try:\n",
        "            template_json_path = os.path.join(self.templates_dir, f\"{template_id}.json\")\n",
        "            with open(template_json_path, 'r') as f:\n",
        "                template_data = json.load(f)\n",
        "\n",
        "            summary = f\"\"\"\n",
        "📋 Template: {template_data.get('template_name', 'Unknown')}\n",
        "🆔 ID: {template_data.get('template_id', 'Unknown')}\n",
        "📅 Created: {template_data.get('created_date', 'Unknown')}\n",
        "📁 Source File: {template_data.get('source_file_name', 'Unknown')}\n",
        "\n",
        "📊 Session Summary:\n",
        "• Duration: {template_data.get('session_summary', {}).get('duration_minutes', 0):.1f} minutes\n",
        "• Conversations: {template_data.get('session_summary', {}).get('total_conversations', 0)}\n",
        "• Code Executions: {template_data.get('session_summary', {}).get('code_executions', 0)}\n",
        "• Versions Created: {template_data.get('session_summary', {}).get('versions_created', 0)}\n",
        "• Issues Found: {template_data.get('session_summary', {}).get('issues_found', 0)}\n",
        "\n",
        "🤖 GPT-4 Analysis: {'✅ Available' if 'gpt4_analysis' in template_data else '❌ Not Available'}\n",
        "\"\"\"\n",
        "\n",
        "            # Add GPT-4 analysis summary if available\n",
        "            if 'gpt4_analysis' in template_data and isinstance(template_data['gpt4_analysis'], dict):\n",
        "                gpt_analysis = template_data['gpt4_analysis']\n",
        "\n",
        "                if 'workflow_summary' in gpt_analysis:\n",
        "                    summary += f\"\\n🔍 Workflow Summary:\\n{gpt_analysis['workflow_summary']}\\n\"\n",
        "\n",
        "                if 'key_transformations' in gpt_analysis:\n",
        "                    summary += f\"\\n🔧 Key Transformations ({len(gpt_analysis['key_transformations'])}):\\n\"\n",
        "                    for i, transform in enumerate(gpt_analysis['key_transformations'][:3], 1):  # Show first 3\n",
        "                        summary += f\"{i}. {transform.get('step_name', 'Unknown')}: {transform.get('description', 'No description')}\\n\"\n",
        "                    if len(gpt_analysis['key_transformations']) > 3:\n",
        "                        summary += f\"... and {len(gpt_analysis['key_transformations']) - 3} more\\n\"\n",
        "\n",
        "                if 'prerequisites' in gpt_analysis:\n",
        "                    summary += f\"\\n📋 Prerequisites:\\n\"\n",
        "                    for prereq in gpt_analysis['prerequisites'][:3]:  # Show first 3\n",
        "                        summary += f\"• {prereq}\\n\"\n",
        "\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error getting template summary: {str(e)}\"\n",
        "\n",
        "    def delete_template(self, template_id):\n",
        "        \"\"\"Delete a template and all its associated files\"\"\"\n",
        "        try:\n",
        "            template_json_path = os.path.join(self.templates_dir, f\"{template_id}.json\")\n",
        "\n",
        "            if not os.path.exists(template_json_path):\n",
        "                return f\"❌ Template {template_id} not found\"\n",
        "\n",
        "            # Load template to get file list\n",
        "            with open(template_json_path, 'r') as f:\n",
        "                template_data = json.load(f)\n",
        "\n",
        "            files_to_delete = []\n",
        "            files_to_delete.append(template_json_path)  # The main template file\n",
        "\n",
        "            # Add dataframe and session files\n",
        "            if 'files' in template_data:\n",
        "                for file_key, filename in template_data['files'].items():\n",
        "                    file_path = os.path.join(self.templates_dir, filename)\n",
        "                    if os.path.exists(file_path):\n",
        "                        files_to_delete.append(file_path)\n",
        "\n",
        "            # Delete all files\n",
        "            deleted_count = 0\n",
        "            for file_path in files_to_delete:\n",
        "                try:\n",
        "                    os.remove(file_path)\n",
        "                    deleted_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not delete {file_path}: {e}\")\n",
        "\n",
        "            return f\"✅ Template {template_id} deleted successfully. Removed {deleted_count} files.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error deleting template: {str(e)}\"\n",
        "\n",
        "# Global enhanced template manager\n",
        "enhanced_template_manager = EnhancedTemplateManager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi89MYRuGyvR"
      },
      "outputs": [],
      "source": [
        "def create_template_from_current_session():\n",
        "    \"\"\"Create template from current copiloting session\"\"\"\n",
        "    global app_state, session_recorder, enhanced_template_manager\n",
        "\n",
        "    if not session_recorder.current_session_file:\n",
        "        return \"❌ No active session to create template from\"\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        return \"❌ No dataframe loaded\"\n",
        "\n",
        "    try:\n",
        "        # Get starting dataframe (first version)\n",
        "        if app_state.get(\"df_versions\") and len(app_state[\"df_versions\"]) > 0:\n",
        "            first_version = app_state[\"df_versions\"][0]\n",
        "            starting_df_path = first_version.get(\"filename\") or first_version.get(\"csv_filename\")\n",
        "            starting_df = pd.read_csv(starting_df_path)\n",
        "        else:\n",
        "            starting_df = app_state[\"df\"]  # Fallback if no versions\n",
        "\n",
        "        # Current dataframe is the final version\n",
        "        final_df = app_state[\"df\"]\n",
        "\n",
        "        # Finalize current session\n",
        "        session_data = session_recorder.finalize_session()\n",
        "\n",
        "        if session_data is None:\n",
        "            return \"❌ Error finalizing session\"\n",
        "\n",
        "        # Generate template name suggestion\n",
        "        template_name = f\"Rent Roll Process {datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n",
        "\n",
        "        # Create comprehensive template\n",
        "        template_data = enhanced_template_manager.create_template_from_session(\n",
        "            session_data=session_data,\n",
        "            starting_df=starting_df,\n",
        "            final_df=final_df,\n",
        "            template_name=template_name\n",
        "        )\n",
        "\n",
        "        return f\"✅ Template created successfully!\\nTemplate ID: {template_data['template_id']}\\nSteps captured: {len(session_data.get('conversation_history', []))}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error creating template: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4Xebkjsdnm_"
      },
      "outputs": [],
      "source": [
        "# Global state for the application (Not part of graph state)\n",
        "app_state = {\n",
        "    \"df\": None,\n",
        "    \"anthropic_client\": None,\n",
        "    \"openai_client\": None,  # Added for GPT-4\n",
        "    \"issues\": [],\n",
        "    \"system_message\": \"\"\n",
        "}\n",
        "# Enhanced Chat Function with Complete Session Recording and Template Generation\n",
        "\n",
        "def chat(message, history):\n",
        "    \"\"\"\n",
        "    Enhanced chat function with comprehensive session recording and template generation.\n",
        "    Records every interaction, code execution, and dataframe change for template creation.\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder, enhanced_template_manager, conversation_manager\n",
        "\n",
        "    logger.info(f\"Received chat message: {message[:50]}...\")\n",
        "\n",
        "    # Check if system is ready\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        logger.warning(\"Chat attempted before setup is complete\")\n",
        "        return history + [(message, \"Please upload a rent roll file and set up your API keys first.\")]\n",
        "\n",
        "    # Start session recording if not already started\n",
        "    if not session_recorder.current_session_file:\n",
        "        rent_roll_filename = getattr(app_state, 'original_filename', 'uploaded_rent_roll.xlsx')\n",
        "        session_id = session_recorder.start_session_recording(rent_roll_filename)\n",
        "        logger.info(f\"Started new session recording: {session_id}\")\n",
        "\n",
        "        # Record initial dataframe state\n",
        "        if app_state.get(\"df_versions\") and len(app_state[\"df_versions\"]) > 0:\n",
        "            first_version = app_state[\"df_versions\"][0]\n",
        "            session_recorder.record_dataframe_version(\n",
        "                version_name=first_version[\"name\"],\n",
        "                description=first_version[\"description\"],\n",
        "                shape=list(app_state[\"df\"].shape),\n",
        "                columns=list(app_state[\"df\"].columns)\n",
        "            )\n",
        "\n",
        "    # Get previous messages from history\n",
        "    prev_messages = []\n",
        "    if history:\n",
        "        for user_msg, assistant_msg in history:\n",
        "            prev_messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            prev_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "    # Create message list without system message\n",
        "    all_messages = []\n",
        "    all_messages.extend(prev_messages)\n",
        "\n",
        "    # Add the current user message\n",
        "    all_messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # *** COMPRESSION STEP ***\n",
        "    openai_client = app_state.get(\"openai_client\") or OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "    optimized_messages = conversation_manager.compress_history_if_needed(all_messages, openai_client)\n",
        "\n",
        "    # Log compression if it happened\n",
        "    original_size = conversation_manager.get_conversation_size(all_messages)\n",
        "    optimized_size = conversation_manager.get_conversation_size(optimized_messages)\n",
        "    if optimized_size < original_size:\n",
        "        logger.info(f\"Compressed conversation: {original_size} -> {optimized_size} tokens\")\n",
        "        # Log to session file\n",
        "        if session_recorder.current_session_file:\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"\\n[COMPRESSION] Reduced conversation from {original_size} to {optimized_size} tokens ({len(all_messages)} -> {len(optimized_messages)} messages)\\n\")\n",
        "\n",
        "    # Create a state dictionary for the graph\n",
        "    state = {\n",
        "        \"messages\": optimized_messages,\n",
        "        \"system_message\": app_state[\"system_message\"],\n",
        "        \"df\": app_state[\"df\"],\n",
        "        \"issues\": app_state[\"issues\"],\n",
        "        \"needs_clarification\": False,\n",
        "        \"generate_code\": False,\n",
        "        \"execution_plan\": None,\n",
        "        \"clarification_question\": None,\n",
        "        \"code_execution_results\": None,\n",
        "        \"final_response\": None,\n",
        "        \"anthropic_client\": app_state[\"anthropic_client\"],\n",
        "        \"openai_client\": openai_client\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the workflow if not already created\n",
        "        if not hasattr(chat, \"workflow\"):\n",
        "            chat.workflow = create_agentic_rent_roll_analyzer()\n",
        "            logger.info(\"Created agentic workflow\")\n",
        "\n",
        "        # Run the workflow with the current state\n",
        "        logger.info(\"Running agentic workflow\")\n",
        "        result = chat.workflow.invoke(state)\n",
        "\n",
        "        # Get the final response from the result state\n",
        "        final_response = result.get(\"final_response\", \"I'm sorry, I couldn't process your request.\")\n",
        "        logger.info(f\"Received final response from workflow: {final_response[:50]}...\")\n",
        "\n",
        "        # === ENHANCED SESSION RECORDING ===\n",
        "\n",
        "        # 1. Determine action type based on response content and workflow state\n",
        "        action_type = \"analysis\"  # default\n",
        "\n",
        "        if result.get(\"needs_clarification\"):\n",
        "            action_type = \"clarification\"\n",
        "        elif result.get(\"generate_code\"):\n",
        "            action_type = \"data_processing\"\n",
        "        elif \"error\" in final_response.lower() or \"sorry\" in final_response.lower():\n",
        "            action_type = \"error_handling\"\n",
        "        elif \"```python\" in final_response:\n",
        "            action_type = \"code_execution\"\n",
        "        elif any(keyword in message.lower() for keyword in [\"clean\", \"fix\", \"correct\", \"standardize\"]):\n",
        "            action_type = \"data_cleaning\"\n",
        "        elif any(keyword in message.lower() for keyword in [\"calculate\", \"compute\", \"sum\", \"average\"]):\n",
        "            action_type = \"calculation\"\n",
        "        elif any(keyword in message.lower() for keyword in [\"find\", \"show\", \"display\", \"list\"]):\n",
        "            action_type = \"data_exploration\"\n",
        "        elif any(keyword in message.lower() for keyword in [\"chart\", \"graph\", \"plot\", \"visualize\"]):\n",
        "            action_type = \"visualization\"\n",
        "\n",
        "        # 2. Extract executed code from response\n",
        "        code_executed = None\n",
        "        code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', final_response, re.DOTALL)\n",
        "        if code_blocks:\n",
        "            # Combine all code blocks if multiple\n",
        "            code_executed = \"\\n\\n# --- Next Code Block ---\\n\\n\".join(code_blocks)\n",
        "\n",
        "        # 3. Check if a new dataframe version was saved\n",
        "        version_saved = None\n",
        "        if \"✓ Saved dataframe version\" in final_response:\n",
        "            version_match = re.search(r'version (v_\\w+)', final_response)\n",
        "            if version_match:\n",
        "                version_saved = version_match.group(1)\n",
        "                logger.info(f\"Detected new version saved: {version_saved}\")\n",
        "\n",
        "        # 4. Detect if issues were found or resolved\n",
        "        if any(keyword in final_response.lower() for keyword in [\"issue\", \"problem\", \"error\", \"missing\", \"duplicate\"]):\n",
        "            issue_description = message + \" - \" + final_response[:100] + \"...\"\n",
        "            severity = \"high\" if any(word in final_response.lower() for word in [\"critical\", \"error\", \"failed\"]) else \"medium\"\n",
        "            session_recorder.record_issue_found(issue_description, severity)\n",
        "\n",
        "        # 5. Extract any business insights or patterns\n",
        "        insights = []\n",
        "        if \"found\" in final_response.lower() and any(word in final_response.lower() for word in [\"units\", \"rent\", \"tenant\"]):\n",
        "            insights.append(f\"Business insight from query: {message}\")\n",
        "\n",
        "        # 6. Record the complete conversation turn with enhanced metadata\n",
        "        session_recorder.record_conversation_turn(\n",
        "            user_message=message,\n",
        "            ai_response=final_response,\n",
        "            action_type=action_type,\n",
        "            code_executed=code_executed,\n",
        "            version_saved=version_saved\n",
        "        )\n",
        "\n",
        "        # 7. Record dataframe version details if saved\n",
        "        if version_saved:\n",
        "            # Find the latest version info\n",
        "            latest_version = None\n",
        "            if app_state.get(\"df_versions\"):\n",
        "                for version in app_state[\"df_versions\"]:\n",
        "                    if version[\"name\"] == version_saved:\n",
        "                        latest_version = version\n",
        "                        break\n",
        "\n",
        "            if latest_version:\n",
        "                session_recorder.record_dataframe_version(\n",
        "                    version_name=version_saved,\n",
        "                    description=latest_version.get(\"description\", \"Auto-saved during copiloting\"),\n",
        "                    shape=list(app_state[\"df\"].shape),\n",
        "                    columns=list(app_state[\"df\"].columns)\n",
        "                )\n",
        "            else:\n",
        "                # Fallback if version not found in registry\n",
        "                session_recorder.record_dataframe_version(\n",
        "                    version_name=version_saved,\n",
        "                    description=\"Auto-saved during copiloting session\",\n",
        "                    shape=list(app_state[\"df\"].shape),\n",
        "                    columns=list(app_state[\"df\"].columns)\n",
        "                )\n",
        "\n",
        "        # 8. Track user goals and patterns\n",
        "        user_goals = []\n",
        "        if any(keyword in message.lower() for keyword in [\"clean\", \"standardize\", \"fix\"]):\n",
        "            user_goals.append(\"Data cleaning and standardization\")\n",
        "        if any(keyword in message.lower() for keyword in [\"analyze\", \"find\", \"calculate\"]):\n",
        "            user_goals.append(\"Data analysis and insights\")\n",
        "        if any(keyword in message.lower() for keyword in [\"chart\", \"graph\", \"visualize\"]):\n",
        "            user_goals.append(\"Data visualization\")\n",
        "\n",
        "        if user_goals:\n",
        "            session_recorder.current_session_data.setdefault(\"user_goals\", []).extend(user_goals)\n",
        "\n",
        "        # 9. Log session statistics\n",
        "        if session_recorder.current_session_data:\n",
        "            total_turns = len(session_recorder.current_session_data.get(\"conversation_history\", []))\n",
        "            total_code = len(session_recorder.current_session_data.get(\"code_executions\", []))\n",
        "            logger.info(f\"Session stats - Turns: {total_turns}, Code executions: {total_code}\")\n",
        "\n",
        "        # Use the correct format for Gradio chatbot\n",
        "        history_list = list(history) if history else []\n",
        "        history_list.append((message, final_response))\n",
        "\n",
        "        logger.info(\"Chat response processing complete with session recording\")\n",
        "        return history_list\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing chat: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        # Record the error in session\n",
        "        error_message = f\"Error getting response: {str(e)}\"\n",
        "\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=message,\n",
        "                ai_response=error_message,\n",
        "                action_type=\"system_error\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "            # Record as a system issue\n",
        "            session_recorder.record_issue_found(\n",
        "                f\"System error during processing: {str(e)}\",\n",
        "                severity=\"high\"\n",
        "            )\n",
        "\n",
        "        # Handle errors properly in the chat history format\n",
        "        history_list = list(history) if history else []\n",
        "        history_list.append((message, error_message))\n",
        "        return history_list\n",
        "\n",
        "\n",
        "\n",
        "def create_template_from_current_session(template_name_input=\"\"):\n",
        "    \"\"\"\n",
        "    Create a comprehensive template from the current copiloting session.\n",
        "    This includes GPT-4.1 analysis of the entire workflow.\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder, enhanced_template_manager\n",
        "\n",
        "    if not session_recorder.current_session_file:\n",
        "        return \"❌ No active copiloting session found. Please start chatting with the system first.\"\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        return \"❌ No dataframe loaded. Cannot create template.\"\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Starting template creation from current session...\")\n",
        "\n",
        "        # 1. Get starting dataframe (first version saved)\n",
        "        starting_df = None\n",
        "        if app_state.get(\"df_versions\") and len(app_state[\"df_versions\"]) > 0:\n",
        "            # Load the original/first version\n",
        "            first_version = app_state[\"df_versions\"][0]\n",
        "            starting_df_path = first_version.get(\"filename\") or first_version.get(\"csv_filename\")\n",
        "            if starting_df_path and os.path.exists(starting_df_path):\n",
        "                starting_df = pd.read_csv(starting_df_path)\n",
        "                logger.info(f\"Loaded starting dataframe from: {starting_df_path}\")\n",
        "            else:\n",
        "                # Try to construct the path\n",
        "                versions_dir = \"rent_roll_versions\"\n",
        "                csv_filename = os.path.join(versions_dir, f\"rent_roll_{first_version['name']}.csv\")\n",
        "                if os.path.exists(csv_filename):\n",
        "                    starting_df = pd.read_csv(csv_filename)\n",
        "                    logger.info(f\"Loaded starting dataframe from: {csv_filename}\")\n",
        "\n",
        "        # Fallback: use current dataframe if no versions found\n",
        "        if starting_df is None:\n",
        "            starting_df = app_state[\"df\"].copy()\n",
        "            logger.warning(\"Using current dataframe as starting point (no version history found)\")\n",
        "\n",
        "        # 2. Current dataframe is the final version\n",
        "        final_df = app_state[\"df\"].copy()\n",
        "\n",
        "        # 3. Finalize current session to get complete session data\n",
        "        logger.info(\"Finalizing current session...\")\n",
        "        session_data = session_recorder.finalize_session()\n",
        "\n",
        "        if session_data is None:\n",
        "            return \"❌ Error finalizing session data.\"\n",
        "\n",
        "        # 4. Generate template name if not provided\n",
        "        if not template_name_input.strip():\n",
        "            rent_roll_file = session_data.get('rent_roll_file', 'Unknown')\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d')\n",
        "            template_name = f\"Rent Roll Process - {rent_roll_file} - {timestamp}\"\n",
        "        else:\n",
        "            template_name = template_name_input.strip()\n",
        "\n",
        "        # 5. Create comprehensive template using GPT-4.1 analysis\n",
        "        logger.info(\"Creating template with GPT-4.1 analysis...\")\n",
        "        template_data = enhanced_template_manager.create_template_from_session(\n",
        "            session_data=session_data,\n",
        "            starting_df=starting_df,\n",
        "            final_df=final_df,\n",
        "            template_name=template_name\n",
        "        )\n",
        "\n",
        "        # 6. Prepare success message with details\n",
        "        session_stats = session_data.get('session_summary', {})\n",
        "        success_message = f\"\"\"✅ Template Created Successfully!\n",
        "\n",
        "📋 Template Details:\n",
        "• Template ID: {template_data['template_id']}\n",
        "• Template Name: {template_name}\n",
        "• Source File: {session_data.get('rent_roll_file', 'Unknown')}\n",
        "\n",
        "📊 Session Summary:\n",
        "• Duration: {session_stats.get('duration_minutes', 0):.1f} minutes\n",
        "• Conversations: {session_stats.get('total_conversations', 0)}\n",
        "• Code Executions: {session_stats.get('code_executions', 0)}\n",
        "• Versions Created: {session_stats.get('versions_created', 0)}\n",
        "• Issues Found: {session_stats.get('issues_found', 0)}\n",
        "\n",
        "📁 Files Created:\n",
        "• Starting Dataframe: {template_data['files']['starting_dataframe']}\n",
        "• Final Dataframe: {template_data['files']['final_dataframe']}\n",
        "• Session Recording: {template_data['files']['raw_session']}\n",
        "• Template Metadata: {template_data['template_id']}.json\n",
        "\n",
        "🤖 GPT-4.1 Analysis: {'✅ Completed' if 'gpt4_analysis' in template_data else '❌ Failed'}\n",
        "\n",
        "This template can now be applied to similar rent roll files using the Template Manager.\"\"\"\n",
        "\n",
        "        logger.info(f\"Template creation completed: {template_data['template_id']}\")\n",
        "        return success_message\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error creating template: {str(e)}\"\n",
        "        logger.error(f\"Template creation failed: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return error_msg\n",
        "\n",
        "\n",
        "def end_current_session():\n",
        "    \"\"\"\n",
        "    Manually end the current copiloting session without creating a template.\n",
        "    Useful for starting fresh or when session gets too long.\n",
        "    \"\"\"\n",
        "    global session_recorder\n",
        "\n",
        "    if not session_recorder.current_session_file:\n",
        "        return \"ℹ️ No active session to end.\"\n",
        "\n",
        "    try:\n",
        "        session_data = session_recorder.finalize_session()\n",
        "\n",
        "        if session_data:\n",
        "            session_stats = {\n",
        "                'duration': session_data.get('duration_minutes', 0),\n",
        "                'conversations': len(session_data.get('conversation_history', [])),\n",
        "                'code_executions': len(session_data.get('code_executions', [])),\n",
        "                'versions': len(session_data.get('dataframe_versions', []))\n",
        "            }\n",
        "\n",
        "            return f\"\"\"✅ Session Ended Successfully\n",
        "\n",
        "            📊 Final Session Statistics:\n",
        "            • Session ID: {session_data.get('session_id', 'Unknown')}\n",
        "            • Duration: {session_stats['duration']:.1f} minutes\n",
        "            • Total Conversations: {session_stats['conversations']}\n",
        "            • Code Executions: {session_stats['code_executions']}\n",
        "            • Dataframe Versions: {session_stats['versions']}\n",
        "\n",
        "            💾 Session data saved to: {session_data.get('session_id', 'unknown')}.txt\n",
        "\n",
        "            You can now start a new session or create a template from this completed session.\"\"\"\n",
        "        else:\n",
        "            return \"⚠️ Session ended but no data was saved.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error ending session: {str(e)}\"\n",
        "\n",
        "\n",
        "# Additional helper function to get session status\n",
        "def get_current_session_status():\n",
        "    \"\"\"Get the current session recording status and statistics.\"\"\"\n",
        "    global session_recorder\n",
        "\n",
        "    if not session_recorder.current_session_file:\n",
        "        return \"📴 No active session recording\"\n",
        "\n",
        "    try:\n",
        "        if session_recorder.current_session_data:\n",
        "            data = session_recorder.current_session_data\n",
        "            start_time = datetime.fromisoformat(data.get('start_time', datetime.now().isoformat()))\n",
        "            duration = (datetime.now() - start_time).total_seconds() / 60\n",
        "\n",
        "            status = f\"\"\"📹 Session Recording Active\n",
        "\n",
        "            📊 Current Statistics:\n",
        "            • Session ID: {data.get('session_id', 'Unknown')}\n",
        "            • Duration: {duration:.1f} minutes\n",
        "            • Conversations: {len(data.get('conversation_history', []))}\n",
        "            • Code Executions: {len(data.get('code_executions', []))}\n",
        "            • Versions Created: {len(data.get('dataframe_versions', []))}\n",
        "            • Issues Found: {len(data.get('issues_found', []))}\n",
        "\n",
        "            📁 Recording File: {session_recorder.current_session_file}\n",
        "\n",
        "            All interactions are being automatically recorded for template creation.\"\"\"\n",
        "\n",
        "            return status\n",
        "        else:\n",
        "            return \"📹 Session recording active but no data collected yet\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error getting session status: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jZK7cESeNDd"
      },
      "outputs": [],
      "source": [
        "def view_data():\n",
        "    \"\"\"Return a preview of the rent roll data.\"\"\"\n",
        "    global app_state  # Use app_state instead of agent_state\n",
        "\n",
        "    logger.info(\"View data requested\")\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:  # Note the dictionary access with [\"df\"]\n",
        "        logger.warning(\"View data requested but no data is loaded\")\n",
        "        return \"No rent roll data loaded yet.\"\n",
        "\n",
        "    # Generate HTML representation of the dataframe\n",
        "    logger.info(f\"Generating HTML preview of data with {len(app_state['df'])} rows\")\n",
        "    html = f\"\"\"\n",
        "    <h3>Rent Roll Data</h3>\n",
        "    <p>{len(app_state['df'])} rows × {len(app_state['df'].columns)} columns</p>\n",
        "    {app_state['df'].head(10).fillna('').to_html(index=False)}\n",
        "    \"\"\"\n",
        "\n",
        "    return html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jho5cKJ9ehpb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clear_chat():\n",
        "    \"\"\"Reset the chat history.\"\"\"\n",
        "    logger.info(\"Clearing chat history\")\n",
        "    return []  # Return empty list for Gradio chat history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ofDGmDz8n-"
      },
      "outputs": [],
      "source": [
        "def view_dataframe_versions():\n",
        "    \"\"\"\n",
        "    FIXED: Return HTML showing all versions of the rent roll dataframe\n",
        "    \"\"\"\n",
        "    global app_state\n",
        "\n",
        "    logger.info(\"Generating dataframe versions view\")\n",
        "\n",
        "    # First check app_state registry\n",
        "    registry_versions = []\n",
        "    if app_state and \"df_versions\" in app_state and app_state[\"df_versions\"]:\n",
        "        registry_versions = app_state[\"df_versions\"]\n",
        "        logger.info(f\"Found {len(registry_versions)} versions in registry\")\n",
        "\n",
        "    # Also check physical files\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "    file_versions = []\n",
        "\n",
        "    if os.path.exists(versions_dir):\n",
        "        try:\n",
        "            all_files = os.listdir(versions_dir)\n",
        "            csv_files = [f for f in all_files if f.endswith('.csv') and 'rent_roll_v_' in f]\n",
        "\n",
        "            for csv_file in csv_files:\n",
        "                # Extract version name from filename\n",
        "                if csv_file.startswith('rent_roll_v_'):\n",
        "                    version_name = csv_file.replace('rent_roll_', '').replace('.csv', '')\n",
        "\n",
        "                    file_path = os.path.join(versions_dir, csv_file)\n",
        "                    file_stats = os.stat(file_path)\n",
        "\n",
        "                    # Try to get dataframe info\n",
        "                    try:\n",
        "                        temp_df = pd.read_csv(file_path)\n",
        "                        df_info = f\"{len(temp_df)} rows × {len(temp_df.columns)} columns\"\n",
        "                    except:\n",
        "                        df_info = \"Unable to read file\"\n",
        "\n",
        "                    file_versions.append({\n",
        "                        'version_name': version_name,\n",
        "                        'file_path': file_path,\n",
        "                        'file_size': file_stats.st_size,\n",
        "                        'modified_time': datetime.fromtimestamp(file_stats.st_mtime),\n",
        "                        'df_info': df_info,\n",
        "                        'source': 'file'\n",
        "                    })\n",
        "\n",
        "            logger.info(f\"Found {len(file_versions)} version files on disk\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading versions directory: {e}\")\n",
        "\n",
        "    # Combine and deduplicate versions\n",
        "    all_versions = {}\n",
        "\n",
        "    # Add registry versions\n",
        "    for version in registry_versions:\n",
        "        all_versions[version['name']] = {\n",
        "            'version_name': version['name'],\n",
        "            'description': version.get('description', ''),\n",
        "            'timestamp': datetime.strptime(version['timestamp'], \"%Y%m%d_%H%M%S\") if 'timestamp' in version else None,\n",
        "            'shape': version.get('shape', 'Unknown'),\n",
        "            'is_original': version.get('is_original', False),\n",
        "            'csv_filename': version.get('csv_filename', ''),\n",
        "            'source': 'registry'\n",
        "        }\n",
        "\n",
        "    # Add file versions (update existing or add new)\n",
        "    for version in file_versions:\n",
        "        name = version['version_name']\n",
        "        if name in all_versions:\n",
        "            # Update with file info\n",
        "            all_versions[name].update({\n",
        "                'file_path': version['file_path'],\n",
        "                'file_size': version['file_size'],\n",
        "                'modified_time': version['modified_time'],\n",
        "                'df_info': version['df_info'],\n",
        "                'file_exists': True\n",
        "            })\n",
        "        else:\n",
        "            # Add as file-only version\n",
        "            all_versions[name] = {\n",
        "                'version_name': name,\n",
        "                'description': 'Found in directory',\n",
        "                'timestamp': version['modified_time'],\n",
        "                'df_info': version['df_info'],\n",
        "                'file_path': version['file_path'],\n",
        "                'file_size': version['file_size'],\n",
        "                'modified_time': version['modified_time'],\n",
        "                'is_original': False,\n",
        "                'source': 'file_only',\n",
        "                'file_exists': True\n",
        "            }\n",
        "\n",
        "    if not all_versions:\n",
        "        return \"\"\"\n",
        "        <h3 style=\"color: white;\">No Rent Roll Versions Found</h3>\n",
        "        <p style=\"color: white;\">No versions have been saved yet. Save a version first using the 'Save Version' functionality.</p>\n",
        "        \"\"\"\n",
        "\n",
        "    # Sort versions by timestamp\n",
        "    sorted_versions = sorted(all_versions.values(), key=lambda x: x.get('timestamp') or x.get('modified_time') or datetime.min)\n",
        "\n",
        "    # Create enhanced HTML table\n",
        "    html = f\"\"\"\n",
        "    <h3 style=\"color: white;\">Rent Roll Dataframe Version History</h3>\n",
        "    <p style=\"color: white;\">Found {len(sorted_versions)} version(s) - Registry: {len(registry_versions)}, Files: {len(file_versions)}</p>\n",
        "    <table border=\"1\" cellpadding=\"5\" cellspacing=\"0\" style=\"width: 100%; border-collapse: collapse; color: white;\">\n",
        "        <thead style=\"background-color: #009879;\">\n",
        "            <tr>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Version Name</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Status</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Created</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Size</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Data</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Description</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Source</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "    \"\"\"\n",
        "\n",
        "    for i, version in enumerate(sorted_versions):\n",
        "        # Determine status badges\n",
        "        status_badges = []\n",
        "\n",
        "        if version.get('is_original') or i == 0:\n",
        "            status_badges.append('<span style=\"background-color: #3949ab; color: white; padding: 3px 6px; border-radius: 3px; display: inline-block;\">ORIGINAL</span>')\n",
        "\n",
        "        if i == len(sorted_versions) - 1:\n",
        "            status_badges.append('<span style=\"background-color: #43a047; color: white; padding: 3px 6px; border-radius: 3px; display: inline-block;\">LATEST</span>')\n",
        "\n",
        "        if version.get('source') == 'registry':\n",
        "            status_badges.append('<span style=\"background-color: #607d8b; color: white; padding: 3px 6px; border-radius: 3px; display: inline-block;\">TRACKED</span>')\n",
        "\n",
        "        if version.get('file_exists'):\n",
        "            status_badges.append('<span style=\"background-color: #4caf50; color: white; padding: 3px 6px; border-radius: 3px; display: inline-block;\">FILE OK</span>')\n",
        "\n",
        "        status_html = ' '.join(status_badges) if status_badges else '<span style=\"color: #999;\">-</span>'\n",
        "\n",
        "        # Format timestamp\n",
        "        timestamp = version.get('timestamp') or version.get('modified_time')\n",
        "        time_str = timestamp.strftime('%Y-%m-%d %H:%M:%S') if timestamp else 'Unknown'\n",
        "\n",
        "        # Format file size\n",
        "        file_size = version.get('file_size', 0)\n",
        "        size_str = f\"{file_size:,} bytes ({file_size/1024:.1f} KB)\" if file_size > 0 else 'Unknown'\n",
        "\n",
        "        # Data info\n",
        "        data_info = version.get('df_info') or version.get('shape', 'Unknown')\n",
        "        if isinstance(data_info, list) and len(data_info) == 2:\n",
        "            data_info = f\"{data_info[0]} rows × {data_info[1]} columns\"\n",
        "\n",
        "        html += f\"\"\"\n",
        "        <tr style=\"background-color: #25292e; color: white; border-bottom: 1px solid #333;\">\n",
        "            <td style=\"padding: 10px;\"><code style=\"font-family: monospace; font-weight: bold;\">{version['version_name']}</code></td>\n",
        "            <td style=\"padding: 10px;\">{status_html}</td>\n",
        "            <td style=\"padding: 10px;\">{time_str}</td>\n",
        "            <td style=\"padding: 10px;\">{size_str}</td>\n",
        "            <td style=\"padding: 10px;\">{data_info}</td>\n",
        "            <td style=\"padding: 10px;\">{version.get('description', 'No description')}</td>\n",
        "            <td style=\"padding: 10px;\">{version.get('source', 'unknown').title()}</td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "\n",
        "    html += \"\"\"\n",
        "        </tbody>\n",
        "    </table>\n",
        "    <p style=\"color: #999; font-size: 12px; margin-top: 10px;\">\n",
        "        Registry: Versions tracked in memory | Files: Versions found on disk | Tracked: In both registry and disk\n",
        "    </p>\n",
        "    \"\"\"\n",
        "\n",
        "    logger.info(f\"Generated version history HTML with {len(sorted_versions)} versions\")\n",
        "    return html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoP4lkp0EWzW"
      },
      "outputs": [],
      "source": [
        "def ensure_version_system_initialized():\n",
        "    \"\"\"\n",
        "    FIXED: Ensure the version system is properly initialized\n",
        "    \"\"\"\n",
        "    global app_state\n",
        "\n",
        "    if app_state is None:\n",
        "        logger.warning(\"app_state is None, initializing...\")\n",
        "        app_state = {}\n",
        "\n",
        "    if \"df_versions\" not in app_state:\n",
        "        logger.info(\"Initializing df_versions list\")\n",
        "        app_state[\"df_versions\"] = []\n",
        "\n",
        "    # Create versions directory\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "    os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "    logger.info(f\"Version system initialized. Registry has {len(app_state['df_versions'])} versions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIY11okJUJ28"
      },
      "outputs": [],
      "source": [
        "def enhance_user_prompt_with_context(user_prompt, history):\n",
        "    \"\"\"\n",
        "    Enhanced version: Enhance user prompt with rent roll context and CRE best practices using GPT-4\n",
        "    Now properly integrates with version-specific dataframe loading\n",
        "    \"\"\"\n",
        "    global app_state\n",
        "\n",
        "    if not user_prompt.strip():\n",
        "        return user_prompt\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        return user_prompt + \" (Note: Please upload a rent roll file first for better analysis)\"\n",
        "\n",
        "    try:\n",
        "        # Step 1: Detect version reference in user prompt\n",
        "        version_pattern = r'v_\\d{8}_\\d{6}'\n",
        "        version_match = re.search(version_pattern, user_prompt)\n",
        "\n",
        "        version_context = \"\"\n",
        "        df = app_state[\"df\"]  # Default to current dataframe\n",
        "        version_info_text = \"\"\n",
        "\n",
        "        if version_match:\n",
        "            requested_version = version_match.group(0)\n",
        "            logger.info(f\"User requested specific version in prompt enhancement: {requested_version}\")\n",
        "\n",
        "            # Try to load that specific version for context\n",
        "            versions_dir = \"rent_roll_versions\"\n",
        "            version_file = os.path.join(versions_dir, f\"rent_roll_{requested_version}.csv\")\n",
        "\n",
        "            if os.path.exists(version_file):\n",
        "                # Load the requested version for context\n",
        "                version_df = pd.read_csv(version_file)\n",
        "                logger.info(f\"Successfully loaded version {requested_version} for enhancement with shape {version_df.shape}\")\n",
        "\n",
        "                # Find version metadata\n",
        "                version_info = None\n",
        "                for v in app_state.get(\"df_versions\", []):\n",
        "                    if v.get(\"name\") == requested_version:\n",
        "                        version_info = v\n",
        "                        break\n",
        "\n",
        "                # Use this version for context\n",
        "                df = version_df\n",
        "\n",
        "                # Create detailed version context\n",
        "                version_context = f\"\"\"\n",
        "        VERSION-SPECIFIC ENHANCEMENT CONTEXT:\n",
        "        ===================================\n",
        "        User requested version: {requested_version}\n",
        "        Version Description: {version_info.get('description', 'No description available') if version_info else 'Version not found in registry'}\n",
        "        Version Created: {version_info.get('timestamp', 'Unknown') if version_info else 'Unknown'}\n",
        "        Version Shape: {version_df.shape}\n",
        "        Current/Latest Version Shape: {app_state[\"df\"].shape}\n",
        "\n",
        "        Version Differences:\n",
        "        - Row count difference: {version_df.shape[0] - app_state[\"df\"].shape[0]} rows\n",
        "        - Column count difference: {version_df.shape[1] - app_state[\"df\"].shape[1]} columns\n",
        "        - Version is {'older' if version_info and app_state.get('df_versions', [])[-1]['name'] != requested_version else 'the latest'} version\n",
        "\n",
        "        CRITICAL: The enhanced prompt must specify that analysis should be performed on version {requested_version}\n",
        "        \"\"\"\n",
        "\n",
        "                version_info_text = f\"version {requested_version}\"\n",
        "\n",
        "                # Check if columns differ\n",
        "                current_cols = set(app_state[\"df\"].columns)\n",
        "                version_cols = set(version_df.columns)\n",
        "\n",
        "                if current_cols != version_cols:\n",
        "                    added_cols = current_cols - version_cols\n",
        "                    removed_cols = version_cols - current_cols\n",
        "\n",
        "                    if added_cols:\n",
        "                        version_context += f\"\\n        Columns added since this version: {list(added_cols)}\"\n",
        "                    if removed_cols:\n",
        "                        version_context += f\"\\n        Columns removed since this version: {list(removed_cols)}\"\n",
        "\n",
        "            else:\n",
        "                # Version file not found\n",
        "                logger.warning(f\"Requested version {requested_version} not found at {version_file} during enhancement\")\n",
        "                version_context = f\"\"\"\n",
        "        VERSION REQUEST NOTE FOR ENHANCEMENT:\n",
        "        ===================================\n",
        "        ⚠️ User requested version '{requested_version}' but the file was not found.\n",
        "        Enhancement will use current dataframe context but note the version request.\n",
        "        Available versions: {[v['name'] for v in app_state.get('df_versions', [])][:5]}...\n",
        "\n",
        "        CRITICAL: The enhanced prompt must include a note about the version request failure\n",
        "        \"\"\"\n",
        "                version_info_text = f\"requested version {requested_version} (file not found, using current)\"\n",
        "        else:\n",
        "            # No version specified, note that we're using current\n",
        "            latest_version_name = app_state.get('df_versions', [])[-1]['name'] if app_state.get('df_versions') else 'current'\n",
        "            version_context = f\"\"\"\n",
        "        USING CURRENT/LATEST VERSION FOR ENHANCEMENT: {latest_version_name}\n",
        "        =============================================================\n",
        "        No specific version requested - using the latest dataframe for context\n",
        "        \"\"\"\n",
        "            version_info_text = \"current/latest version\"\n",
        "\n",
        "        # Prepare comprehensive context with the selected dataframe\n",
        "        df_context = f\"\"\"\n",
        "        RENT ROLL CONTEXT FOR PROMPT ENHANCEMENT:\n",
        "        ========================================\n",
        "\n",
        "        {version_context}\n",
        "\n",
        "        DataFrame Info ({version_info_text}):\n",
        "        - Shape: {df.shape} (rows: {df.shape[0]}, columns: {df.shape[1]})\n",
        "        - Columns: {list(df.columns)}\n",
        "        - Data Types: {dict(df.dtypes.astype(str))}\n",
        "\n",
        "        Sample Data from {version_info_text.upper()} (first 10 rows):\n",
        "        {df.head(10).to_string()}\n",
        "\n",
        "        Data Quality Overview:\n",
        "        - Null values per column: {dict(df.isnull().sum())}\n",
        "        - Memory usage: {df.memory_usage(deep=True).sum()} bytes\n",
        "\n",
        "        Available Versions: {len(app_state.get('df_versions', []))} saved versions\n",
        "        Version Names: {[v['name'] for v in app_state.get('df_versions', [])][:5]}{'...' if len(app_state.get('df_versions', [])) > 5 else ''}\n",
        "        \"\"\"\n",
        "\n",
        "        # Get recent chat context (last 3 exchanges)\n",
        "        recent_context = \"\"\n",
        "        if history and len(history) > 0:\n",
        "            recent_exchanges = history[-3:]  # Last 3 exchanges\n",
        "            for i, (user_msg, ai_msg) in enumerate(recent_exchanges, 1):\n",
        "                recent_context += f\"Exchange {i}:\\n\"\n",
        "                recent_context += f\"User: {user_msg[:100]}...\\n\"\n",
        "                recent_context += f\"AI: {ai_msg[:100]}...\\n\\n\"\n",
        "\n",
        "        # Get issues context if available\n",
        "        issues_context = \"\"\n",
        "        if app_state.get(\"issues\"):\n",
        "            issues_context = f\"\\nKnown Issues in Data:\\n\" + \"\\n\".join([f\"- {issue[:100]}...\" for issue in app_state[\"issues\"][:5]])\n",
        "\n",
        "        # MODIFIED: Create professional, instructional, single-paragraph enhancement prompt\n",
        "        enhancement_prompt = f\"\"\"\n",
        "        You are an expert commercial real estate data analyst. Transform the user's request into a single, comprehensive, professional paragraph that provides detailed instructions for rent roll analysis.\n",
        "\n",
        "        CURRENT USER PROMPT: \"{user_prompt}\"\n",
        "\n",
        "        {df_context}\n",
        "\n",
        "        RECENT CONVERSATION CONTEXT:\n",
        "        {recent_context}\n",
        "\n",
        "        {issues_context}\n",
        "\n",
        "        VERSION AWARENESS:\n",
        "        - The user {'HAS SPECIFICALLY REQUESTED' if version_match else 'has NOT specified'} a particular version to work with\n",
        "        {'- Working with version: ' + requested_version + ' (MUST be included in enhanced prompt)' if version_match else '- Working with the current/latest version'}\n",
        "\n",
        "        ENHANCEMENT REQUIREMENTS:\n",
        "        Transform the user's request into a single, professional paragraph that:\n",
        "        • {'Clearly specifies analysis should be performed on version ' + requested_version if version_match else 'Notes that analysis should use the current/latest version'}\n",
        "        • References specific column names from the dataset: {list(df.columns)[:8]}\n",
        "        • Uses authoritative, instructional language with technical precision\n",
        "        • Includes specific CRE terminology and business context\n",
        "        • Specifies expected outputs, validation steps, and quality checks\n",
        "        • Provides clear methodology and success criteria\n",
        "        • Maintains professional tone throughout\n",
        "\n",
        "        EXAMPLE TRANSFORMATIONS:\n",
        "\n",
        "        \"show me the data\" →\n",
        "        {\"Perform a comprehensive analysis of rent roll dataset version \" + requested_version if version_match else \"Perform a comprehensive analysis of the current rent roll dataset\"} by generating summary statistics for all key columns including tenant names, unit identifiers, rent amounts, and lease dates, conducting a thorough data quality assessment to identify missing values, inconsistencies, and outliers, calculating portfolio-level metrics such as total occupied units, average rent per unit, occupancy rates, and rent distribution patterns, and producing a detailed report that highlights any data anomalies requiring immediate attention while ensuring all findings are validated against industry standards for commercial real estate analysis.\n",
        "\n",
        "        \"fix the unit column\" →\n",
        "        {\"Execute data standardization procedures on the Unit column within \" + (\"version \" + requested_version if version_match else \"the current rent roll dataset\")} by systematically analyzing existing data patterns to identify formatting inconsistencies, implementing a standardized unit identifier format that aligns with commercial real estate best practices, validating data integrity through cross-referencing with other unit-related columns, correcting all identified discrepancies while maintaining audit trail documentation, and generating a comprehensive before-and-after comparison report that demonstrates the improvements made and confirms all unit identifiers are properly formatted for subsequent portfolio analysis and reporting requirements.\n",
        "\n",
        "        Return ONLY a single professional paragraph that transforms the user's request into comprehensive, authoritative instructions. Use technical precision, industry terminology, and maintain an instructional tone throughout.\n",
        "        \"\"\"\n",
        "\n",
        "        # Call Claude 3.5 Sonnet for enhancement\n",
        "        client = Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "        response = client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=200,\n",
        "            temperature=0.2,\n",
        "            system=\"You are an expert at creating professional, authoritative, single-paragraph instructions for commercial real estate rent roll analysis. Transform user requests into comprehensive, technical instructions that specify methodology, expected outputs, and validation steps. Return only one professional paragraph.\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": enhancement_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        enhanced_prompt = response.content[0].text.strip()\n",
        "\n",
        "        # Remove any quotes if Claude wrapped the response\n",
        "        if enhanced_prompt.startswith('\"') and enhanced_prompt.endswith('\"'):\n",
        "            enhanced_prompt = enhanced_prompt[1:-1]\n",
        "\n",
        "        logger.info(f\"Enhanced prompt with Claude 3.5 Sonnet: {enhanced_prompt[:100]}...\")\n",
        "        return enhanced_prompt\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error enhancing prompt with version awareness: {e}\")\n",
        "        # Return original prompt with a note if enhancement fails\n",
        "        version_note = f\" (Note: Working with {version_match.group(0) if version_match else 'current version'} - enhancement temporarily unavailable)\"\n",
        "        return f\"{user_prompt}{version_note}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBw7t_p3RhSm"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# def enhance_user_prompt_with_context(user_prompt, history):\n",
        "#     \"\"\"\n",
        "#     Enhanced version: Enhance user prompt with rent roll context and CRE best practices using GPT-4\n",
        "#     Now properly integrates with version-specific dataframe loading\n",
        "#     \"\"\"\n",
        "#     global app_state\n",
        "\n",
        "#     if not user_prompt.strip():\n",
        "#         return user_prompt\n",
        "\n",
        "#     if app_state is None or app_state[\"df\"] is None:\n",
        "#         return user_prompt + \" (Note: Please upload a rent roll file first for better analysis)\"\n",
        "\n",
        "#     try:\n",
        "#         # Step 1: Detect version reference in user prompt\n",
        "#         version_pattern = r'v_\\d{8}_\\d{6}'\n",
        "#         version_match = re.search(version_pattern, user_prompt)\n",
        "\n",
        "#         version_context = \"\"\n",
        "#         df = app_state[\"df\"]  # Default to current dataframe\n",
        "#         version_info_text = \"\"\n",
        "\n",
        "#         if version_match:\n",
        "#             requested_version = version_match.group(0)\n",
        "#             logger.info(f\"User requested specific version in prompt enhancement: {requested_version}\")\n",
        "\n",
        "#             # Try to load that specific version for context\n",
        "#             versions_dir = \"rent_roll_versions\"\n",
        "#             version_file = os.path.join(versions_dir, f\"rent_roll_{requested_version}.csv\")\n",
        "\n",
        "#             if os.path.exists(version_file):\n",
        "#                 # Load the requested version for context\n",
        "#                 version_df = pd.read_csv(version_file)\n",
        "#                 logger.info(f\"Successfully loaded version {requested_version} for enhancement with shape {version_df.shape}\")\n",
        "\n",
        "#                 # Find version metadata\n",
        "#                 version_info = None\n",
        "#                 for v in app_state.get(\"df_versions\", []):\n",
        "#                     if v.get(\"name\") == requested_version:\n",
        "#                         version_info = v\n",
        "#                         break\n",
        "\n",
        "#                 # Use this version for context\n",
        "#                 df = version_df\n",
        "\n",
        "#                 # Create detailed version context\n",
        "#                 version_context = f\"\"\"\n",
        "#         VERSION-SPECIFIC ENHANCEMENT CONTEXT:\n",
        "#         ===================================\n",
        "#         User requested version: {requested_version}\n",
        "#         Version Description: {version_info.get('description', 'No description available') if version_info else 'Version not found in registry'}\n",
        "#         Version Created: {version_info.get('timestamp', 'Unknown') if version_info else 'Unknown'}\n",
        "#         Version Shape: {version_df.shape}\n",
        "#         Current/Latest Version Shape: {app_state[\"df\"].shape}\n",
        "\n",
        "#         Version Differences:\n",
        "#         - Row count difference: {version_df.shape[0] - app_state[\"df\"].shape[0]} rows\n",
        "#         - Column count difference: {version_df.shape[1] - app_state[\"df\"].shape[1]} columns\n",
        "#         - Version is {'older' if version_info and app_state.get('df_versions', [])[-1]['name'] != requested_version else 'the latest'} version\n",
        "\n",
        "#         CRITICAL: The enhanced prompt must specify that analysis should be performed on version {requested_version}\n",
        "#         \"\"\"\n",
        "\n",
        "#                 version_info_text = f\"version {requested_version}\"\n",
        "\n",
        "#                 # Check if columns differ\n",
        "#                 current_cols = set(app_state[\"df\"].columns)\n",
        "#                 version_cols = set(version_df.columns)\n",
        "\n",
        "#                 if current_cols != version_cols:\n",
        "#                     added_cols = current_cols - version_cols\n",
        "#                     removed_cols = version_cols - current_cols\n",
        "\n",
        "#                     if added_cols:\n",
        "#                         version_context += f\"\\n        Columns added since this version: {list(added_cols)}\"\n",
        "#                     if removed_cols:\n",
        "#                         version_context += f\"\\n        Columns removed since this version: {list(removed_cols)}\"\n",
        "\n",
        "#             else:\n",
        "#                 # Version file not found\n",
        "#                 logger.warning(f\"Requested version {requested_version} not found at {version_file} during enhancement\")\n",
        "#                 version_context = f\"\"\"\n",
        "#         VERSION REQUEST NOTE FOR ENHANCEMENT:\n",
        "#         ===================================\n",
        "#         ⚠️ User requested version '{requested_version}' but the file was not found.\n",
        "#         Enhancement will use current dataframe context but note the version request.\n",
        "#         Available versions: {[v['name'] for v in app_state.get('df_versions', [])][:5]}...\n",
        "\n",
        "#         CRITICAL: The enhanced prompt must include a note about the version request failure\n",
        "#         \"\"\"\n",
        "#                 version_info_text = f\"requested version {requested_version} (file not found, using current)\"\n",
        "#         else:\n",
        "#             # No version specified, note that we're using current\n",
        "#             latest_version_name = app_state.get('df_versions', [])[-1]['name'] if app_state.get('df_versions') else 'current'\n",
        "#             version_context = f\"\"\"\n",
        "#         USING CURRENT/LATEST VERSION FOR ENHANCEMENT: {latest_version_name}\n",
        "#         =============================================================\n",
        "#         No specific version requested - using the latest dataframe for context\n",
        "#         \"\"\"\n",
        "#             version_info_text = \"current/latest version\"\n",
        "\n",
        "#         # Prepare comprehensive context with the selected dataframe\n",
        "#         df_context = f\"\"\"\n",
        "#         RENT ROLL CONTEXT FOR PROMPT ENHANCEMENT:\n",
        "#         ========================================\n",
        "\n",
        "#         {version_context}\n",
        "\n",
        "#         DataFrame Info ({version_info_text}):\n",
        "#         - Shape: {df.shape} (rows: {df.shape[0]}, columns: {df.shape[1]})\n",
        "#         - Columns: {list(df.columns)}\n",
        "#         - Data Types: {dict(df.dtypes.astype(str))}\n",
        "\n",
        "#         Sample Data from {version_info_text.upper()} (first 10 rows):\n",
        "#         {df.head(10).to_string()}\n",
        "\n",
        "#         Data Quality Overview:\n",
        "#         - Null values per column: {dict(df.isnull().sum())}\n",
        "#         - Memory usage: {df.memory_usage(deep=True).sum()} bytes\n",
        "\n",
        "#         Available Versions: {len(app_state.get('df_versions', []))} saved versions\n",
        "#         Version Names: {[v['name'] for v in app_state.get('df_versions', [])][:5]}{'...' if len(app_state.get('df_versions', [])) > 5 else ''}\n",
        "#         \"\"\"\n",
        "\n",
        "#         # Get recent chat context (last 3 exchanges)\n",
        "#         recent_context = \"\"\n",
        "#         if history and len(history) > 0:\n",
        "#             recent_exchanges = history[-3:]  # Last 3 exchanges\n",
        "#             for i, (user_msg, ai_msg) in enumerate(recent_exchanges, 1):\n",
        "#                 recent_context += f\"Exchange {i}:\\n\"\n",
        "#                 recent_context += f\"User: {user_msg[:100]}...\\n\"\n",
        "#                 recent_context += f\"AI: {ai_msg[:100]}...\\n\\n\"\n",
        "\n",
        "#         # Get issues context if available\n",
        "#         issues_context = \"\"\n",
        "#         if app_state.get(\"issues\"):\n",
        "#             issues_context = f\"\\nKnown Issues in Data:\\n\" + \"\\n\".join([f\"- {issue[:100]}...\" for issue in app_state[\"issues\"][:5]])\n",
        "\n",
        "#         # Create enhancement prompt for GPT-4 with enhanced version awareness\n",
        "#         enhancement_prompt = f\"\"\"\n",
        "#         You are an expert commercial real estate analyst assistant. Your job is to enhance user prompts to be more specific, actionable, and contextually aware for rent roll analysis.\n",
        "\n",
        "#         CURRENT USER PROMPT: \"{user_prompt}\"\n",
        "\n",
        "#         {df_context}\n",
        "\n",
        "#         RECENT CONVERSATION CONTEXT:\n",
        "#         {recent_context}\n",
        "\n",
        "#         {issues_context}\n",
        "\n",
        "#         VERSION AWARENESS CRITICAL INSTRUCTIONS:\n",
        "#         =======================================\n",
        "#         - The user {'HAS SPECIFICALLY REQUESTED' if version_match else 'has NOT specified'} a particular version to work with\n",
        "#         {'- Working with version: ' + requested_version + ' (MUST be included in enhanced prompt)' if version_match else '- Working with the current/latest version'}\n",
        "#         - Ensure the enhanced prompt clearly specifies which version should be used for analysis\n",
        "#         - If a specific version was requested, the enhanced prompt MUST include the version identifier\n",
        "#         - Make it clear that version-specific analysis is required when applicable\n",
        "\n",
        "#         Please enhance the user's prompt by:\n",
        "\n",
        "#         1. **Version Specification**: {'Clearly state that analysis should be performed on version ' + requested_version if version_match else 'Note that analysis should use the current/latest version'}\n",
        "#         2. **Add Specific Context**: Reference actual column names from the {version_info_text} dataframe\n",
        "#         3. **CRE Terminology**: Use proper commercial real estate terminology\n",
        "#         4. **Technical Precision**: Specify data types, calculations, or analysis methods needed\n",
        "#         5. **Business Goals**: Frame the request in terms of CRE business objectives\n",
        "#         6. **Actionable Instructions**: Make the request more specific and executable\n",
        "#         7. **Data Validation**: Include any data quality checks that should be performed\n",
        "\n",
        "#         ENHANCEMENT GUIDELINES WITH VERSION AWARENESS:\n",
        "#         =============================================\n",
        "#         - If a version is specified, ALWAYS include it prominently in the enhanced prompt\n",
        "#         - If asking about rent, specify which rent columns (base rent, total rent, etc.) from the {version_info_text}\n",
        "#         - If asking about tenants, reference actual tenant-related columns from the {version_info_text}\n",
        "#         - If asking about dates, specify lease start/end date columns from the {version_info_text}\n",
        "#         - If asking about calculations, be specific about formulas needed for the {version_info_text}\n",
        "#         - If asking about analysis, suggest specific CRE metrics or insights for the {version_info_text}\n",
        "#         - If asking about cleaning, reference specific data quality issues in the {version_info_text}\n",
        "#         - Keep the user's original intent but make it more professional and version-aware\n",
        "\n",
        "#         EXAMPLE TRANSFORMATIONS WITH VERSION AWARENESS:\n",
        "#         =============================================\n",
        "\n",
        "#         {\"show me the data from \" + requested_version if version_match else \"show me the data\"} →\n",
        "#         {\"Display a comprehensive overview of the rent roll data from version \" + requested_version if version_match else \"Display a comprehensive overview of the current rent roll data\"}, including tenant information from [Tenant] column, rent amounts from [Base Rent] and [Total Rent] columns, lease dates from [Lease Start Date] and [Lease End Date] columns, and occupancy status. {\"Compare key metrics with the current version and highlight any significant changes.\" if version_match else \"Highlight any data quality issues or notable patterns.\"}\n",
        "\n",
        "#         {\"fix the rent in \" + (\"version \" + requested_version if version_match else \"the table\")} →\n",
        "#         {\"Using \" + (\"version \" + requested_version if version_match else \"the current rent roll data\")}, analyze and standardize the rent-related columns ([Base Rent], [Total Rent], [CAM Charges] if available) by: 1) Identifying and correcting formatting inconsistencies, 2) Validating that total rent calculations are accurate, 3) Flagging any unusual rent amounts that may need review, and 4) Ensuring all currency values are properly formatted for CRE reporting standards. Save the cleaned data as a new version with a descriptive name.\n",
        "\n",
        "#         Return ONLY the enhanced prompt, nothing else. The enhanced prompt should be professional, version-aware, specific, and ready for immediate execution by a CRE analysis AI system.\n",
        "#         \"\"\"\n",
        "\n",
        "#         # Call GPT-4 for enhancement\n",
        "#         client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4o\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": \"You are an expert at enhancing user prompts for commercial real estate rent roll analysis with precise version awareness. Return only the enhanced prompt, nothing else.\"},\n",
        "#                 {\"role\": \"user\", \"content\": enhancement_prompt}\n",
        "#             ],\n",
        "#             max_tokens=800,\n",
        "#             temperature=0.3\n",
        "#         )\n",
        "\n",
        "#         enhanced_prompt = response.choices[0].message.content.strip()\n",
        "\n",
        "#         # Remove any quotes if GPT-4 wrapped the response\n",
        "#         if enhanced_prompt.startswith('\"') and enhanced_prompt.endswith('\"'):\n",
        "#             enhanced_prompt = enhanced_prompt[1:-1]\n",
        "\n",
        "#         logger.info(f\"Enhanced prompt with version awareness: {enhanced_prompt[:100]}...\")\n",
        "#         return enhanced_prompt\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Error enhancing prompt with version awareness: {e}\")\n",
        "#         # Return original prompt with a note if enhancement fails\n",
        "#         version_note = f\" (Note: Working with {version_match.group(0) if version_match else 'current version'} - enhancement temporarily unavailable)\"\n",
        "#         return f\"{user_prompt}{version_note}\"\n",
        "\n",
        "\n",
        "\n",
        "def enhance_prompt_interface(user_prompt, history):\n",
        "    \"\"\"\n",
        "    Interface function for the enhance button - returns enhanced prompt and status message\n",
        "    \"\"\"\n",
        "    if not user_prompt.strip():\n",
        "        return \"Please enter a prompt first\", user_prompt\n",
        "\n",
        "    try:\n",
        "        # Check if version is mentioned\n",
        "        version_pattern = r'v_\\d{8}_\\d{6}'\n",
        "        has_version = bool(re.search(version_pattern, user_prompt))\n",
        "\n",
        "        enhanced = enhance_user_prompt_with_context(user_prompt, history)\n",
        "\n",
        "        if enhanced != user_prompt:\n",
        "            if has_version:\n",
        "                status_msg = \"✨ Prompt enhanced with specific version context and CRE best practices!\"\n",
        "            else:\n",
        "                status_msg = \"✨ Prompt enhanced with rent roll context and CRE best practices!\"\n",
        "        else:\n",
        "            status_msg = \"ℹ️ Prompt is already well-structured\"\n",
        "\n",
        "        return status_msg, enhanced\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Enhancement failed: {str(e)}\", user_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boE57lJDwQrW"
      },
      "outputs": [],
      "source": [
        "def analyze_dataframe_changes_with_gpt4(original_df, modified_df, user_description=\"\"):\n",
        "    \"\"\"\n",
        "    Enhanced version with ALL CRITICAL FIXES APPLIED:\n",
        "    1. Fixed logging bug with proper string formatting\n",
        "    2. Focus on data type/formatting precision\n",
        "    3. Implement attempt-to-attempt learning\n",
        "    4. Add data type validation for exact target schema matching\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    import traceback\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create detailed logging directory\n",
        "    logs_dir = \"manual_edit_analysis_logs\"\n",
        "    os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "    # Generate unique log session ID\n",
        "    log_session_id = f\"manual_edit_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]}\"\n",
        "    log_file_path = os.path.join(logs_dir, f\"{log_session_id}_detailed_analysis.txt\")\n",
        "\n",
        "    # FIXED LOGGING FUNCTION - Proper string formatting\n",
        "    def log_to_file(content, section_title=\"\"):\n",
        "        try:\n",
        "            with open(log_file_path, 'a', encoding='utf-8') as f:\n",
        "                if section_title:\n",
        "                    f.write(f\"\\n{'='*80}\\n\")\n",
        "                    f.write(f\"{section_title}\\n\")\n",
        "                    f.write(f\"{'='*80}\\n\")\n",
        "                f.write(f\"{content}\\n\")\n",
        "                f.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR writing to log file: {e}\")\n",
        "\n",
        "    # Initialize comprehensive log\n",
        "    try:\n",
        "        log_to_file(f\"\"\"COMPREHENSIVE MANUAL EDIT ANALYSIS LOG - ENHANCED VERSION\n",
        "Session ID: {log_session_id}\n",
        "Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "User Description: \"{user_description}\"\n",
        "Log File Path: {log_file_path}\n",
        "\n",
        "ENHANCEMENTS APPLIED:\n",
        "1. ✅ Fixed logging string formatting bug\n",
        "2. ✅ Enhanced data type/formatting precision matching\n",
        "3. ✅ Implemented attempt-to-attempt learning system\n",
        "4. ✅ Added comprehensive data type validation\n",
        "\n",
        "ANALYSIS OVERVIEW:\n",
        "This log contains the complete workflow of analyzing manual dataframe edits:\n",
        "1. Original vs Modified dataframe comparison with data type analysis\n",
        "2. GPT-4.1 prompt generation with schema validation\n",
        "3. Claude API calls with learning feedback loops\n",
        "4. Code execution with precision validation\n",
        "5. Replication success/failure analysis with detailed diagnostics\n",
        "\n",
        "FUNCTION EXECUTION STATUS: STARTING...\n",
        "\"\"\", \"SESSION INITIALIZATION\")\n",
        "\n",
        "        print(f\"📝 Enhanced manual edit analysis log initialized: {log_file_path}\")\n",
        "\n",
        "    except Exception as init_error:\n",
        "        print(f\"CRITICAL: Could not initialize log file: {init_error}\")\n",
        "\n",
        "    try:\n",
        "        # ENHANCED DATAFRAME ANALYSIS with data type focus\n",
        "        log_to_file(f\"\"\"ORIGINAL DATAFRAME COMPLETE ANALYSIS:\n",
        "Shape: {original_df.shape}\n",
        "Columns: {list(original_df.columns)}\n",
        "Data Types: {dict(original_df.dtypes.astype(str))}\n",
        "Data Type Details: {json.dumps({col: str(dtype) for col, dtype in original_df.dtypes.items()}, indent=2)}\n",
        "Memory Usage: {original_df.memory_usage(deep=True).sum()} bytes\n",
        "Null Counts: {dict(original_df.isnull().sum())}\n",
        "Index Type: {type(original_df.index).__name__}\n",
        "Column Order: {list(original_df.columns)}\n",
        "\n",
        "FIRST 10 ROWS PREVIEW:\n",
        "{original_df.head(10).to_string()}\n",
        "\n",
        "COMPLETE ORIGINAL DATAFRAME (ALL ROWS):\n",
        "{original_df.to_string(max_rows=None, max_cols=None)}\n",
        "\n",
        "ORIGINAL DATAFRAME AS CSV:\n",
        "{original_df.to_csv(index=False)}\n",
        "\n",
        "DATA TYPE ANALYSIS:\n",
        "{_analyze_dataframe_schema(original_df, \"ORIGINAL\")}\n",
        "\"\"\", \"ORIGINAL DATAFRAME ANALYSIS\")\n",
        "\n",
        "        log_to_file(f\"\"\"MODIFIED DATAFRAME COMPLETE ANALYSIS:\n",
        "Shape: {modified_df.shape}\n",
        "Columns: {list(modified_df.columns)}\n",
        "Data Types: {dict(modified_df.dtypes.astype(str))}\n",
        "Data Type Details: {json.dumps({col: str(dtype) for col, dtype in modified_df.dtypes.items()}, indent=2)}\n",
        "Memory Usage: {modified_df.memory_usage(deep=True).sum()} bytes\n",
        "Null Counts: {dict(modified_df.isnull().sum())}\n",
        "Index Type: {type(modified_df.index).__name__}\n",
        "Column Order: {list(modified_df.columns)}\n",
        "\n",
        "FIRST 10 ROWS PREVIEW:\n",
        "{modified_df.head(10).to_string()}\n",
        "\n",
        "COMPLETE MODIFIED DATAFRAME (ALL ROWS):\n",
        "{modified_df.to_string(max_rows=None, max_cols=None)}\n",
        "\n",
        "MODIFIED DATAFRAME AS CSV:\n",
        "{modified_df.to_csv(index=False)}\n",
        "\n",
        "DATA TYPE ANALYSIS:\n",
        "{_analyze_dataframe_schema(modified_df, \"MODIFIED\")}\n",
        "\"\"\", \"MODIFIED DATAFRAME ANALYSIS\")\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        try:\n",
        "            client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "            log_to_file(\"✅ OpenAI client initialized successfully\", \"API CLIENT SETUP\")\n",
        "        except Exception as client_error:\n",
        "            log_to_file(f\"❌ Failed to initialize OpenAI client: {client_error}\", \"API CLIENT SETUP ERROR\")\n",
        "            raise client_error\n",
        "\n",
        "        # ENHANCED SCHEMA COMPARISON\n",
        "        schema_comparison = _compare_dataframe_schemas(original_df, modified_df)\n",
        "        log_to_file(f\"\"\"ENHANCED SCHEMA COMPARISON:\n",
        "{json.dumps(schema_comparison, indent=2)}\n",
        "\"\"\", \"SCHEMA COMPARISON ANALYSIS\")\n",
        "\n",
        "        # Prepare enhanced comparison data for GPT-4\n",
        "        original_info = {\n",
        "            \"shape\": original_df.shape,\n",
        "            \"columns\": list(original_df.columns),\n",
        "            \"dtypes\": dict(original_df.dtypes.astype(str)),\n",
        "            \"dtype_details\": {col: str(dtype) for col, dtype in original_df.dtypes.items()},\n",
        "            \"full_data\": original_df.to_string(max_rows=None, max_cols=None),\n",
        "            \"full_csv\": original_df.to_csv(index=False),\n",
        "            \"null_counts\": dict(original_df.isnull().sum()),\n",
        "            \"memory_usage\": original_df.memory_usage(deep=True).sum(),\n",
        "            \"schema_analysis\": _analyze_dataframe_schema(original_df, \"ORIGINAL\"),\n",
        "            \"summary_stats\": original_df.describe(include='all').to_string() if len(original_df) > 0 else \"No data\"\n",
        "        }\n",
        "\n",
        "        modified_info = {\n",
        "            \"shape\": modified_df.shape,\n",
        "            \"columns\": list(modified_df.columns),\n",
        "            \"dtypes\": dict(modified_df.dtypes.astype(str)),\n",
        "            \"dtype_details\": {col: str(dtype) for col, dtype in modified_df.dtypes.items()},\n",
        "            \"full_data\": modified_df.to_string(max_rows=None, max_cols=None),\n",
        "            \"full_csv\": modified_df.to_csv(index=False),\n",
        "            \"null_counts\": dict(modified_df.isnull().sum()),\n",
        "            \"memory_usage\": modified_df.memory_usage(deep=True).sum(),\n",
        "            \"schema_analysis\": _analyze_dataframe_schema(modified_df, \"MODIFIED\"),\n",
        "            \"summary_stats\": modified_df.describe(include='all').to_string() if len(modified_df) > 0 else \"No data\"\n",
        "        }\n",
        "\n",
        "        # Enhanced structural change detection\n",
        "        shape_changed = original_df.shape != modified_df.shape\n",
        "        columns_changed = set(original_df.columns) != set(modified_df.columns)\n",
        "        schema_changes = schema_comparison\n",
        "\n",
        "        log_to_file(f\"\"\"ENHANCED STRUCTURAL CHANGE ANALYSIS:\n",
        "Shape Changed: {shape_changed}\n",
        "- Original Shape: {original_df.shape}\n",
        "- Modified Shape: {modified_df.shape}\n",
        "- Rows Added: {max(0, modified_df.shape[0] - original_df.shape[0])}\n",
        "- Rows Removed: {max(0, original_df.shape[0] - modified_df.shape[0])}\n",
        "- Columns Added: {max(0, modified_df.shape[1] - original_df.shape[1])}\n",
        "- Columns Removed: {max(0, original_df.shape[1] - modified_df.shape[1])}\n",
        "\n",
        "Columns Changed: {columns_changed}\n",
        "- Original Columns: {list(original_df.columns)}\n",
        "- Modified Columns: {list(modified_df.columns)}\n",
        "- Added Columns: {list(set(modified_df.columns) - set(original_df.columns))}\n",
        "- Removed Columns: {list(set(original_df.columns) - set(modified_df.columns))}\n",
        "\n",
        "SCHEMA CHANGES DETECTED:\n",
        "{json.dumps(schema_changes, indent=2)}\n",
        "\"\"\", \"ENHANCED STRUCTURAL CHANGE ANALYSIS\")\n",
        "\n",
        "        # Enhanced cell-by-cell comparison with data type focus\n",
        "        data_changes_detected = False\n",
        "        changed_cells = []\n",
        "        total_changes = 0\n",
        "        changed_rows = set()\n",
        "        changed_columns = set()\n",
        "        data_type_mismatches = []\n",
        "\n",
        "        if original_df.shape == modified_df.shape and list(original_df.columns) == list(modified_df.columns):\n",
        "            print(f\"🔍 Enhanced comparison: {len(original_df)} rows × {len(original_df.columns)} columns...\")\n",
        "            log_to_file(f\"\"\"STARTING ENHANCED CELL-BY-CELL COMPARISON:\n",
        "Total cells to compare: {original_df.shape[0] * original_df.shape[1]}\n",
        "Comparing {len(original_df)} rows × {len(original_df.columns)} columns\n",
        "Enhanced features: Data type validation, formatting precision, null handling\n",
        "This may take time for large dataframes...\n",
        "\"\"\", \"ENHANCED CELL-BY-CELL COMPARISON START\")\n",
        "\n",
        "            # Enhanced cell comparison with data type awareness\n",
        "            for i in range(len(original_df)):\n",
        "                row_has_changes = False\n",
        "                row_changes = []\n",
        "\n",
        "                for col in original_df.columns:\n",
        "                    try:\n",
        "                        orig_val = original_df.iloc[i][col]\n",
        "                        mod_val = modified_df.iloc[i][col]\n",
        "\n",
        "                        # Enhanced comparison with data type awareness\n",
        "                        change_detected, change_detail = _enhanced_cell_comparison(\n",
        "                            orig_val, mod_val, i, col,\n",
        "                            original_df.dtypes[col], modified_df.dtypes[col]\n",
        "                        )\n",
        "\n",
        "                        if change_detected:\n",
        "                            data_changes_detected = True\n",
        "                            total_changes += 1\n",
        "                            row_has_changes = True\n",
        "                            changed_columns.add(col)\n",
        "                            row_changes.append(change_detail)\n",
        "                            changed_cells.append(change_detail)\n",
        "\n",
        "                            # Track data type mismatches\n",
        "                            if change_detail.get('data_type_changed'):\n",
        "                                data_type_mismatches.append(change_detail)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        log_to_file(f\"ERROR comparing cell at row {i}, column '{col}': {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if row_has_changes:\n",
        "                    changed_rows.add(i)\n",
        "\n",
        "                # Log progress every 100 rows\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    log_to_file(f\"Progress: Processed {i + 1}/{len(original_df)} rows, found {total_changes} changes so far\")\n",
        "\n",
        "            print(f\"✅ Enhanced comparison finished: {total_changes} changes, {len(data_type_mismatches)} data type mismatches\")\n",
        "\n",
        "            # Log comprehensive enhanced change analysis\n",
        "            log_to_file(f\"\"\"ENHANCED CELL-BY-CELL COMPARISON RESULTS:\n",
        "=====================================\n",
        "Total Changes Detected: {total_changes}\n",
        "Data Type Mismatches: {len(data_type_mismatches)}\n",
        "Affected Rows: {len(changed_rows)} out of {original_df.shape[0]} ({len(changed_rows)/original_df.shape[0]*100:.1f}%)\n",
        "Affected Columns: {len(changed_columns)} out of {len(original_df.columns)} ({len(changed_columns)/len(original_df.columns)*100:.1f}%)\n",
        "\n",
        "AFFECTED COLUMNS LIST:\n",
        "{list(changed_columns)}\n",
        "\n",
        "AFFECTED ROWS LIST:\n",
        "{sorted(list(changed_rows))[:50]}  # Show first 50 rows\n",
        "\n",
        "DATA TYPE MISMATCHES:\n",
        "{json.dumps(data_type_mismatches, indent=2) if data_type_mismatches else \"None detected\"}\n",
        "\n",
        "ALL DETECTED CHANGES ({len(changed_cells)} total):\n",
        "\"\"\", \"ENHANCED COMPREHENSIVE CHANGE DETECTION RESULTS\")\n",
        "\n",
        "            # Log ALL changes with enhanced details\n",
        "            for i, change in enumerate(changed_cells[:100]):  # Show first 100 changes\n",
        "                change_type = change.get('change_type', 'unknown')\n",
        "                data_type_info = f\" [DType: {change.get('original_dtype', 'unknown')} → {change.get('modified_dtype', 'unknown')}]\" if change.get('data_type_changed') else \"\"\n",
        "                log_to_file(f\"Change {i+1}: Row {change['row']}, Column '{change['column']}' ({change_type}){data_type_info}: '{change['original']}' → '{change['modified']}'\")\n",
        "\n",
        "        else:\n",
        "            log_to_file(f\"\"\"CANNOT PERFORM CELL-BY-CELL COMPARISON:\n",
        "Reason: Shape or column structure differs\n",
        "Original shape: {original_df.shape}\n",
        "Modified shape: {modified_df.shape}\n",
        "Original columns: {list(original_df.columns)}\n",
        "Modified columns: {list(modified_df.columns)}\n",
        "\"\"\", \"CELL-BY-CELL COMPARISON SKIPPED\")\n",
        "\n",
        "        # Enhanced change statistics\n",
        "        total_cells = original_df.shape[0] * original_df.shape[1] if original_df.size > 0 else 1\n",
        "        change_density = total_changes / total_cells\n",
        "\n",
        "        log_to_file(f\"\"\"ENHANCED COMPREHENSIVE STATISTICAL ANALYSIS:\n",
        "===============================\n",
        "Total Cells in Original: {total_cells}\n",
        "Total Cells Changed: {total_changes}\n",
        "Change Density: {change_density*100:.4f}%\n",
        "Data Type Mismatches: {len(data_type_mismatches)}\n",
        "Percentage of Rows Affected: {len(changed_rows)/original_df.shape[0]*100:.2f}% ({len(changed_rows)}/{original_df.shape[0]})\n",
        "Percentage of Columns Affected: {len(changed_columns)/len(original_df.columns)*100:.2f}% ({len(changed_columns)}/{len(original_df.columns)})\n",
        "\n",
        "ENHANCED CHANGE PATTERN ANALYSIS:\n",
        "- NaN Changes: {len([c for c in changed_cells if c.get('change_type') == 'nan_change'])}\n",
        "- Value Changes: {len([c for c in changed_cells if c.get('change_type') == 'value_change'])}\n",
        "- Data Type Changes: {len([c for c in changed_cells if c.get('data_type_changed')])}\n",
        "- Formatting Changes: {len([c for c in changed_cells if c.get('change_type') == 'formatting_change'])}\n",
        "- Most Affected Columns: {sorted(changed_columns)[:10]}\n",
        "- Row Change Distribution: Every {original_df.shape[0]//max(1,len(changed_rows)):.0f} rows on average\n",
        "\n",
        "SCHEMA COMPATIBILITY ANALYSIS:\n",
        "{json.dumps(schema_comparison, indent=2)}\n",
        "\"\"\", \"ENHANCED COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
        "\n",
        "        # STEP 1: Enhanced GPT-4.1 prompt generation with schema awareness\n",
        "        print(\"🧠 Enhanced GPT-4.1: Analyzing changes with schema validation...\")\n",
        "        log_to_file(\"🧠 STARTING ENHANCED GPT-4.1 ANALYSIS WITH SCHEMA VALIDATION...\", \"ENHANCED GPT-4.1 PROMPT GENERATION START\")\n",
        "\n",
        "        claude_prompt = _generate_enhanced_claude_prompt_with_gpt4_logged(\n",
        "            client, original_info, modified_info, user_description,\n",
        "            total_changes, changed_rows, changed_columns, changed_cells,\n",
        "            schema_comparison, data_type_mismatches, log_to_file\n",
        "        )\n",
        "\n",
        "        # STEP 2: Enhanced Claude prompt testing with learning system\n",
        "        print(\"🤖 Enhanced Claude testing with learning system...\")\n",
        "        log_to_file(\"🤖 STARTING ENHANCED CLAUDE REPLICATION WITH LEARNING SYSTEM...\", \"ENHANCED CLAUDE REPLICATION TESTING START\")\n",
        "\n",
        "        replication_results = _test_claude_prompt_replication_with_learning_logged(\n",
        "            original_df, modified_df, claude_prompt, schema_comparison, max_attempts=3, log_to_file=log_to_file\n",
        "        )\n",
        "\n",
        "        # STEP 3: Enhanced final analysis\n",
        "        log_to_file(\"📊 GENERATING ENHANCED FINAL ANALYSIS...\", \"ENHANCED FINAL ANALYSIS GENERATION START\")\n",
        "\n",
        "        final_analysis = _get_enhanced_final_analysis_with_prompts_logged(\n",
        "            client, original_info, modified_info, user_description,\n",
        "            total_changes, changed_rows, changed_columns, changed_cells,\n",
        "            schema_comparison, data_type_mismatches, claude_prompt, replication_results, log_to_file\n",
        "        )\n",
        "\n",
        "        # Enhanced metadata\n",
        "        final_analysis[\"raw_gpt_response\"] = final_analysis.get(\"raw_gpt_response\", \"\")\n",
        "        final_analysis[\"complete_comparison_performed\"] = True\n",
        "        final_analysis[\"enhanced_features_applied\"] = True\n",
        "        final_analysis[\"log_file_path\"] = log_file_path\n",
        "        final_analysis[\"full_change_statistics\"] = {\n",
        "            \"total_cells\": total_cells,\n",
        "            \"total_changes\": total_changes,\n",
        "            \"change_density\": change_density,\n",
        "            \"data_type_mismatches\": len(data_type_mismatches),\n",
        "            \"affected_rows\": list(changed_rows),\n",
        "            \"affected_columns\": list(changed_columns),\n",
        "            \"schema_changes\": schema_comparison,\n",
        "            \"replication_tested\": True,\n",
        "            \"replication_success\": replication_results[\"final_success\"],\n",
        "            \"replication_attempts\": len(replication_results[\"attempts\"]),\n",
        "            \"learning_applied\": True,\n",
        "            \"all_changes\": changed_cells\n",
        "        }\n",
        "\n",
        "        # FIXED: Proper string formatting in final log\n",
        "        best_attempt_score = replication_results.get(\"best_attempt\", {}).get(\"match_score\", 0)\n",
        "        best_score_formatted = f\"{best_attempt_score:.2%}\" if best_attempt_score > 0 else \"N/A\"\n",
        "\n",
        "        log_to_file(f\"\"\"ENHANCED ANALYSIS EXECUTION COMPLETED SUCCESSFULLY\n",
        "==========================================\n",
        "Total Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Log File Saved: {log_file_path}\n",
        "Log File Size: {os.path.getsize(log_file_path)} bytes\n",
        "\n",
        "ENHANCED FINAL RESULTS SUMMARY:\n",
        "- Replication Success: {replication_results[\"final_success\"]}\n",
        "- Best Match Score: {best_score_formatted}\n",
        "- Total Replication Attempts: {len(replication_results[\"attempts\"])}\n",
        "- Changes Detected: {total_changes}\n",
        "- Data Type Mismatches: {len(data_type_mismatches)}\n",
        "- Change Density: {change_density*100:.4f}%\n",
        "- Learning System Applied: ✅\n",
        "- Schema Validation Applied: ✅\n",
        "\n",
        "ENHANCEMENTS SUCCESSFULLY APPLIED:\n",
        "1. ✅ Fixed logging string formatting bug\n",
        "2. ✅ Enhanced data type/formatting precision matching\n",
        "3. ✅ Implemented attempt-to-attempt learning system\n",
        "4. ✅ Added comprehensive data type validation\n",
        "\n",
        "COMPLETE ENHANCED FINAL ANALYSIS OBJECT:\n",
        "{json.dumps(final_analysis, indent=2, default=str)}\n",
        "\n",
        "✅ ENHANCED ANALYSIS COMPLETE - ALL LOGS SAVED TO: {log_file_path}\n",
        "\"\"\", \"ENHANCED FINAL EXECUTION RESULTS\")\n",
        "\n",
        "        print(f\"📝 Enhanced analysis with all fixes applied - logs saved to: {log_file_path}\")\n",
        "        return final_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        # FIXED: Proper error logging without string formatting issues\n",
        "        error_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        error_type = type(e).__name__\n",
        "        error_message = str(e)\n",
        "        stack_trace = traceback.format_exc()\n",
        "\n",
        "        original_shape = original_df.shape if 'original_df' in locals() else 'Unknown'\n",
        "        modified_shape = modified_df.shape if 'modified_df' in locals() else 'Unknown'\n",
        "\n",
        "        error_details = f\"\"\"CRITICAL ERROR DURING ENHANCED ANALYSIS EXECUTION\n",
        "========================================\n",
        "Error Time: {error_time}\n",
        "Error Type: {error_type}\n",
        "Error Message: {error_message}\n",
        "\n",
        "FULL STACK TRACE:\n",
        "{stack_trace}\n",
        "\n",
        "EXECUTION CONTEXT:\n",
        "- User Description: \"{user_description}\"\n",
        "- Original DF Shape: {original_shape}\n",
        "- Modified DF Shape: {modified_shape}\n",
        "- Log File: {log_file_path}\n",
        "\n",
        "❌ ENHANCED ANALYSIS FAILED - ERROR LOGGED TO: {log_file_path}\n",
        "\"\"\"\n",
        "\n",
        "        log_to_file(error_details, \"CRITICAL ERROR\")\n",
        "\n",
        "        error_msg = f\"Error in enhanced GPT-4 dataframe analysis: {e}\"\n",
        "        logger.error(error_msg)\n",
        "        print(f\"❌ Enhanced analysis failed but logs saved to: {log_file_path}\")\n",
        "\n",
        "        return {\n",
        "            \"change_summary\": f\"Enhanced dataframe analysis failed: {user_description}\",\n",
        "            \"change_type\": \"data_edit\",\n",
        "            \"session_description\": f\"User made changes to entire dataframe. Description: {user_description}. Error in enhanced analysis: {error_message}\",\n",
        "            \"error\": error_message,\n",
        "            \"enhanced_features_applied\": False,\n",
        "            \"complete_comparison_performed\": False,\n",
        "            \"log_file_path\": log_file_path,\n",
        "            \"error_logged\": True\n",
        "        }\n",
        "\n",
        "\n",
        "# ENHANCEMENT 1: Data Type Schema Analysis Functions\n",
        "def _analyze_dataframe_schema(df, label=\"\"):\n",
        "    \"\"\"\n",
        "    Analyze dataframe schema with detailed data type information\n",
        "    \"\"\"\n",
        "    schema_analysis = {\n",
        "        \"label\": label,\n",
        "        \"shape\": df.shape,\n",
        "        \"column_count\": len(df.columns),\n",
        "        \"row_count\": len(df),\n",
        "        \"columns\": {}\n",
        "    }\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_data = df[col]\n",
        "        schema_analysis[\"columns\"][col] = {\n",
        "            \"dtype\": str(col_data.dtype),\n",
        "            \"python_type\": str(type(col_data.dtype)),\n",
        "            \"null_count\": int(col_data.isnull().sum()),\n",
        "            \"null_percentage\": float(col_data.isnull().sum() / len(df) * 100) if len(df) > 0 else 0,\n",
        "            \"unique_count\": int(col_data.nunique()),\n",
        "            \"memory_usage\": int(col_data.memory_usage(deep=True)),\n",
        "            \"sample_values\": [str(val) for val in col_data.dropna().head(3).tolist()],\n",
        "            \"is_numeric\": pd.api.types.is_numeric_dtype(col_data),\n",
        "            \"is_datetime\": pd.api.types.is_datetime64_any_dtype(col_data),\n",
        "            \"is_categorical\": pd.api.types.is_categorical_dtype(col_data),\n",
        "            \"is_object\": pd.api.types.is_object_dtype(col_data)\n",
        "        }\n",
        "\n",
        "    return schema_analysis\n",
        "\n",
        "\n",
        "def _compare_dataframe_schemas(original_df, modified_df):\n",
        "    \"\"\"\n",
        "    Compare schemas between original and modified dataframes\n",
        "    \"\"\"\n",
        "    orig_schema = _analyze_dataframe_schema(original_df, \"ORIGINAL\")\n",
        "    mod_schema = _analyze_dataframe_schema(modified_df, \"MODIFIED\")\n",
        "\n",
        "    comparison = {\n",
        "        \"shape_changed\": orig_schema[\"shape\"] != mod_schema[\"shape\"],\n",
        "        \"column_count_changed\": orig_schema[\"column_count\"] != mod_schema[\"column_count\"],\n",
        "        \"row_count_changed\": orig_schema[\"row_count\"] != mod_schema[\"row_count\"],\n",
        "        \"column_changes\": {},\n",
        "        \"schema_compatibility_score\": 0.0,\n",
        "        \"critical_issues\": []\n",
        "    }\n",
        "\n",
        "    # Analyze column-by-column changes\n",
        "    all_columns = set(original_df.columns) | set(modified_df.columns)\n",
        "    compatible_columns = 0\n",
        "\n",
        "    for col in all_columns:\n",
        "        if col in original_df.columns and col in modified_df.columns:\n",
        "            orig_col = orig_schema[\"columns\"][col]\n",
        "            mod_col = mod_schema[\"columns\"][col]\n",
        "\n",
        "            column_change = {\n",
        "                \"exists_in_both\": True,\n",
        "                \"dtype_changed\": orig_col[\"dtype\"] != mod_col[\"dtype\"],\n",
        "                \"original_dtype\": orig_col[\"dtype\"],\n",
        "                \"modified_dtype\": mod_col[\"dtype\"],\n",
        "                \"null_count_changed\": orig_col[\"null_count\"] != mod_col[\"null_count\"],\n",
        "                \"type_compatibility\": _check_type_compatibility(orig_col[\"dtype\"], mod_col[\"dtype\"])\n",
        "            }\n",
        "\n",
        "            if column_change[\"type_compatibility\"]:\n",
        "                compatible_columns += 1\n",
        "            else:\n",
        "                comparison[\"critical_issues\"].append(f\"Column '{col}': Incompatible type change {orig_col['dtype']} → {mod_col['dtype']}\")\n",
        "\n",
        "        elif col in original_df.columns:\n",
        "            column_change = {\n",
        "                \"exists_in_both\": False,\n",
        "                \"removed\": True,\n",
        "                \"original_dtype\": orig_schema[\"columns\"][col][\"dtype\"]\n",
        "            }\n",
        "            comparison[\"critical_issues\"].append(f\"Column '{col}' was removed\")\n",
        "\n",
        "        else:\n",
        "            column_change = {\n",
        "                \"exists_in_both\": False,\n",
        "                \"added\": True,\n",
        "                \"modified_dtype\": mod_schema[\"columns\"][col][\"dtype\"]\n",
        "            }\n",
        "            comparison[\"critical_issues\"].append(f\"Column '{col}' was added\")\n",
        "\n",
        "        comparison[\"column_changes\"][col] = column_change\n",
        "\n",
        "    # Calculate compatibility score\n",
        "    total_columns = len(all_columns)\n",
        "    comparison[\"schema_compatibility_score\"] = compatible_columns / total_columns if total_columns > 0 else 1.0\n",
        "\n",
        "    return comparison\n",
        "\n",
        "\n",
        "def _check_type_compatibility(orig_dtype, mod_dtype):\n",
        "    \"\"\"\n",
        "    Check if two data types are compatible for replication\n",
        "    \"\"\"\n",
        "    # Exact match\n",
        "    if str(orig_dtype) == str(mod_dtype):\n",
        "        return True\n",
        "\n",
        "    # Compatible numeric types\n",
        "    numeric_types = ['int64', 'int32', 'float64', 'float32', 'number']\n",
        "    if any(t in str(orig_dtype).lower() for t in numeric_types) and any(t in str(mod_dtype).lower() for t in numeric_types):\n",
        "        return True\n",
        "\n",
        "    # Compatible string/object types\n",
        "    string_types = ['object', 'string', 'str']\n",
        "    if any(t in str(orig_dtype).lower() for t in string_types) and any(t in str(mod_dtype).lower() for t in string_types):\n",
        "        return True\n",
        "\n",
        "    # Compatible datetime types\n",
        "    datetime_types = ['datetime', 'timestamp']\n",
        "    if any(t in str(orig_dtype).lower() for t in datetime_types) and any(t in str(mod_dtype).lower() for t in datetime_types):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# ENHANCEMENT 2: Enhanced Cell Comparison with Data Type Awareness\n",
        "def _enhanced_cell_comparison(orig_val, mod_val, row_idx, col_name, orig_dtype, mod_dtype):\n",
        "    \"\"\"\n",
        "    Enhanced cell comparison with data type awareness and precision handling\n",
        "    \"\"\"\n",
        "    change_detail = {\n",
        "        \"row\": row_idx,\n",
        "        \"column\": col_name,\n",
        "        \"original\": str(orig_val),\n",
        "        \"modified\": str(mod_val),\n",
        "        \"original_dtype\": str(orig_dtype),\n",
        "        \"modified_dtype\": str(mod_dtype),\n",
        "        \"data_type_changed\": str(orig_dtype) != str(mod_dtype),\n",
        "        \"change_type\": \"no_change\"\n",
        "    }\n",
        "\n",
        "    # Handle NaN comparisons\n",
        "    if pd.isna(orig_val) and pd.isna(mod_val):\n",
        "        return False, change_detail\n",
        "    elif pd.isna(orig_val) or pd.isna(mod_val):\n",
        "        change_detail[\"change_type\"] = \"nan_change\"\n",
        "        return True, change_detail\n",
        "\n",
        "    # Data type change detection\n",
        "    if str(orig_dtype) != str(mod_dtype):\n",
        "        change_detail[\"change_type\"] = \"data_type_change\"\n",
        "        # Still check if values are equivalent despite type change\n",
        "        if _are_values_equivalent(orig_val, mod_val):\n",
        "            change_detail[\"change_type\"] = \"data_type_change_equivalent_value\"\n",
        "            return True, change_detail\n",
        "        else:\n",
        "            return True, change_detail\n",
        "\n",
        "    # Enhanced value comparison with precision handling\n",
        "    if _are_values_equivalent(orig_val, mod_val):\n",
        "        return False, change_detail\n",
        "\n",
        "    # Check for formatting differences\n",
        "    if _are_formatting_differences_only(orig_val, mod_val):\n",
        "        change_detail[\"change_type\"] = \"formatting_change\"\n",
        "        return True, change_detail\n",
        "\n",
        "    # Significant value change\n",
        "    change_detail[\"change_type\"] = \"value_change\"\n",
        "    return True, change_detail\n",
        "\n",
        "\n",
        "def _are_values_equivalent(val1, val2):\n",
        "    \"\"\"\n",
        "    Check if two values are equivalent despite potential formatting differences\n",
        "    \"\"\"\n",
        "    # Exact string match after stripping\n",
        "    if str(val1).strip() == str(val2).strip():\n",
        "        return True\n",
        "\n",
        "    # Numeric equivalence check\n",
        "    try:\n",
        "        # Handle currency and comma formatting\n",
        "        clean_val1 = str(val1).replace(',', '').replace('$', '').strip()\n",
        "        clean_val2 = str(val2).replace(',', '').replace('$', '').strip()\n",
        "\n",
        "        num1 = float(clean_val1)\n",
        "        num2 = float(clean_val2)\n",
        "\n",
        "        # Allow small floating point differences\n",
        "        return abs(num1 - num2) < 1e-10\n",
        "\n",
        "    except (ValueError, TypeError):\n",
        "        pass\n",
        "\n",
        "    # Date equivalence check\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        date1 = pd.to_datetime(val1)\n",
        "        date2 = pd.to_datetime(val2)\n",
        "        return date1 == date2\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def _are_formatting_differences_only(val1, val2):\n",
        "    \"\"\"\n",
        "    Check if differences are only formatting (whitespace, case, etc.)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Case-insensitive comparison\n",
        "        if str(val1).strip().lower() == str(val2).strip().lower():\n",
        "            return True\n",
        "\n",
        "        # Numeric formatting differences\n",
        "        clean_val1 = str(val1).replace(',', '').replace('$', '').replace(' ', '').strip()\n",
        "        clean_val2 = str(val2).replace(',', '').replace('$', '').replace(' ', '').strip()\n",
        "\n",
        "        if clean_val1 == clean_val2:\n",
        "            return True\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# ENHANCEMENT 3: Enhanced GPT-4.1 Prompt Generation with Schema Awareness\n",
        "def _generate_enhanced_claude_prompt_with_gpt4_logged(client, original_info, modified_info, user_description,\n",
        "                                                     total_changes, changed_rows, changed_columns, changed_cells,\n",
        "                                                     schema_comparison, data_type_mismatches, log_to_file):\n",
        "    \"\"\"\n",
        "    Enhanced GPT-4.1 prompt generation with comprehensive schema awareness and data type focus\n",
        "    \"\"\"\n",
        "    # Prepare detailed schema information for GPT-4.1\n",
        "    schema_issues = schema_comparison.get(\"critical_issues\", [])\n",
        "    compatibility_score = schema_comparison.get(\"schema_compatibility_score\", 0)\n",
        "\n",
        "    sample_changes = changed_cells[:15] if changed_cells else []\n",
        "    sample_type_mismatches = data_type_mismatches[:5] if data_type_mismatches else []\n",
        "\n",
        "    enhanced_prompt_request = f\"\"\"\n",
        "    You are an expert at analyzing dataframe changes and generating PRECISE prompts for Claude 3.7 to replicate manual edits with EXACT data type and formatting precision.\n",
        "\n",
        "    CRITICAL REQUIREMENTS:\n",
        "    1. Focus on DATA TYPE PRECISION - ensure exact dtype matching\n",
        "    2. Handle FORMATTING with precision (decimals, currency, dates)\n",
        "    3. Provide SPECIFIC transformation logic, not general filtering\n",
        "    4. Include EXPLICIT data type conversion instructions\n",
        "    5. Address schema compatibility issues\n",
        "\n",
        "    ORIGINAL DATAFRAME:\n",
        "    Shape: {original_info['shape']}\n",
        "    Columns: {original_info['columns']}\n",
        "    Data Types: {original_info['dtype_details']}\n",
        "\n",
        "    Data Sample:\n",
        "    {original_info['full_data'][:3000]}\n",
        "\n",
        "    MODIFIED DATAFRAME:\n",
        "    Shape: {modified_info['shape']}\n",
        "    Columns: {modified_info['columns']}\n",
        "    Data Types: {modified_info['dtype_details']}\n",
        "\n",
        "    Data Sample:\n",
        "    {modified_info['full_data'][:3000]}\n",
        "\n",
        "    ENHANCED CHANGE ANALYSIS:\n",
        "    - Total changes: {total_changes}\n",
        "    - Schema compatibility score: {compatibility_score:.2f}\n",
        "    - Data type mismatches: {len(data_type_mismatches)}\n",
        "    - Critical schema issues: {schema_issues}\n",
        "    - Changed rows: {list(changed_rows)[:20] if changed_rows else []}\n",
        "    - Changed columns: {list(changed_columns)}\n",
        "\n",
        "    SAMPLE DETAILED CHANGES:\n",
        "    {json.dumps(sample_changes, indent=2)}\n",
        "\n",
        "    DATA TYPE MISMATCHES DETECTED:\n",
        "    {json.dumps(sample_type_mismatches, indent=2)}\n",
        "\n",
        "    USER DESCRIPTION: \"{user_description}\"\n",
        "\n",
        "    Generate a PRECISE prompt for Claude 3.7 that will replicate these exact changes with:\n",
        "\n",
        "    1. EXACT DATA TYPE SPECIFICATIONS:\n",
        "       - Include explicit dtype conversion commands\n",
        "       - Specify decimal precision for floats\n",
        "       - Handle string formatting requirements\n",
        "\n",
        "    2. SPECIFIC TRANSFORMATION LOGIC:\n",
        "       - Exact cell selection criteria\n",
        "       - Precise value transformation formulas\n",
        "       - Row-by-row or column-by-column instructions if needed\n",
        "\n",
        "    3. SCHEMA VALIDATION:\n",
        "       - Ensure output matches target schema exactly\n",
        "       - Include data type verification steps\n",
        "\n",
        "    4. BUSINESS CONTEXT for rent roll data:\n",
        "       - Consider tenant information updates\n",
        "       - Handle rent calculations precisely\n",
        "       - Manage occupancy status changes\n",
        "\n",
        "    The prompt should be executable Python pandas code that produces the EXACT modified dataframe.\n",
        "\n",
        "    Return ONLY the Claude prompt text that focuses on precision and data type accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Log the enhanced GPT-4.1 prompt\n",
        "    log_to_file(f\"\"\"ENHANCED GPT-4.1 PROMPT TO GENERATE CLAUDE INSTRUCTIONS:\n",
        "Model: gpt-4.1\n",
        "Temperature: 0.05 (Lower for precision)\n",
        "Max Tokens: 4000\n",
        "\n",
        "ENHANCED PROMPT FEATURES:\n",
        "- Schema compatibility analysis included\n",
        "- Data type mismatch focus\n",
        "- Formatting precision requirements\n",
        "- Sample change details provided\n",
        "\n",
        "COMPLETE ENHANCED PROMPT SENT TO GPT-4.1:\n",
        "{'-'*60}\n",
        "{enhanced_prompt_request}\n",
        "{'-'*60}\n",
        "\"\"\", \"ENHANCED GPT-4.1 REQUEST\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert at generating precise data transformation prompts with exact data type and formatting specifications. Focus on precision and schema accuracy.\"},\n",
        "                {\"role\": \"user\", \"content\": enhanced_prompt_request}\n",
        "            ],\n",
        "            max_tokens=4000,\n",
        "            temperature=0.05  # Lower temperature for more precision\n",
        "        )\n",
        "\n",
        "        enhanced_gpt_response = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Log enhanced GPT-4.1 response\n",
        "        log_to_file(f\"\"\"ENHANCED GPT-4.1 RESPONSE (CLAUDE PROMPT):\n",
        "Response Length: {len(enhanced_gpt_response)} characters\n",
        "Tokens Used: Approximately {len(enhanced_gpt_response.split())} words\n",
        "Schema Focus: ✅ Applied\n",
        "Data Type Precision: ✅ Applied\n",
        "\n",
        "GENERATED ENHANCED CLAUDE PROMPT:\n",
        "{'-'*60}\n",
        "{enhanced_gpt_response}\n",
        "{'-'*60}\n",
        "\"\"\", \"ENHANCED GPT-4.1 RESPONSE\")\n",
        "\n",
        "        return enhanced_gpt_response\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Enhanced GPT-4.1 API Error: {str(e)}\"\n",
        "        log_to_file(f\"\"\"ENHANCED GPT-4.1 API CALL FAILED:\n",
        "Error: {error_msg}\n",
        "Fallback: Using enhanced basic prompt with schema awareness\n",
        "\"\"\")\n",
        "\n",
        "        # Enhanced fallback prompt with schema awareness\n",
        "        fallback_prompt = f\"\"\"Replicate the manual edits described as: {user_description}\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Maintain exact data types: {modified_info['dtype_details']}\n",
        "- Total changes to make: {total_changes}\n",
        "- Focus on columns: {list(changed_columns)}\n",
        "- Ensure precise formatting and data type conversion\n",
        "- Output must match target schema exactly\n",
        "\n",
        "Sample changes for reference:\n",
        "{json.dumps(sample_changes[:5], indent=2)}\n",
        "\"\"\"\n",
        "        return fallback_prompt\n",
        "\n",
        "\n",
        "# ENHANCEMENT 4: Claude Testing with Learning System\n",
        "def _test_claude_prompt_replication_with_learning_logged(original_df, target_df, claude_prompt, schema_comparison, max_attempts=3, log_to_file=None):\n",
        "    \"\"\"\n",
        "    Enhanced Claude testing with attempt-to-attempt learning system\n",
        "    \"\"\"\n",
        "    attempts = []\n",
        "    current_prompt = claude_prompt\n",
        "    learning_history = []\n",
        "\n",
        "    log_to_file(f\"\"\"ENHANCED CLAUDE REPLICATION TESTING WITH LEARNING STARTED:\n",
        "Maximum Attempts: {max_attempts}\n",
        "Target DataFrame Shape: {target_df.shape}\n",
        "Original DataFrame Shape: {original_df.shape}\n",
        "Schema Compatibility Score: {schema_comparison.get('schema_compatibility_score', 0):.2f}\n",
        "Critical Schema Issues: {len(schema_comparison.get('critical_issues', []))}\n",
        "\n",
        "LEARNING SYSTEM FEATURES:\n",
        "✅ Attempt-to-attempt feedback loop\n",
        "✅ Schema validation focus\n",
        "✅ Data type precision tracking\n",
        "✅ Progressive prompt refinement\n",
        "\n",
        "INITIAL ENHANCED CLAUDE PROMPT TO TEST:\n",
        "{'-'*60}\n",
        "{claude_prompt}\n",
        "{'-'*60}\n",
        "\"\"\", \"ENHANCED REPLICATION TESTING INITIALIZATION\")\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        print(f\"🔄 Enhanced Attempt {attempt + 1}/{max_attempts} with learning system\")\n",
        "        log_to_file(f\"Starting enhanced attempt {attempt + 1}/{max_attempts} with accumulated learning...\", f\"ENHANCED ATTEMPT {attempt + 1}\")\n",
        "\n",
        "        try:\n",
        "            # Apply learning from previous attempts\n",
        "            if attempt > 0:\n",
        "                current_prompt = _apply_learning_to_prompt(current_prompt, learning_history, target_df, log_to_file)\n",
        "\n",
        "            # Call Claude with enhanced logging\n",
        "            claude_response = _call_enhanced_claude_logged(current_prompt, original_df, target_df, schema_comparison, log_to_file, attempt + 1)\n",
        "\n",
        "            # Enhanced code extraction\n",
        "            code_blocks = _extract_enhanced_code_blocks(claude_response)\n",
        "\n",
        "            log_to_file(f\"\"\"ENHANCED CODE EXTRACTION RESULTS:\n",
        "Found {len(code_blocks)} code blocks\n",
        "Enhanced Features: Schema validation, data type checking\n",
        "\n",
        "Code Blocks Extracted:\n",
        "\"\"\")\n",
        "\n",
        "            for i, code in enumerate(code_blocks):\n",
        "                log_to_file(f\"Enhanced Code Block {i+1}:\\n```python\\n{code}\\n```\\n\")\n",
        "\n",
        "            if not code_blocks:\n",
        "                attempt_result = {\n",
        "                    \"attempt\": attempt + 1,\n",
        "                    \"success\": False,\n",
        "                    \"error\": \"No code found in Claude response\",\n",
        "                    \"prompt_used\": current_prompt,\n",
        "                    \"learning_applied\": len(learning_history) > 0\n",
        "                }\n",
        "                attempts.append(attempt_result)\n",
        "                learning_history.append({\n",
        "                    \"attempt\": attempt + 1,\n",
        "                    \"issue\": \"no_code_generated\",\n",
        "                    \"resolution\": \"request_explicit_code_blocks\"\n",
        "                })\n",
        "                log_to_file(\"No code blocks found - adding to learning history\")\n",
        "                continue\n",
        "\n",
        "            # Enhanced code execution with schema validation\n",
        "            test_df = original_df.copy()\n",
        "            exec_globals = {\n",
        "                \"df\": test_df,\n",
        "                \"pd\": pd,\n",
        "                \"np\": np,\n",
        "                \"os\": os,\n",
        "                \"datetime\": datetime\n",
        "            }\n",
        "\n",
        "            log_to_file(\"Starting enhanced code execution with schema validation...\")\n",
        "            execution_output = \"\"\n",
        "            execution_success = True\n",
        "\n",
        "            for i, code in enumerate(code_blocks):\n",
        "                try:\n",
        "                    log_to_file(f\"Executing enhanced code block {i+1}...\")\n",
        "                    exec(code, exec_globals)\n",
        "                    execution_output += f\"Enhanced code block {i+1} executed successfully\\n\"\n",
        "                except Exception as exec_error:\n",
        "                    execution_output += f\"Enhanced code block {i+1} failed: {str(exec_error)}\\n\"\n",
        "                    log_to_file(f\"Enhanced code block {i+1} execution error: {str(exec_error)}\")\n",
        "                    execution_success = False\n",
        "                    learning_history.append({\n",
        "                        \"attempt\": attempt + 1,\n",
        "                        \"issue\": f\"execution_error_{i+1}\",\n",
        "                        \"error\": str(exec_error),\n",
        "                        \"resolution\": \"fix_syntax_and_logic\"\n",
        "                    })\n",
        "\n",
        "            result_df = exec_globals[\"df\"]\n",
        "            log_to_file(f\"Enhanced execution completed. Result DataFrame shape: {result_df.shape}\")\n",
        "\n",
        "            # Enhanced validation with schema checking\n",
        "            enhanced_validation = _enhanced_result_validation(target_df, result_df, schema_comparison)\n",
        "            match_score = enhanced_validation[\"overall_score\"]\n",
        "            schema_match = enhanced_validation[\"schema_match\"]\n",
        "            data_type_match = enhanced_validation[\"data_type_match\"]\n",
        "\n",
        "            attempt_result = {\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"success\": match_score >= 0.95 and schema_match,\n",
        "                \"match_score\": match_score,\n",
        "                \"schema_match\": schema_match,\n",
        "                \"data_type_match\": data_type_match,\n",
        "                \"enhanced_validation\": enhanced_validation,\n",
        "                \"generated_code\": code_blocks,\n",
        "                \"prompt_used\": current_prompt,\n",
        "                \"result_shape\": result_df.shape,\n",
        "                \"target_shape\": target_df.shape,\n",
        "                \"execution_output\": execution_output,\n",
        "                \"execution_success\": execution_success,\n",
        "                \"learning_applied\": len(learning_history) > 0\n",
        "            }\n",
        "\n",
        "            attempts.append(attempt_result)\n",
        "\n",
        "            log_to_file(f\"\"\"ENHANCED ATTEMPT {attempt + 1} RESULTS:\n",
        "Overall Match Score: {match_score:.2%}\n",
        "Schema Match: {'✅ PASSED' if schema_match else '❌ FAILED'}\n",
        "Data Type Match: {'✅ PASSED' if data_type_match else '❌ FAILED'}\n",
        "Success Threshold (95% + Schema): {'✅ PASSED' if (match_score >= 0.95 and schema_match) else '❌ FAILED'}\n",
        "Result Shape: {result_df.shape}\n",
        "Target Shape: {target_df.shape}\n",
        "Execution Success: {'✅ YES' if execution_success else '❌ NO'}\n",
        "Learning Applied: {'✅ YES' if len(learning_history) > 0 else '❌ NO'}\n",
        "\n",
        "ENHANCED VALIDATION DETAILS:\n",
        "{json.dumps(enhanced_validation, indent=2)}\n",
        "\n",
        "Execution Output:\n",
        "{execution_output}\n",
        "\"\"\")\n",
        "\n",
        "            print(f\"📊 Enhanced Match Score: {match_score:.2%}, Schema: {'✅' if schema_match else '❌'}\")\n",
        "\n",
        "            # Add learning insights for next attempt\n",
        "            if match_score < 0.95 or not schema_match:\n",
        "                learning_insights = _analyze_failure_for_learning(target_df, result_df, enhanced_validation)\n",
        "                learning_history.extend(learning_insights)\n",
        "                log_to_file(f\"Learning insights added: {json.dumps(learning_insights, indent=2)}\")\n",
        "\n",
        "            if match_score >= 0.95 and schema_match:\n",
        "                print(\"✅ Enhanced replication successful!\")\n",
        "                log_to_file(\"🎉 ENHANCED REPLICATION SUCCESSFUL! Stopping attempts.\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            attempt_result = {\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"success\": False,\n",
        "                \"error\": error_msg,\n",
        "                \"prompt_used\": current_prompt,\n",
        "                \"learning_applied\": len(learning_history) > 0\n",
        "            }\n",
        "            attempts.append(attempt_result)\n",
        "            learning_history.append({\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"issue\": \"execution_exception\",\n",
        "                \"error\": error_msg,\n",
        "                \"resolution\": \"improve_error_handling\"\n",
        "            })\n",
        "            log_to_file(f\"ENHANCED ATTEMPT {attempt + 1} FAILED with error: {error_msg}\")\n",
        "            print(f\"❌ Enhanced Error: {error_msg}\")\n",
        "\n",
        "    final_success = any(attempt[\"success\"] for attempt in attempts)\n",
        "    best_attempt = max(attempts, key=lambda x: x.get(\"match_score\", 0)) if attempts else None\n",
        "\n",
        "    log_to_file(f\"\"\"ENHANCED REPLICATION TESTING COMPLETE:\n",
        "Final Success: {final_success}\n",
        "Best Match Score: {best_attempt.get('match_score', 0):.2% if best_attempt else 'N/A'}\n",
        "Best Schema Match: {best_attempt.get('schema_match', False) if best_attempt else False}\n",
        "Total Attempts: {len(attempts)}\n",
        "Total Learning Insights: {len(learning_history)}\n",
        "\n",
        "LEARNING HISTORY SUMMARY:\n",
        "{json.dumps(learning_history, indent=2)}\n",
        "\n",
        "ALL ENHANCED ATTEMPTS SUMMARY:\n",
        "\"\"\", \"ENHANCED REPLICATION TESTING RESULTS\")\n",
        "\n",
        "    for i, attempt in enumerate(attempts, 1):\n",
        "        success = attempt.get('success', False)\n",
        "        score = attempt.get('match_score', 0)\n",
        "        schema_match = attempt.get('schema_match', False)\n",
        "        learning = attempt.get('learning_applied', False)\n",
        "        error = attempt.get('error', 'None')\n",
        "\n",
        "        log_to_file(f\"Enhanced Attempt {i}: Success={success}, Score={score:.2%}, Schema={schema_match}, Learning={learning}, Error={error}\")\n",
        "\n",
        "    return {\n",
        "        \"attempts\": attempts,\n",
        "        \"final_success\": final_success,\n",
        "        \"best_attempt\": best_attempt,\n",
        "        \"final_prompt\": best_attempt[\"prompt_used\"] if best_attempt else claude_prompt,\n",
        "        \"learning_history\": learning_history,\n",
        "        \"learning_applied\": len(learning_history) > 0\n",
        "    }\n",
        "\n",
        "\n",
        "# Learning System Helper Functions\n",
        "def _apply_learning_to_prompt(base_prompt, learning_history, target_df, log_to_file):\n",
        "    \"\"\"\n",
        "    Apply accumulated learning to improve the prompt\n",
        "    \"\"\"\n",
        "    if not learning_history:\n",
        "        return base_prompt\n",
        "\n",
        "    # Analyze learning patterns\n",
        "    common_issues = {}\n",
        "    for learning in learning_history:\n",
        "        issue = learning.get(\"issue\", \"unknown\")\n",
        "        if issue in common_issues:\n",
        "            common_issues[issue] += 1\n",
        "        else:\n",
        "            common_issues[issue] = 1\n",
        "\n",
        "    # Generate improvement suggestions based on learning\n",
        "    improvements = []\n",
        "\n",
        "    if \"execution_error\" in [l.get(\"issue\", \"\") for l in learning_history]:\n",
        "        improvements.append(\"Focus on syntactically correct pandas operations\")\n",
        "\n",
        "    if \"schema_mismatch\" in [l.get(\"issue\", \"\") for l in learning_history]:\n",
        "        improvements.append(f\"Ensure exact data types: {dict(target_df.dtypes)}\")\n",
        "\n",
        "    if \"data_type_mismatch\" in [l.get(\"issue\", \"\") for l in learning_history]:\n",
        "        improvements.append(\"Include explicit dtype conversions using .astype()\")\n",
        "\n",
        "    if \"no_code_generated\" in [l.get(\"issue\", \"\") for l in learning_history]:\n",
        "        improvements.append(\"Generate code in ```python ``` blocks\")\n",
        "\n",
        "    # Apply improvements to prompt\n",
        "    if improvements:\n",
        "        learning_section = f\"\"\"\n",
        "\n",
        "LEARNING FROM PREVIOUS ATTEMPTS:\n",
        "Based on previous attempts, please pay special attention to:\n",
        "{chr(10).join([f\"• {improvement}\" for improvement in improvements])}\n",
        "\n",
        "COMMON ISSUES TO AVOID:\n",
        "{chr(10).join([f\"• {issue}: occurred {count} time(s)\" for issue, count in common_issues.items()])}\n",
        "\"\"\"\n",
        "        improved_prompt = base_prompt + learning_section\n",
        "\n",
        "        log_to_file(f\"\"\"LEARNING APPLIED TO PROMPT:\n",
        "Improvements Added: {len(improvements)}\n",
        "Common Issues Addressed: {len(common_issues)}\n",
        "\n",
        "LEARNING SECTION ADDED:\n",
        "{learning_section}\n",
        "\"\"\")\n",
        "\n",
        "        return improved_prompt\n",
        "\n",
        "    return base_prompt\n",
        "\n",
        "\n",
        "def _analyze_failure_for_learning(target_df, result_df, enhanced_validation):\n",
        "    \"\"\"\n",
        "    Analyze failure to extract learning insights for next attempt\n",
        "    \"\"\"\n",
        "    insights = []\n",
        "\n",
        "    # Schema issues\n",
        "    if not enhanced_validation.get(\"schema_match\", True):\n",
        "        if enhanced_validation.get(\"shape_mismatch\", False):\n",
        "            insights.append({\n",
        "                \"issue\": \"shape_mismatch\",\n",
        "                \"details\": f\"Target: {target_df.shape}, Got: {result_df.shape}\",\n",
        "                \"resolution\": \"check_row_filtering_and_column_selection\"\n",
        "            })\n",
        "\n",
        "        if enhanced_validation.get(\"column_mismatch\", False):\n",
        "            insights.append({\n",
        "                \"issue\": \"column_mismatch\",\n",
        "                \"details\": f\"Target cols: {list(target_df.columns)}, Got cols: {list(result_df.columns)}\",\n",
        "                \"resolution\": \"ensure_exact_column_names_and_order\"\n",
        "            })\n",
        "\n",
        "    # Data type issues\n",
        "    if not enhanced_validation.get(\"data_type_match\", True):\n",
        "        type_mismatches = enhanced_validation.get(\"data_type_details\", {})\n",
        "        for col, details in type_mismatches.items():\n",
        "            if details.get(\"mismatch\", False):\n",
        "                insights.append({\n",
        "                    \"issue\": \"data_type_mismatch\",\n",
        "                    \"column\": col,\n",
        "                    \"expected\": details.get(\"target_dtype\"),\n",
        "                    \"got\": details.get(\"result_dtype\"),\n",
        "                    \"resolution\": f\"add_explicit_conversion: df['{col}'] = df['{col}'].astype('{details.get('target_dtype')}')\"\n",
        "                })\n",
        "\n",
        "    # Content issues\n",
        "    content_score = enhanced_validation.get(\"content_score\", 1.0)\n",
        "    if content_score < 0.9:\n",
        "        insights.append({\n",
        "            \"issue\": \"content_mismatch\",\n",
        "            \"score\": content_score,\n",
        "            \"resolution\": \"review_transformation_logic_for_cell_values\"\n",
        "        })\n",
        "\n",
        "    return insights\n",
        "\n",
        "\n",
        "# Enhanced Result Validation\n",
        "def _enhanced_result_validation(target_df, result_df, schema_comparison):\n",
        "    \"\"\"\n",
        "    Enhanced validation with comprehensive schema and data type checking\n",
        "    \"\"\"\n",
        "    validation = {\n",
        "        \"overall_score\": 0.0,\n",
        "        \"schema_match\": False,\n",
        "        \"data_type_match\": False,\n",
        "        \"content_score\": 0.0,\n",
        "        \"shape_mismatch\": False,\n",
        "        \"column_mismatch\": False,\n",
        "        \"data_type_details\": {}\n",
        "    }\n",
        "\n",
        "    # Shape validation\n",
        "    if target_df.shape != result_df.shape:\n",
        "        validation[\"shape_mismatch\"] = True\n",
        "        validation[\"overall_score\"] = 0.0\n",
        "        return validation\n",
        "\n",
        "    # Column validation\n",
        "    if list(target_df.columns) != list(result_df.columns):\n",
        "        validation[\"column_mismatch\"] = True\n",
        "        validation[\"overall_score\"] = 0.1\n",
        "        return validation\n",
        "\n",
        "    validation[\"schema_match\"] = True\n",
        "\n",
        "    # Data type validation\n",
        "    type_match_count = 0\n",
        "    total_columns = len(target_df.columns)\n",
        "\n",
        "    for col in target_df.columns:\n",
        "        target_dtype = str(target_df[col].dtype)\n",
        "        result_dtype = str(result_df[col].dtype)\n",
        "\n",
        "        type_compatible = _check_type_compatibility(target_dtype, result_dtype)\n",
        "\n",
        "        validation[\"data_type_details\"][col] = {\n",
        "            \"target_dtype\": target_dtype,\n",
        "            \"result_dtype\": result_dtype,\n",
        "            \"exact_match\": target_dtype == result_dtype,\n",
        "            \"compatible\": type_compatible,\n",
        "            \"mismatch\": not type_compatible\n",
        "        }\n",
        "\n",
        "        if type_compatible:\n",
        "            type_match_count += 1\n",
        "\n",
        "    validation[\"data_type_match\"] = type_match_count == total_columns\n",
        "\n",
        "    # Content validation using the enhanced comparison\n",
        "    if validation[\"schema_match\"] and len(target_df) > 0:\n",
        "        validation[\"content_score\"] = _calculate_enhanced_match_score(target_df, result_df)\n",
        "    else:\n",
        "        validation[\"content_score\"] = 0.0\n",
        "\n",
        "    # Overall score calculation\n",
        "    schema_weight = 0.3\n",
        "    dtype_weight = 0.3\n",
        "    content_weight = 0.4\n",
        "\n",
        "    schema_score = 1.0 if validation[\"schema_match\"] else 0.0\n",
        "    dtype_score = type_match_count / total_columns if total_columns > 0 else 1.0\n",
        "\n",
        "    validation[\"overall_score\"] = (\n",
        "        schema_weight * schema_score +\n",
        "        dtype_weight * dtype_score +\n",
        "        content_weight * validation[\"content_score\"]\n",
        "    )\n",
        "\n",
        "    return validation\n",
        "\n",
        "\n",
        "def _calculate_enhanced_match_score(target_df, result_df):\n",
        "    \"\"\"\n",
        "    Enhanced match score calculation with improved precision\n",
        "    \"\"\"\n",
        "    if target_df.empty and result_df.empty:\n",
        "        return 1.0\n",
        "    if target_df.empty or result_df.empty:\n",
        "        return 0.0\n",
        "    if target_df.shape != result_df.shape:\n",
        "        return 0.0\n",
        "    if list(target_df.columns) != list(result_df.columns):\n",
        "        return 0.0\n",
        "\n",
        "    total_cells = 0\n",
        "    matching_cells = 0\n",
        "\n",
        "    for i in range(len(target_df)):\n",
        "        for col in target_df.columns:\n",
        "            total_cells += 1\n",
        "\n",
        "            try:\n",
        "                target_val = target_df.iloc[i][col]\n",
        "                result_val = result_df.iloc[i][col]\n",
        "\n",
        "                if _are_values_equivalent(target_val, result_val):\n",
        "                    matching_cells += 1\n",
        "\n",
        "            except (IndexError, KeyError):\n",
        "                continue\n",
        "\n",
        "    return matching_cells / total_cells if total_cells > 0 else 0.0\n",
        "\n",
        "\n",
        "# Enhanced Claude API Call\n",
        "def _call_enhanced_claude_logged(prompt, original_df, target_df, schema_comparison, log_to_file, attempt_number):\n",
        "    \"\"\"\n",
        "    Enhanced Claude API call with schema context and improved prompting\n",
        "    \"\"\"\n",
        "    target_schema_info = {\n",
        "        \"shape\": target_df.shape,\n",
        "        \"dtypes\": dict(target_df.dtypes),\n",
        "        \"columns\": list(target_df.columns),\n",
        "        \"sample_data\": target_df.head(3).to_string()\n",
        "    }\n",
        "\n",
        "    log_to_file(f\"\"\"CALLING ENHANCED CLAUDE API - ATTEMPT {attempt_number}:\n",
        "Model: claude-sonnet-4-20250514\n",
        "Temperature: 0.1 (Optimized for precision)\n",
        "Max Tokens: 3000\n",
        "\n",
        "ENHANCED FEATURES:\n",
        "✅ Target schema context provided\n",
        "✅ Data type precision focus\n",
        "✅ Schema compatibility warnings included\n",
        "\n",
        "PROMPT BEING SENT TO CLAUDE:\n",
        "{'-'*60}\n",
        "{prompt}\n",
        "{'-'*60}\n",
        "\n",
        "ENHANCED CONTEXT PROVIDED:\n",
        "Original DataFrame Shape: {original_df.shape}\n",
        "Target DataFrame Shape: {target_df.shape}\n",
        "Schema Compatibility Score: {schema_comparison.get('schema_compatibility_score', 0):.2f}\n",
        "Critical Issues: {len(schema_comparison.get('critical_issues', []))}\n",
        "\n",
        "TARGET SCHEMA REQUIREMENTS:\n",
        "{json.dumps(target_schema_info, indent=2, default=str)}\n",
        "\"\"\", f\"ENHANCED CLAUDE API CALL {attempt_number}\")\n",
        "\n",
        "    try:\n",
        "        # Get Anthropic client\n",
        "        anthropic_client = Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "        # Enhanced dataframe context with target schema\n",
        "        enhanced_df_summary = f\"\"\"\n",
        "        CURRENT DATAFRAME CONTENT:\n",
        "        {original_df.to_string(max_rows=50)}\n",
        "\n",
        "        CURRENT DATAFRAME STATISTICS:\n",
        "        - Shape: {original_df.shape}\n",
        "        - Columns: {list(original_df.columns)}\n",
        "        - Data types: {dict(original_df.dtypes)}\n",
        "        - Null values per column: {dict(original_df.isnull().sum())}\n",
        "\n",
        "        TARGET SCHEMA REQUIREMENTS (CRITICAL):\n",
        "        - Target Shape: {target_df.shape}\n",
        "        - Target Columns: {list(target_df.columns)}\n",
        "        - Target Data Types: {dict(target_df.dtypes)}\n",
        "        - Schema Compatibility Issues: {schema_comparison.get('critical_issues', [])}\n",
        "\n",
        "        SAMPLE TARGET DATA:\n",
        "        {target_df.head(3).to_string()}\n",
        "        \"\"\"\n",
        "\n",
        "        # Enhanced system prompt\n",
        "        enhanced_claude_system_prompt = \"\"\"You are an expert Python data analyst specializing in applying existing code to new dataframes with minimal modifications.\n",
        "\n",
        "        CORE PRINCIPLES:\n",
        "        1. Preserve original business logic at all costs\n",
        "        2. Make only minimal changes necessary for compatibility\n",
        "        3. If column names differ, update references but keep all calculations identical\n",
        "        4. Maintain the same data transformation goals\n",
        "        5. Use the original code structure as much as possible\n",
        "\n",
        "        Your job is to take working code and adapt it minimally to work with a new dataframe structure.\"\"\"\n",
        "\n",
        "        # Enhanced messages for Claude\n",
        "        # enhanced_claude_messages = [\n",
        "        #     {\n",
        "        #         \"role\": \"user\",\n",
        "        #         \"content\": f\"Here is the enhanced context:\\n\\n{enhanced_df_summary}\\n\\nTASK WITH PRECISION REQUIREMENTS: {prompt}\\n\\nGenerate precise Python code to accomplish this exact transformation with schema validation.\"\n",
        "        #     }\n",
        "        # ]\n",
        "\n",
        "        # Call Claude\n",
        "        claude_response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            system=enhanced_claude_system_prompt,\n",
        "            messages=[{\"role\": \"user\", \"content\": full_claude_prompt}],\n",
        "            max_tokens=3000,\n",
        "            temperature=0.1  # Lower temperature for more precision\n",
        "        )\n",
        "\n",
        "        # Extract response text\n",
        "        response_text = claude_response.content[0].text\n",
        "\n",
        "        log_to_file(f\"\"\"ENHANCED CLAUDE API RESPONSE - ATTEMPT {attempt_number}:\n",
        "Response Length: {len(response_text)} characters\n",
        "Enhanced Features Applied: ✅\n",
        "Response Received Successfully: ✅\n",
        "\n",
        "FULL ENHANCED CLAUDE RESPONSE:\n",
        "{'-'*60}\n",
        "{response_text}\n",
        "{'-'*60}\n",
        "\"\"\")\n",
        "\n",
        "        return response_text\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Enhanced Claude API Error: {str(e)}\"\n",
        "        log_to_file(f\"\"\"ENHANCED CLAUDE API CALL FAILED - ATTEMPT {attempt_number}:\n",
        "Error: {error_msg}\n",
        "Using enhanced fallback response\n",
        "\"\"\")\n",
        "\n",
        "        # Enhanced fallback response\n",
        "        enhanced_fallback_response = f\"\"\"\n",
        "        I'll help you with this enhanced transformation task. Here's the precise code:\n",
        "\n",
        "        ```python\n",
        "        # Enhanced error calling Claude API: {str(e)}\n",
        "        # Enhanced fallback code with schema awareness\n",
        "        print(\"Enhanced Claude API error, using schema-aware fallback\")\n",
        "        print(f\"Current dataframe shape: {{df.shape}}\")\n",
        "        print(f\"Target shape should be: {target_df.shape}\")\n",
        "        print(f\"Target dtypes: {dict(target_df.dtypes)}\")\n",
        "        print(\"Please review transformation logic manually\")\n",
        "\n",
        "        # Basic schema validation\n",
        "        if df.shape != {target_df.shape}:\n",
        "            print(\"Warning: Shape mismatch detected\")\n",
        "\n",
        "        # Show current vs target schema\n",
        "        print(\"\\\\nCurrent dtypes:\", dict(df.dtypes))\n",
        "        print(\"Target dtypes:\", {dict(target_df.dtypes)})\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        log_to_file(f\"Enhanced fallback response generated:\\n{enhanced_fallback_response}\")\n",
        "        return enhanced_fallback_response\n",
        "\n",
        "\n",
        "def _extract_enhanced_code_blocks(response_text):\n",
        "    \"\"\"\n",
        "    Enhanced code block extraction with validation\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Enhanced pattern matching for code blocks\n",
        "    patterns = [\n",
        "        r'```python\\s*(.*?)```',\n",
        "        r'```\\s*python\\s*(.*?)```',\n",
        "        r'```\\s*(.*?)```',\n",
        "        r'`([^`]+)`'  # Single backticks as fallback\n",
        "    ]\n",
        "\n",
        "    code_blocks = []\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            cleaned_code = match.strip()\n",
        "            if cleaned_code and len(cleaned_code) > 10:  # Filter out very short matches\n",
        "                code_blocks.append(cleaned_code)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    unique_blocks = []\n",
        "    for block in code_blocks:\n",
        "        if block not in seen:\n",
        "            seen.add(block)\n",
        "            unique_blocks.append(block)\n",
        "\n",
        "    return unique_blocks\n",
        "\n",
        "\n",
        "# Enhanced Final Analysis Generation\n",
        "def _get_enhanced_final_analysis_with_prompts_logged(client, original_info, modified_info, user_description,\n",
        "                                                   total_changes, changed_rows, changed_columns, changed_cells,\n",
        "                                                   schema_comparison, data_type_mismatches, claude_prompt,\n",
        "                                                   replication_results, log_to_file):\n",
        "    \"\"\"\n",
        "    Enhanced final analysis with comprehensive learning and schema insights\n",
        "    \"\"\"\n",
        "    best_attempt = replication_results.get(\"best_attempt\", {})\n",
        "    final_claude_prompt = replication_results.get(\"final_prompt\", claude_prompt)\n",
        "    learning_applied = replication_results.get(\"learning_applied\", False)\n",
        "    learning_history = replication_results.get(\"learning_history\", [])\n",
        "\n",
        "    success_status = \"SUCCESS\" if replication_results[\"final_success\"] else \"PARTIAL\"\n",
        "\n",
        "    # Enhanced statistics\n",
        "    total_cells = original_info[\"shape\"][0] * original_info[\"shape\"][1] if original_info[\"shape\"][0] > 0 else 1\n",
        "    change_density = total_changes / total_cells\n",
        "\n",
        "    enhanced_analysis_prompt = f\"\"\"\n",
        "    Analyze this enhanced dataframe change with comprehensive schema and learning insights.\n",
        "\n",
        "    ORIGINAL DATAFRAME:\n",
        "    Shape: {original_info['shape']}\n",
        "    Schema: {original_info['dtype_details']}\n",
        "    Data Sample: {original_info['full_data'][:2000]}...\n",
        "\n",
        "    MODIFIED DATAFRAME:\n",
        "    Shape: {modified_info['shape']}\n",
        "    Schema: {modified_info['dtype_details']}\n",
        "    Data Sample: {modified_info['full_data'][:2000]}...\n",
        "\n",
        "    ENHANCED CHANGE ANALYSIS:\n",
        "    - Total changes: {total_changes} cells ({change_density*100:.1f}%)\n",
        "    - Data type mismatches: {len(data_type_mismatches)}\n",
        "    - Schema compatibility: {schema_comparison.get('schema_compatibility_score', 0):.1%}\n",
        "    - Critical schema issues: {len(schema_comparison.get('critical_issues', []))}\n",
        "    - User description: \"{user_description}\"\n",
        "\n",
        "    REPLICATION TESTING RESULTS:\n",
        "    - Status: {success_status}\n",
        "    - Attempts: {len(replication_results['attempts'])}\n",
        "    - Best match: {best_attempt.get('match_score', 0):.1%}\n",
        "    - Schema match: {best_attempt.get('schema_match', False)}\n",
        "    - Learning applied: {learning_applied}\n",
        "    - Learning insights: {len(learning_history)}\n",
        "\n",
        "    FINAL WORKING PROMPT: {final_claude_prompt}\n",
        "\n",
        "    Provide comprehensive analysis in this exact JSON format:\n",
        "    {{\n",
        "        \"change_summary\": \"ENHANCED ANALYSIS: [comprehensive summary including WORKING_CLAUDE_PROMPT: {final_claude_prompt}] and detailed technical findings with schema validation results\",\n",
        "        \"change_type\": \"data_edit|structure_change|mixed\",\n",
        "        \"structural_changes\": {{\n",
        "            \"rows_added\": {max(0, modified_info['shape'][0] - original_info['shape'][0])},\n",
        "            \"rows_removed\": {max(0, original_info['shape'][0] - modified_info['shape'][0])},\n",
        "            \"columns_added\": {list(set(modified_info['columns']) - set(original_info['columns']))},\n",
        "            \"columns_removed\": {list(set(original_info['columns']) - set(modified_info['columns']))}\n",
        "        }},\n",
        "        \"enhanced_data_modifications\": {{\n",
        "            \"cells_changed\": {total_changes},\n",
        "            \"total_cells\": {total_cells},\n",
        "            \"change_percentage\": {change_density*100:.2f},\n",
        "            \"data_type_mismatches\": {len(data_type_mismatches)},\n",
        "            \"rows_affected\": {len(changed_rows)},\n",
        "            \"columns_affected\": {list(changed_columns)},\n",
        "            \"schema_compatibility_score\": {schema_comparison.get('schema_compatibility_score', 0):.2f},\n",
        "            \"critical_schema_issues\": {len(schema_comparison.get('critical_issues', []))},\n",
        "            \"precision_patterns\": [\"Enhanced patterns identified\"],\n",
        "            \"data_quality_impact\": \"improved|degraded|neutral\"\n",
        "        }},\n",
        "        \"replication_analysis\": {{\n",
        "            \"final_success\": {replication_results['final_success']},\n",
        "            \"best_match_score\": {best_attempt.get('match_score', 0):.2f},\n",
        "            \"schema_validation_passed\": {best_attempt.get('schema_match', False)},\n",
        "            \"learning_system_applied\": {learning_applied},\n",
        "            \"total_learning_insights\": {len(learning_history)},\n",
        "            \"common_issues_resolved\": [\"Issues identified and resolved\"]\n",
        "        }},\n",
        "        \"business_impact\": {{\n",
        "            \"rent_calculations_affected\": \"Enhanced analysis of rent impact with precision focus\",\n",
        "            \"tenant_information_updated\": \"Enhanced analysis of tenant data changes\",\n",
        "            \"occupancy_status_changed\": \"Enhanced analysis of occupancy changes\",\n",
        "            \"data_integrity_maintained\": \"Assessment of data integrity after changes\"\n",
        "        }},\n",
        "        \"enhanced_recommendations\": [\n",
        "            \"Enhanced recommendations based on learning insights\",\n",
        "            \"Schema validation recommendations\",\n",
        "            \"Data type precision improvements\"\n",
        "        ],\n",
        "        \"session_description\": \"ENHANCED_REPLICATION_STATUS: {success_status} | FINAL_CLAUDE_PROMPT: {final_claude_prompt} | LEARNING_APPLIED: {learning_applied} | REPLICATION_ATTEMPTS: {len(replication_results['attempts'])} | USER_EDIT: {user_description} | Changes: {total_changes} cells ({change_density*100:.1f}%) | Best match: {best_attempt.get('match_score', 0):.1%} | Schema: {best_attempt.get('schema_match', False)}\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    log_to_file(f\"\"\"ENHANCED FINAL ANALYSIS GENERATION:\n",
        "Sending enhanced final analysis request to GPT-4.1...\n",
        "Features: Schema insights, learning history, precision focus\n",
        "\n",
        "ENHANCED PROMPT FOR FINAL ANALYSIS:\n",
        "{'-'*60}\n",
        "{enhanced_analysis_prompt}\n",
        "{'-'*60}\n",
        "\"\"\", \"ENHANCED FINAL ANALYSIS GENERATION\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an enhanced data analyst with schema validation expertise. Return only valid JSON with comprehensive insights.\"},\n",
        "                {\"role\": \"user\", \"content\": enhanced_analysis_prompt}\n",
        "            ],\n",
        "            max_tokens=4000,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        enhanced_gpt_final_response = response.choices[0].message.content\n",
        "        log_to_file(f\"\"\"ENHANCED FINAL ANALYSIS GPT-4.1 RESPONSE:\n",
        "{'-'*60}\n",
        "{enhanced_gpt_final_response}\n",
        "{'-'*60}\n",
        "\"\"\")\n",
        "\n",
        "        try:\n",
        "            import json\n",
        "            import re\n",
        "            json_match = re.search(r'{.*}', enhanced_gpt_final_response, re.DOTALL)\n",
        "            if json_match:\n",
        "                enhanced_parsed_analysis = json.loads(json_match.group(0))\n",
        "                log_to_file(\"✅ Successfully parsed JSON from enhanced final analysis response\")\n",
        "                return enhanced_parsed_analysis\n",
        "            else:\n",
        "                log_to_file(\"❌ No JSON found in enhanced final analysis response\")\n",
        "        except Exception as json_error:\n",
        "            log_to_file(f\"❌ Enhanced JSON parsing failed: {str(json_error)}\")\n",
        "\n",
        "    except Exception as api_error:\n",
        "        log_to_file(f\"❌ Enhanced final analysis API call failed: {str(api_error)}\")\n",
        "\n",
        "    # Enhanced fallback analysis\n",
        "    enhanced_fallback_analysis = {\n",
        "        \"change_summary\": f\"ENHANCED_WORKING_CLAUDE_PROMPT: {final_claude_prompt} | ENHANCED_GPT4_ANALYSIS: [embedded] | {user_description} - {total_changes} changes with {replication_results['final_success']} replication and {learning_applied} learning\",\n",
        "        \"change_type\": \"data_edit\",\n",
        "        \"enhanced_data_modifications\": {\n",
        "            \"cells_changed\": total_changes,\n",
        "            \"total_cells\": total_cells,\n",
        "            \"change_percentage\": change_density*100,\n",
        "            \"data_type_mismatches\": len(data_type_mismatches),\n",
        "            \"schema_compatibility_score\": schema_comparison.get('schema_compatibility_score', 0)\n",
        "        },\n",
        "        \"replication_analysis\": {\n",
        "            \"final_success\": replication_results['final_success'],\n",
        "            \"best_match_score\": best_attempt.get('match_score', 0),\n",
        "            \"learning_system_applied\": learning_applied,\n",
        "            \"total_learning_insights\": len(learning_history)\n",
        "        },\n",
        "        \"session_description\": f\"ENHANCED_REPLICATION_STATUS: {success_status} | FINAL_CLAUDE_PROMPT: {final_claude_prompt} | LEARNING_APPLIED: {learning_applied} | USER_EDIT: {user_description} | Changes: {total_changes} cells\"\n",
        "    }\n",
        "\n",
        "    log_to_file(f\"Using enhanced fallback analysis:\\n{json.dumps(enhanced_fallback_analysis, indent=2)}\")\n",
        "    return enhanced_fallback_analysis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Enhanced Log Summary with All Fixes\n",
        "def get_enhanced_manual_edit_logs_summary_with_all_fixes():\n",
        "    \"\"\"\n",
        "    Generate enhanced summary of all manual edit analysis log files with comprehensive insights\n",
        "    \"\"\"\n",
        "    logs_dir = \"manual_edit_analysis_logs\"\n",
        "\n",
        "    if not os.path.exists(logs_dir):\n",
        "        return \"No enhanced manual edit analysis logs found yet.\"\n",
        "\n",
        "    try:\n",
        "        log_files = [f for f in os.listdir(logs_dir) if f.endswith('_detailed_analysis.txt')]\n",
        "\n",
        "        if not log_files:\n",
        "            return \"No enhanced detailed analysis log files found.\"\n",
        "\n",
        "        # Sort by creation time (newest first)\n",
        "        log_files.sort(key=lambda x: os.path.getctime(os.path.join(logs_dir, x)), reverse=True)\n",
        "\n",
        "        enhanced_summary = f\"\"\"📁 ULTIMATE ENHANCED Manual Edit Analysis Logs Summary\n",
        "Found {len(log_files)} comprehensive analysis log files with ALL FIXES APPLIED:\n",
        "\n",
        "🔧 CRITICAL FIXES INCLUDED IN LOGS:\n",
        "✅ 1. Fixed logging string formatting bug\n",
        "✅ 2. Enhanced data type/formatting precision matching\n",
        "✅ 3. Implemented attempt-to-attempt learning system\n",
        "✅ 4. Added comprehensive data type validation\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        total_size = 0\n",
        "        enhanced_features_count = 0\n",
        "        learning_applied_count = 0\n",
        "\n",
        "        for i, log_file in enumerate(log_files[:15], 1):  # Show last 15\n",
        "            file_path = os.path.join(logs_dir, log_file)\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            total_size += file_size\n",
        "            created_time = datetime.fromtimestamp(os.path.getctime(file_path))\n",
        "\n",
        "            # Try to extract enhanced information from log file\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    log_content = f.read()\n",
        "\n",
        "                # Check for enhanced features\n",
        "                has_enhanced_features = \"ENHANCED FEATURES APPLIED\" in log_content\n",
        "                has_learning = \"LEARNING SYSTEM\" in log_content\n",
        "                has_schema_validation = \"SCHEMA VALIDATION\" in log_content\n",
        "\n",
        "                if has_enhanced_features:\n",
        "                    enhanced_features_count += 1\n",
        "                if has_learning:\n",
        "                    learning_applied_count += 1\n",
        "\n",
        "            except Exception:\n",
        "                has_enhanced_features = False\n",
        "                has_learning = False\n",
        "                has_schema_validation = False\n",
        "\n",
        "            # Extract session info from filename\n",
        "            session_id = log_file.replace('_detailed_analysis.txt', '')\n",
        "\n",
        "            enhanced_summary += f\"\"\"{i}. {session_id}\n",
        "   📄 File: {log_file}\n",
        "   📅 Created: {created_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "   💾 Size: {file_size:,} bytes\n",
        "   🔧 Enhanced Features: {'✅ Yes' if has_enhanced_features else '❌ No'}\n",
        "   🧠 Learning Applied: {'✅ Yes' if has_learning else '❌ No'}\n",
        "   📋 Schema Validation: {'✅ Yes' if has_schema_validation else '❌ No'}\n",
        "   📁 Path: {file_path}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        if len(log_files) > 15:\n",
        "            enhanced_summary += f\"... and {len(log_files) - 15} more enhanced log files\\n\"\n",
        "\n",
        "        enhanced_summary += f\"\"\"\n",
        "📈 ENHANCED LOGS STATISTICS:\n",
        "• Total Log Files: {len(log_files)}\n",
        "• Total Storage Used: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\n",
        "• Files with Enhanced Features: {enhanced_features_count}/{len(log_files)} ({enhanced_features_count/len(log_files)*100:.1f}%)\n",
        "• Files with Learning System: {learning_applied_count}/{len(log_files)} ({learning_applied_count/len(log_files)*100:.1f}%)\n",
        "\n",
        "📋 ULTIMATE ENHANCED LOG CONTENTS INCLUDE:\n",
        "• Complete GPT-4.1 prompts and responses with schema awareness\n",
        "• All Claude API calls with learning system feedback loops\n",
        "• Step-by-step replication testing with precision validation\n",
        "• Cell-by-cell dataframe comparison with data type analysis\n",
        "• Schema compatibility analysis and validation results\n",
        "• Business impact assessment with enhanced insights\n",
        "• Learning system evolution and improvement tracking\n",
        "• Data type mismatch detection and resolution strategies\n",
        "• Comprehensive error handling and debugging information\n",
        "• Attempt-to-attempt learning and prompt refinement details\n",
        "\n",
        "🔧 CRITICAL FIXES VERIFICATION:\n",
        "1. ✅ Logging String Formatting: All logs use proper string formatting\n",
        "2. ✅ Data Type Precision: Enhanced schema validation throughout\n",
        "3. ✅ Learning System: Progressive improvement across attempts\n",
        "4. ✅ Type Validation: Comprehensive data type compatibility checking\n",
        "\n",
        "📂 All enhanced logs saved in: {logs_dir}/\n",
        "\"\"\"\n",
        "\n",
        "        return enhanced_summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error reading enhanced log files: {str(e)}\"\n",
        "\n",
        "\n",
        "# Enhanced Log Reader\n",
        "def read_enhanced_manual_edit_log(session_id):\n",
        "    \"\"\"\n",
        "    Read and return enhanced log with analysis insights\n",
        "    \"\"\"\n",
        "    logs_dir = \"manual_edit_analysis_logs\"\n",
        "    log_file_path = os.path.join(logs_dir, f\"{session_id}_detailed_analysis.txt\")\n",
        "\n",
        "    if not os.path.exists(log_file_path):\n",
        "        return f\"Enhanced log file not found: {log_file_path}\"\n",
        "\n",
        "    try:\n",
        "        with open(log_file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Analyze log content for enhanced insights\n",
        "        enhanced_features_detected = \"ENHANCED FEATURES APPLIED\" in content\n",
        "        learning_system_detected = \"LEARNING SYSTEM\" in content\n",
        "        schema_validation_detected = \"SCHEMA VALIDATION\" in content\n",
        "        replication_success = \"REPLICATION SUCCESSFUL\" in content\n",
        "\n",
        "        file_size = len(content)\n",
        "        line_count = content.count('\\n')\n",
        "\n",
        "        return f\"\"\"📄 ULTIMATE ENHANCED Manual Edit Analysis Log: {session_id}\n",
        "{'='*80}\n",
        "\n",
        "🔧 CRITICAL FIXES STATUS:\n",
        "✅ Enhanced Features Applied: {enhanced_features_detected}\n",
        "✅ Learning System Active: {learning_system_detected}\n",
        "✅ Schema Validation Included: {schema_validation_detected}\n",
        "✅ Replication Successful: {replication_success}\n",
        "\n",
        "📊 LOG STATISTICS:\n",
        "• File Size: {file_size:,} characters ({file_size/1024:.1f} KB)\n",
        "• Total Lines: {line_count:,}\n",
        "• Enhanced Analysis: {'✅ Complete' if enhanced_features_detected else '❌ Basic'}\n",
        "\n",
        "{'='*80}\n",
        "\n",
        "{content}\n",
        "\n",
        "{'='*80}\n",
        "End of ultimate enhanced log file: {log_file_path}\n",
        "\n",
        "🔍 ANALYSIS SUMMARY:\n",
        "This log contains {'comprehensive enhanced analysis' if enhanced_features_detected else 'basic analysis'} with:\n",
        "• {'✅' if 'GPT-4.1' in content else '❌'} GPT-4.1 API interactions\n",
        "• {'✅' if 'CLAUDE API' in content else '❌'} Claude API calls and responses\n",
        "• {'✅' if learning_system_detected else '❌'} Learning system feedback loops\n",
        "• {'✅' if schema_validation_detected else '❌'} Schema compatibility validation\n",
        "• {'✅' if 'DATA TYPE' in content else '❌'} Data type precision analysis\n",
        "• {'✅' if replication_success else '❌'} Successful replication testing\n",
        "\"\"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error reading enhanced log file: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5On1I-DuJU9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ahMIzhPwO5y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hPSLoUqgFXm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Update the Gradio event handlers to use enhanced functions\n",
        "def setup_enhanced_edit_data_handlers():\n",
        "    \"\"\"\n",
        "    Setup function to update Gradio event handlers for enhanced edit data functionality.\n",
        "    Add this to your Gradio interface setup.\n",
        "    \"\"\"\n",
        "\n",
        "    # Enhanced event handlers for Edit Data tab\n",
        "    refresh_versions_btn.click(\n",
        "        refresh_version_dropdown,\n",
        "        outputs=[version_dropdown]\n",
        "    )\n",
        "\n",
        "    load_version_btn.click(\n",
        "        load_specific_version,  # Use enhanced version\n",
        "        inputs=[version_dropdown],\n",
        "        outputs=[editable_df, edit_status]\n",
        "    )\n",
        "\n",
        "    save_changes_btn.click(\n",
        "        save_edited_dataframe,  # Use enhanced version\n",
        "        inputs=[editable_df, save_description],\n",
        "        outputs=[save_status, editable_df]\n",
        "    ).then(\n",
        "        refresh_version_dropdown,  # Refresh the dropdown after saving\n",
        "        outputs=[version_dropdown]\n",
        "    )\n",
        "\n",
        "    # You can also add a \"Load Latest\" button that uses the enhanced function\n",
        "    # load_latest_btn.click(\n",
        "    #     load_latest_version_for_editing_enhanced,\n",
        "    #     outputs=[editable_df, edit_status]\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJbOfSqqULbd"
      },
      "outputs": [],
      "source": [
        "class EnhancedTemplateApplicationEngine:\n",
        "    def __init__(self):\n",
        "        self.templates_dir = \"rent_roll_templates\"\n",
        "        self.application_sessions_dir = \"template_applications\"\n",
        "        os.makedirs(self.application_sessions_dir, exist_ok=True)\n",
        "        self.current_application = None\n",
        "\n",
        "    def start_template_application(self, template_id, new_rent_roll_file, new_rent_roll_df):\n",
        "        \"\"\"Initialize a new template application session\"\"\"\n",
        "\n",
        "        # Load the template data\n",
        "        template_data, starting_template_df, final_template_df = enhanced_template_manager.load_template_dataframes(template_id)\n",
        "\n",
        "        if template_data is None:\n",
        "            return None, \"❌ Failed to load template data\"\n",
        "\n",
        "        # Create application session\n",
        "        app_session_id = f\"app_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        self.current_application = {\n",
        "            \"session_id\": app_session_id,\n",
        "            \"template_id\": template_id,\n",
        "            \"template_data\": template_data,\n",
        "            \"starting_template_df\": starting_template_df,\n",
        "            \"final_template_df\": final_template_df,\n",
        "            \"new_rent_roll_file\": new_rent_roll_file,\n",
        "            \"new_rent_roll_df\": new_rent_roll_df.copy(),\n",
        "            \"current_df\": new_rent_roll_df.copy(),\n",
        "            \"step_results\": [],\n",
        "            \"current_step\": 0,\n",
        "            \"total_steps\": len(template_data.get(\"raw_workflow_steps\", [])),\n",
        "            \"completed_steps\": [],\n",
        "            \"failed_steps\": [],\n",
        "            \"log_file\": None,\n",
        "            \"latest_version_file\": None,\n",
        "            \"step_version_files\": {},\n",
        "            \"step_states\": {},  # 🔄 NEW: Store complete state for each step\n",
        "            \"user_inputs_history\": {}  # 📝 NEW: Store user inputs for each step\n",
        "        }\n",
        "\n",
        "        # Create dataframes directory early\n",
        "        dataframes_dir = os.path.join(self.application_sessions_dir, f\"{app_session_id}_dataframes\")\n",
        "        os.makedirs(dataframes_dir, exist_ok=True)\n",
        "\n",
        "        # Create log file\n",
        "        log_filename = f\"{app_session_id}_application_log.txt\"\n",
        "        self.current_application[\"log_file\"] = os.path.join(self.application_sessions_dir, log_filename)\n",
        "\n",
        "        # Save initial version file (Step 0)\n",
        "        initial_version_file = self._save_dataframe_version(\n",
        "            new_rent_roll_df,\n",
        "            \"Initial uploaded rent roll - starting point\",\n",
        "            0\n",
        "        )\n",
        "        self.current_application[\"latest_version_file\"] = initial_version_file\n",
        "        self.current_application[\"step_version_files\"][0] = initial_version_file\n",
        "\n",
        "        # Save initial state\n",
        "        self._save_step_state(0, {\n",
        "            \"dataframe\": new_rent_roll_df.copy(),\n",
        "            \"version_file\": initial_version_file,\n",
        "            \"status\": \"initial\",\n",
        "            \"description\": \"Starting point\"\n",
        "        })\n",
        "\n",
        "        # Write initial log\n",
        "        with open(self.current_application[\"log_file\"], 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"=== ENHANCED TEMPLATE APPLICATION SESSION ===\\n\")\n",
        "            f.write(f\"Session ID: {app_session_id}\\n\")\n",
        "            f.write(f\"Template ID: {template_id}\\n\")\n",
        "            f.write(f\"Template Name: {template_data.get('template_name', 'Unknown')}\\n\")\n",
        "            f.write(f\"New Rent Roll File: {new_rent_roll_file}\\n\")\n",
        "            f.write(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Total Steps to Execute: {self.current_application['total_steps']}\\n\")\n",
        "            f.write(f\"💾 Initial Version File: {initial_version_file}\\n\")\n",
        "            f.write(f\"🆕 Enhanced Features: User input per step, Back navigation\\n\")\n",
        "            f.write(f\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "        return app_session_id, \"✅ Enhanced template application session started successfully\"\n",
        "\n",
        "    def _save_step_state(self, step_number, state_data):\n",
        "        \"\"\"🔄 NEW: Save complete state for a step to enable back navigation (FIXED VERSION)\"\"\"\n",
        "        try:\n",
        "            # ✅ FIX: Ensure we have a dataframe\n",
        "            if \"dataframe\" not in state_data:\n",
        "                print(f\"⚠️ Warning: No dataframe in state_data for step {step_number}\")\n",
        "                # Try to use current_df as fallback\n",
        "                if \"current_df\" in self.current_application:\n",
        "                    state_data[\"dataframe\"] = self.current_application[\"current_df\"].copy()\n",
        "                else:\n",
        "                    print(f\"❌ No current_df available for step {step_number}\")\n",
        "                    return\n",
        "\n",
        "            # Save the state\n",
        "            self.current_application[\"step_states\"][step_number] = {\n",
        "                \"dataframe\": state_data[\"dataframe\"].copy(),\n",
        "                \"version_file\": state_data.get(\"version_file\", \"\"),\n",
        "                \"status\": state_data.get(\"status\", \"unknown\"),\n",
        "                \"description\": state_data.get(\"description\", \"\"),\n",
        "                \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                \"shape\": state_data[\"dataframe\"].shape\n",
        "            }\n",
        "\n",
        "            print(f\"💾 Saved state for step {step_number}: {state_data['dataframe'].shape}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving step state {step_number}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    def _restore_step_state(self, step_number):\n",
        "        \"\"\"🔄 NEW: Restore state from a specific step (FIXED VERSION)\"\"\"\n",
        "        if step_number not in self.current_application[\"step_states\"]:\n",
        "            return False, f\"No saved state for step {step_number}\"\n",
        "\n",
        "        try:\n",
        "            state = self.current_application[\"step_states\"][step_number]\n",
        "\n",
        "            # Debug logging\n",
        "            print(f\"🔍 Restoring step {step_number}\")\n",
        "            print(f\"Available keys in state: {list(state.keys())}\")\n",
        "\n",
        "            # ✅ FIX: Check if dataframe exists in state\n",
        "            if \"dataframe\" not in state:\n",
        "                # Try to load from version file instead\n",
        "                version_file = state.get(\"version_file\")\n",
        "                if version_file and os.path.exists(version_file):\n",
        "                    print(f\"📁 Loading dataframe from version file: {version_file}\")\n",
        "                    restored_df = pd.read_csv(version_file)\n",
        "                else:\n",
        "                    print(f\"❌ No dataframe in state and no valid version file\")\n",
        "                    return False, f\"Cannot restore step {step_number}: no dataframe available\"\n",
        "            else:\n",
        "                # Use dataframe from state\n",
        "                restored_df = state[\"dataframe\"].copy()\n",
        "                print(f\"✅ Loaded dataframe from state: {restored_df.shape}\")\n",
        "\n",
        "            # Restore the dataframe and version file\n",
        "            self.current_application[\"current_df\"] = restored_df\n",
        "            self.current_application[\"latest_version_file\"] = state.get(\"version_file\")\n",
        "\n",
        "            # Update current step\n",
        "            self.current_application[\"current_step\"] = step_number\n",
        "\n",
        "            # Clear any completed/failed steps after this point\n",
        "            self.current_application[\"completed_steps\"] = [s for s in self.current_application[\"completed_steps\"] if s <= step_number]\n",
        "            self.current_application[\"failed_steps\"] = [s for s in self.current_application[\"failed_steps\"] if s <= step_number]\n",
        "\n",
        "            # Clear future states\n",
        "            future_steps = [s for s in self.current_application[\"step_states\"].keys() if s > step_number]\n",
        "            for future_step in future_steps:\n",
        "                del self.current_application[\"step_states\"][future_step]\n",
        "\n",
        "            # Clear future user inputs\n",
        "            future_inputs = [s for s in self.current_application[\"user_inputs_history\"].keys() if s > step_number]\n",
        "            for future_step in future_inputs:\n",
        "                del self.current_application[\"user_inputs_history\"][future_step]\n",
        "\n",
        "            # Log the restoration\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"🔄 RESTORED TO STEP {step_number}\\n\")\n",
        "                f.write(f\"Restored DataFrame Shape: {restored_df.shape}\\n\")\n",
        "                f.write(f\"Restored Version File: {os.path.basename(state.get('version_file', 'None'))}\\n\")\n",
        "                f.write(f\"Restored At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "            return True, f\"✅ Successfully restored to step {step_number} (Shape: {restored_df.shape})\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Exception in _restore_step_state: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False, f\"❌ Error restoring step {step_number}: {str(e)}\"\n",
        "\n",
        "    def get_available_back_steps(self):\n",
        "        \"\"\"🔄 NEW: Get list of steps available for back navigation\"\"\"\n",
        "        if not self.current_application:\n",
        "            return []\n",
        "\n",
        "        current_step = self.current_application[\"current_step\"]\n",
        "        available_steps = []\n",
        "\n",
        "        for step_num in sorted(self.current_application[\"step_states\"].keys()):\n",
        "            if step_num < current_step:\n",
        "                state = self.current_application[\"step_states\"][step_num]\n",
        "                available_steps.append({\n",
        "                    \"step_number\": step_num,\n",
        "                    \"description\": state[\"description\"],\n",
        "                    \"timestamp\": state[\"timestamp\"],\n",
        "                    \"shape\": state[\"shape\"]\n",
        "                })\n",
        "\n",
        "        return available_steps\n",
        "\n",
        "    def go_back_to_step(self, target_step):\n",
        "        \"\"\"🔄 NEW: Navigate back to a specific step\"\"\"\n",
        "        if not self.current_application:\n",
        "            return \"❌ No active application session\"\n",
        "\n",
        "        current_step = self.current_application[\"current_step\"]\n",
        "\n",
        "        if target_step >= current_step:\n",
        "            return f\"❌ Cannot go back to step {target_step} (current step is {current_step})\"\n",
        "\n",
        "        if target_step not in self.current_application[\"step_states\"]:\n",
        "            return f\"❌ No saved state for step {target_step}\"\n",
        "\n",
        "        success, message = self._restore_step_state(target_step)\n",
        "\n",
        "        if success:\n",
        "            return f\"🔄 Successfully went back to step {target_step}. You can now continue from here.\"\n",
        "        else:\n",
        "            return message\n",
        "\n",
        "    def get_next_step_info(self):\n",
        "        \"\"\"Get detailed information about the next step to execute\"\"\"\n",
        "        if not self.current_application:\n",
        "            return None, \"No active application session\"\n",
        "\n",
        "        current_step = self.current_application[\"current_step\"]\n",
        "        workflow_steps = self.current_application[\"template_data\"].get(\"raw_workflow_steps\", [])\n",
        "\n",
        "        if current_step >= len(workflow_steps):\n",
        "            return None, \"All steps completed\"\n",
        "\n",
        "        next_step = workflow_steps[current_step]\n",
        "        step_number = current_step + 1\n",
        "\n",
        "        # Get current dataframe info\n",
        "        current_df = self._get_latest_dataframe_from_versions()\n",
        "\n",
        "        step_info = {\n",
        "            \"step_number\": step_number,\n",
        "            \"total_steps\": len(workflow_steps),\n",
        "            \"original_message\": next_step.get('user_message', 'No message'),\n",
        "            \"original_code\": next_step.get('code_executed', ''),\n",
        "            \"ai_response\": next_step.get('ai_response', ''),\n",
        "            \"current_shape\": current_df.shape,\n",
        "            \"current_columns\": list(current_df.columns),\n",
        "            \"can_go_back\": len(self.get_available_back_steps()) > 0\n",
        "        }\n",
        "\n",
        "        return step_info, f\"Step {step_number} of {len(workflow_steps)}\"\n",
        "\n",
        "    def execute_step_with_user_input(self, user_additional_input=\"\", override_code=\"\"):\n",
        "        \"\"\"📝 NEW: Execute next step with optional user input and code override\"\"\"\n",
        "        if not self.current_application:\n",
        "            return {\"success\": False, \"error\": \"No active application session\"}\n",
        "\n",
        "        # Get next step info\n",
        "        step_info, step_description = self.get_next_step_info()\n",
        "        if step_info is None:\n",
        "            return {\"success\": False, \"error\": \"All steps completed\"}\n",
        "\n",
        "        current_step_num = self.current_application[\"current_step\"] + 1\n",
        "\n",
        "        # Store user input for this step\n",
        "        self.current_application[\"user_inputs_history\"][current_step_num] = {\n",
        "            \"additional_input\": user_additional_input,\n",
        "            \"override_code\": override_code,\n",
        "            \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            print(f\"\\n🚀 EXECUTING STEP {current_step_num} WITH USER INPUT\")\n",
        "            print(f\"📝 Original step: {step_info['original_message'][:100]}...\")\n",
        "            if user_additional_input:\n",
        "                print(f\"👤 User input: {user_additional_input[:100]}...\")\n",
        "            if override_code:\n",
        "                print(f\"🔧 Code override provided\")\n",
        "\n",
        "            # Get current dataframe from latest version file\n",
        "            current_df = self._get_latest_dataframe_from_versions()\n",
        "            self.current_application[\"current_df\"] = current_df.copy()\n",
        "\n",
        "            # Execute step with user input\n",
        "            execution_result = self._execute_step_with_claude_and_user_input(\n",
        "                step_info, current_step_num, user_additional_input, override_code\n",
        "            )\n",
        "\n",
        "            if execution_result.get(\"success\", False):\n",
        "                # ✅ SUCCESS\n",
        "                print(f\"✅ Step {current_step_num} succeeded!\")\n",
        "\n",
        "                # Update latest version file pointer\n",
        "                new_version_file = execution_result.get(\"new_version_file\")\n",
        "                if new_version_file:\n",
        "                    self._update_latest_version_file(new_version_file, current_step_num)\n",
        "\n",
        "                # Save state for this completed step\n",
        "                final_df = execution_result.get(\"updated_df\", current_df)\n",
        "                self._save_step_state(current_step_num, {\n",
        "                    \"dataframe\": final_df,\n",
        "                    \"version_file\": new_version_file,\n",
        "                    \"status\": \"completed\",\n",
        "                    \"description\": f\"Step {current_step_num} completed: {step_info['original_message'][:50]}...\"\n",
        "                })\n",
        "\n",
        "                # Advance to next step\n",
        "                self.current_application[\"current_step\"] += 1\n",
        "                self.current_application[\"completed_steps\"].append(current_step_num)\n",
        "\n",
        "                # Save application state\n",
        "                self._save_application_state()\n",
        "\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"step_number\": current_step_num,\n",
        "                    \"message\": f\"✅ Step {current_step_num} completed successfully\",\n",
        "                    \"execution_result\": execution_result,\n",
        "                    \"can_go_back\": True,\n",
        "                    \"next_step_available\": self.current_application[\"current_step\"] < self.current_application[\"total_steps\"]\n",
        "                }\n",
        "\n",
        "            else:\n",
        "                # ❌ FAILURE\n",
        "                error_msg = execution_result.get(\"error\", \"Unknown error\")\n",
        "                print(f\"❌ Step {current_step_num} failed: {error_msg}\")\n",
        "\n",
        "                # Save failed state (don't advance step)\n",
        "                self._save_step_state(current_step_num, {\n",
        "                    \"dataframe\": current_df,\n",
        "                    \"version_file\": self.current_application[\"latest_version_file\"],\n",
        "                    \"status\": \"failed\",\n",
        "                    \"description\": f\"Step {current_step_num} failed: {error_msg[:50]}...\"\n",
        "                })\n",
        "\n",
        "                return {\n",
        "                    \"success\": False,\n",
        "                    \"step_number\": current_step_num,\n",
        "                    \"error\": error_msg,\n",
        "                    \"execution_result\": execution_result,\n",
        "                    \"can_retry\": True,\n",
        "                    \"can_go_back\": len(self.get_available_back_steps()) > 0\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Execution crashed: {str(e)}\"\n",
        "            print(f\"💥 Step {current_step_num} crashed: {error_msg}\")\n",
        "\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"step_number\": current_step_num,\n",
        "                \"error\": error_msg,\n",
        "                \"can_retry\": True,\n",
        "                \"can_go_back\": len(self.get_available_back_steps()) > 0\n",
        "            }\n",
        "\n",
        "    def _execute_step_with_claude_and_user_input(self, step_info, step_number, user_input, code_override):\n",
        "        \"\"\"📝 NEW: Execute step with Claude, incorporating user input\"\"\"\n",
        "        from anthropic import Anthropic\n",
        "        claude_client = Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "        # Load latest dataframe\n",
        "        latest_df = self._get_latest_dataframe_from_versions()\n",
        "\n",
        "        print(f\"🔗 STEP {step_number} LOADING FROM VERSION FILE: {os.path.basename(self.current_application.get('latest_version_file', 'None'))}\")\n",
        "        print(f\"📊 Loaded DataFrame Shape: {latest_df.shape}\")\n",
        "\n",
        "        # Prepare enhanced Claude prompt with user input\n",
        "        df_context = f\"\"\"\n",
        "CURRENT DATAFRAME STATUS:\n",
        "========================\n",
        "Shape: {latest_df.shape}\n",
        "Columns: {list(latest_df.columns)}\n",
        "Data Types: {dict(latest_df.dtypes.astype(str))}\n",
        "\n",
        "Sample Data (first 3 rows):\n",
        "{latest_df.head(3).to_string()}\n",
        "\n",
        "The dataframe is already loaded as 'df' variable.\n",
        "\"\"\"\n",
        "\n",
        "        # Build the main prompt\n",
        "        main_prompt = f\"\"\"\n",
        "STEP {step_number} EXECUTION WITH USER INPUT\n",
        "==========================================\n",
        "\n",
        "ORIGINAL TEMPLATE STEP:\n",
        "User Intent: \"{step_info['original_message']}\"\n",
        "\n",
        "ORIGINAL AI RESPONSE CONTEXT:\n",
        "{step_info['ai_response'][:500] + \"...\" if len(step_info['ai_response']) > 500 else step_info['ai_response']}\n",
        "\n",
        "ORIGINAL CODE FROM TEMPLATE:\n",
        "```python\n",
        "{step_info['original_code']}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "        # Add user input section if provided\n",
        "        user_input_section = \"\"\n",
        "        if user_input.strip():\n",
        "            user_input_section = f\"\"\"\n",
        "🧑 USER ADDITIONAL INPUT FOR THIS STEP:\n",
        "======================================\n",
        "{user_input}\n",
        "\n",
        "IMPORTANT: Incorporate the user's input into your solution. The user may be:\n",
        "- Requesting modifications to the original approach\n",
        "- Providing specific requirements or constraints\n",
        "- Asking for different analysis or calculations\n",
        "- Suggesting improvements or alternatives\n",
        "\"\"\"\n",
        "\n",
        "        # Add code override section if provided\n",
        "        code_override_section = \"\"\n",
        "        if code_override.strip():\n",
        "            code_override_section = f\"\"\"\n",
        "🔧 USER CODE OVERRIDE:\n",
        "=====================\n",
        "The user has provided custom code to use instead of or in addition to the original:\n",
        "\n",
        "```python\n",
        "{code_override}\n",
        "```\n",
        "\n",
        "INSTRUCTIONS: Use this code as the primary approach, but ensure it still achieves the original step's business objective.\n",
        "\"\"\"\n",
        "\n",
        "        # Final instructions\n",
        "        final_instructions = f\"\"\"\n",
        "{df_context}\n",
        "\n",
        "EXECUTION INSTRUCTIONS:\n",
        "1. The dataframe 'df' is already loaded and available\n",
        "2. Consider the original template step's business objective\n",
        "3. {\"Incorporate the user's additional input/requirements\" if user_input else \"\"}\n",
        "4. {\"Use the user's code override as the primary approach\" if code_override else \"Either use original code or adapt it as needed\"}\n",
        "5. 🔗 CRITICAL: You MUST call save_dataframe_version(df, \"description\") at the end\n",
        "6. Provide clear output about what was accomplished\n",
        "\n",
        "Please provide your code in ```python``` blocks.\n",
        "\"\"\"\n",
        "\n",
        "        full_prompt = main_prompt + user_input_section + code_override_section + final_instructions\n",
        "\n",
        "        # Log the enhanced prompt\n",
        "        with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"📤 SENDING TO CLAUDE 3.7 - STEP {step_number} WITH USER INPUT\\n\")\n",
        "            f.write(f\"Input DataFrame Shape: {latest_df.shape}\\n\")\n",
        "            f.write(f\"User Input Length: {len(user_input)} characters\\n\")\n",
        "            f.write(f\"Code Override Length: {len(code_override)} characters\\n\")\n",
        "            f.write(f\"Full Prompt Length: {len(full_prompt)} characters\\n\\n\")\n",
        "\n",
        "        try:\n",
        "            # Execute with Claude\n",
        "            claude_response = claude_client.messages.create(\n",
        "                model=\"claude-3-7-sonnet-20250219\",\n",
        "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "                max_tokens=4000,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            response_text = claude_response.content[0].text\n",
        "\n",
        "            # Log Claude's response\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"📥 RECEIVED FROM CLAUDE 3.7 - STEP {step_number}\\n\")\n",
        "                f.write(f\"Response Length: {len(response_text)} characters\\n\\n\")\n",
        "\n",
        "            # Extract and execute code blocks\n",
        "            import re\n",
        "            code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "\n",
        "            if not code_blocks:\n",
        "                return {\n",
        "                    \"success\": False,\n",
        "                    \"summary\": f\"Step {step_number}: No executable code generated by Claude\",\n",
        "                    \"claude_response\": response_text,\n",
        "                    \"error\": \"No code blocks found in Claude's response\"\n",
        "                }\n",
        "\n",
        "            # Execute the code with enhanced version saving\n",
        "            version_file_created = None\n",
        "\n",
        "            def enhanced_save_dataframe_version(df, operation_description=\"\"):\n",
        "                nonlocal version_file_created\n",
        "                version_file_created = self._save_dataframe_version(df, operation_description, step_number)\n",
        "                return version_file_created\n",
        "\n",
        "            exec_globals = {\n",
        "                \"df\": latest_df.copy(),\n",
        "                \"pd\": pd,\n",
        "                \"np\": np,\n",
        "                \"datetime\": datetime,\n",
        "                \"os\": os,\n",
        "                \"print_formatted_table\": lambda df, title=\"DataFrame\", max_rows=10: self.print_formatted_table(df, title, max_rows),\n",
        "                \"save_dataframe_version\": enhanced_save_dataframe_version,\n",
        "                \"print\": print,\n",
        "                \"len\": len,\n",
        "                \"str\": str,\n",
        "                \"int\": int,\n",
        "                \"float\": float\n",
        "            }\n",
        "\n",
        "            execution_success = False\n",
        "            execution_output = \"\"\n",
        "\n",
        "            import io\n",
        "            from contextlib import redirect_stdout\n",
        "            output_buffer = io.StringIO()\n",
        "\n",
        "            try:\n",
        "                with redirect_stdout(output_buffer):\n",
        "                    for i, code_block in enumerate(code_blocks, 1):\n",
        "                        print(f\"--- Executing Code Block {i} ---\")\n",
        "                        exec(code_block, exec_globals)\n",
        "                        print(f\"--- Code Block {i} Completed ---\\n\")\n",
        "\n",
        "                execution_output = output_buffer.getvalue()\n",
        "                updated_df = exec_globals[\"df\"]\n",
        "                execution_success = True\n",
        "\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                execution_output = f\"Execution error: {str(e)}\\n{traceback.format_exc()}\"\n",
        "                execution_success = False\n",
        "                updated_df = latest_df\n",
        "\n",
        "            # Create version file if none was created during execution\n",
        "            if execution_success and not version_file_created:\n",
        "                print(\"⚠️ No version file created during execution, creating one now...\")\n",
        "                version_file_created = self._save_dataframe_version(\n",
        "                    updated_df,\n",
        "                    f\"Step {step_number} completed with user input\",\n",
        "                    step_number\n",
        "                )\n",
        "\n",
        "            # Update memory reference\n",
        "            self.current_application[\"current_df\"] = updated_df.copy()\n",
        "\n",
        "            # Log results\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                result_status = \"✅ SUCCESS\" if execution_success else \"❌ FAILED\"\n",
        "                f.write(f\"{result_status} - STEP {step_number} WITH USER INPUT\\n\")\n",
        "                f.write(f\"Input Shape: {latest_df.shape}\\n\")\n",
        "                f.write(f\"Output Shape: {updated_df.shape}\\n\")\n",
        "                f.write(f\"User Input Applied: {bool(user_input)}\\n\")\n",
        "                f.write(f\"Code Override Used: {bool(code_override)}\\n\")\n",
        "                if version_file_created:\n",
        "                    f.write(f\"🔗 Version File Created: {os.path.basename(version_file_created)}\\n\")\n",
        "                f.write(f\"Execution Output:\\n{execution_output}\\n\\n\")\n",
        "\n",
        "            return {\n",
        "                \"success\": execution_success,\n",
        "                \"summary\": f\"Step {step_number}: {'✅ Successfully executed with user input' if execution_success else '❌ Execution failed'}\",\n",
        "                \"claude_response\": response_text,\n",
        "                \"executed_code\": \"\\n\\n# --- Next Code Block ---\\n\\n\".join(code_blocks),\n",
        "                \"execution_output\": execution_output,\n",
        "                \"updated_df\": updated_df,\n",
        "                \"new_version_file\": version_file_created,\n",
        "                \"user_input_applied\": bool(user_input),\n",
        "                \"code_override_used\": bool(code_override)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"❌ CLAUDE API FAILED - STEP {step_number}\\n\")\n",
        "                f.write(f\"Error: {str(e)}\\n\\n\")\n",
        "\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"summary\": f\"Step {step_number}: Claude execution failed - {str(e)}\",\n",
        "                \"error\": str(e),\n",
        "                \"claude_response\": \"Failed to get response from Claude\"\n",
        "            }\n",
        "\n",
        "    # Include all the existing helper methods from the original class\n",
        "    def _get_latest_dataframe_from_versions(self):\n",
        "        \"\"\"Load dataframe from the latest version file in the chain\"\"\"\n",
        "        try:\n",
        "            if not self.current_application:\n",
        "                raise Exception(\"No active application session\")\n",
        "\n",
        "            latest_version_file = self.current_application.get(\"latest_version_file\")\n",
        "\n",
        "            if not latest_version_file or not os.path.exists(latest_version_file):\n",
        "                print(\"⚠️ No valid latest version file, using current_df fallback\")\n",
        "                return self.current_application[\"current_df\"].copy()\n",
        "\n",
        "            df = pd.read_csv(latest_version_file)\n",
        "\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"🔗 LOADED FROM VERSION FILE: {os.path.basename(latest_version_file)}\\n\")\n",
        "                f.write(f\"Shape: {df.shape}\\n\")\n",
        "                f.write(f\"Columns: {list(df.columns)}\\n\\n\")\n",
        "\n",
        "            print(f\"🔗 Loaded latest dataframe from: {os.path.basename(latest_version_file)}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading from version file: {e}\")\n",
        "            return self.current_application[\"current_df\"].copy()\n",
        "\n",
        "    def _update_latest_version_file(self, new_version_file, step_number):\n",
        "        \"\"\"Update the latest version file pointer after saving\"\"\"\n",
        "        if new_version_file and os.path.exists(new_version_file):\n",
        "            self.current_application[\"latest_version_file\"] = new_version_file\n",
        "            self.current_application[\"step_version_files\"][step_number] = new_version_file\n",
        "\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"🔗 UPDATED LATEST VERSION: Step {step_number} -> {os.path.basename(new_version_file)}\\n\")\n",
        "\n",
        "    def _save_dataframe_version(self, df, description, step_number):\n",
        "        \"\"\"Save dataframe version and return file path for chaining\"\"\"\n",
        "        try:\n",
        "            session_id = self.current_application[\"session_id\"]\n",
        "            dataframes_dir = os.path.join(self.application_sessions_dir, f\"{session_id}_dataframes\")\n",
        "            os.makedirs(dataframes_dir, exist_ok=True)\n",
        "\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            version_filename = f\"step_{step_number}_version_{timestamp}.csv\"\n",
        "            version_filepath = os.path.join(dataframes_dir, version_filename)\n",
        "\n",
        "            df.to_csv(version_filepath, index=False)\n",
        "\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"💾 Saved dataframe version: {version_filename}\\n\")\n",
        "                f.write(f\"📝 Description: {description}\\n\")\n",
        "                f.write(f\"📊 Shape: {df.shape}\\n\")\n",
        "                f.write(f\"🔗 File Path: {version_filepath}\\n\\n\")\n",
        "\n",
        "            print(f\"💾 Saved version: {version_filename} (Shape: {df.shape})\")\n",
        "            return version_filepath\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Version save failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _save_application_state(self):\n",
        "        \"\"\"Save current application state to file\"\"\"\n",
        "        try:\n",
        "            if not self.current_application:\n",
        "                return False\n",
        "\n",
        "            session_id = self.current_application[\"session_id\"]\n",
        "            session_file = os.path.join(self.application_sessions_dir, f\"{session_id}_state.json\")\n",
        "\n",
        "            save_data = self.current_application.copy()\n",
        "\n",
        "            # Remove large dataframes from save data\n",
        "            dataframes_to_remove = [\"current_df\", \"starting_template_df\", \"final_template_df\", \"new_rent_roll_df\"]\n",
        "            for df_key in dataframes_to_remove:\n",
        "                if df_key in save_data:\n",
        "                    if hasattr(save_data[df_key], 'shape'):\n",
        "                        save_data[f\"{df_key}_shape\"] = save_data[df_key].shape\n",
        "                    del save_data[df_key]\n",
        "\n",
        "            # Also remove dataframes from step states\n",
        "            if \"step_states\" in save_data:\n",
        "                for step_num in save_data[\"step_states\"]:\n",
        "                    if \"dataframe\" in save_data[\"step_states\"][step_num]:\n",
        "                        save_data[\"step_states\"][step_num][\"dataframe_shape\"] = save_data[\"step_states\"][step_num][\"dataframe\"].shape\n",
        "                        del save_data[\"step_states\"][step_num][\"dataframe\"]\n",
        "\n",
        "            with open(session_file, 'w') as f:\n",
        "                json.dump(save_data, f, indent=2, default=str)\n",
        "\n",
        "            print(f\"💾 Application state saved: {session_file}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to save application state: {e}\")\n",
        "            return False\n",
        "\n",
        "    def print_formatted_table(self, df, title=\"DataFrame\", max_rows=10):\n",
        "        \"\"\"Format and print dataframe nicely\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"{title}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Shape: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "        print(\"\\nData:\")\n",
        "        if len(df) > max_rows:\n",
        "            print(df.head(max_rows).to_string())\n",
        "            print(f\"\\n... ({len(df) - max_rows} more rows)\")\n",
        "        else:\n",
        "            print(df.to_string())\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "    def get_application_status(self):\n",
        "        \"\"\"Get current application status with enhanced info\"\"\"\n",
        "        if not self.current_application:\n",
        "            return \"📴 No active template application session\"\n",
        "\n",
        "        total = self.current_application[\"total_steps\"]\n",
        "        current = self.current_application[\"current_step\"]\n",
        "        completed = len(self.current_application[\"completed_steps\"])\n",
        "        failed = len(self.current_application[\"failed_steps\"])\n",
        "\n",
        "        # Show latest version file info\n",
        "        latest_version = self.current_application.get(\"latest_version_file\")\n",
        "        version_info = f\"🔗 Latest Version: {os.path.basename(latest_version)}\" if latest_version else \"🔗 No version file yet\"\n",
        "\n",
        "        # Show back navigation info\n",
        "        back_steps = self.get_available_back_steps()\n",
        "        back_info = f\"🔄 Can go back to: {len(back_steps)} previous steps\" if back_steps else \"🔄 No back navigation available yet\"\n",
        "\n",
        "        return f\"\"\"📋 Enhanced Template Application Status\n",
        "\n",
        "🎯 Template: {self.current_application['template_data'].get('template_name', 'Unknown')}\n",
        "📁 Processing: {self.current_application['new_rent_roll_file']}\n",
        "\n",
        "📊 Progress:\n",
        "• Current Step: {current}/{total}\n",
        "• Completed: {completed}\n",
        "• Failed: {failed}\n",
        "• Remaining: {total - current}\n",
        "\n",
        "{version_info}\n",
        "{back_info}\n",
        "\n",
        "🔄 Status: {'🎉 Completed' if current >= total else '⏳ Ready for next step with user input'}\n",
        "🆕 Enhanced Features: ✅ User input per step, ✅ Back navigation\"\"\"\n",
        "\n",
        "    def _finalize_application(self):\n",
        "        \"\"\"Finalize using the latest version file\"\"\"\n",
        "        if not self.current_application:\n",
        "            return \"No active session to finalize\"\n",
        "\n",
        "        session_id = self.current_application[\"session_id\"]\n",
        "        total_steps = self.current_application[\"total_steps\"]\n",
        "        completed = len(self.current_application[\"completed_steps\"])\n",
        "        failed = len(self.current_application[\"failed_steps\"])\n",
        "\n",
        "        # Load the final dataframe from the latest version file\n",
        "        final_df = self._get_latest_dataframe_from_versions()\n",
        "\n",
        "        # Save final result\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        final_csv = os.path.join(self.application_sessions_dir, f\"{session_id}_FINAL_RESULT_{timestamp}.csv\")\n",
        "        final_excel = os.path.join(self.application_sessions_dir, f\"{session_id}_FINAL_RESULT_{timestamp}.xlsx\")\n",
        "\n",
        "        final_df.to_csv(final_csv, index=False)\n",
        "        final_df.to_excel(final_excel, index=False, engine='openpyxl')\n",
        "\n",
        "        # Log final summary\n",
        "        with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"\\n{'='*60}\\n\")\n",
        "            f.write(\"🎉 ENHANCED TEMPLATE APPLICATION COMPLETED\\n\")\n",
        "            f.write(f\"{'='*60}\\n\")\n",
        "            f.write(f\"Total Steps: {total_steps}\\n\")\n",
        "            f.write(f\"Completed Successfully: {completed}\\n\")\n",
        "            f.write(f\"Failed: {failed}\\n\")\n",
        "            f.write(f\"Success Rate: {(completed/total_steps)*100:.1f}%\\n\")\n",
        "            f.write(f\"Final DataFrame Shape: {final_df.shape}\\n\")\n",
        "            f.write(f\"🔗 Latest Version File: {self.current_application.get('latest_version_file', 'None')}\\n\")\n",
        "            f.write(f\"🆕 Enhanced Features Used: User input per step, Back navigation\\n\")\n",
        "            f.write(f\"Final Result CSV: {final_csv}\\n\")\n",
        "            f.write(f\"Final Result Excel: {final_excel}\\n\")\n",
        "            f.write(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"{'='*60}\\n\")\n",
        "\n",
        "        self.current_application = None\n",
        "\n",
        "        return f\"\"\"🎉 Enhanced Template Application Completed!\n",
        "📊 Results:\n",
        "• Total Steps: {total_steps}\n",
        "• Successfully Completed: {completed}\n",
        "• Failed: {failed}\n",
        "• Success Rate: {(completed/total_steps)*100:.1f}%\n",
        "\n",
        "📁 Final Output:\n",
        "• CSV: {final_csv}\n",
        "• Excel: {final_excel}\n",
        "• Shape: {final_df.shape}\n",
        "\n",
        "🆕 Enhanced features used: User input per step, Back navigation\n",
        "✅ Version file workflow ensures proper step chaining!\"\"\"\n",
        "\n",
        "\n",
        "# Global enhanced template application engine\n",
        "enhanced_template_app_engine = EnhancedTemplateApplicationEngine()\n",
        "\n",
        "# Enhanced functions for the Template Application tab\n",
        "def load_template_for_enhanced_application(template_id):\n",
        "    \"\"\"Load template details for enhanced application\"\"\"\n",
        "    if not template_id:\n",
        "        return \"Please enter a template ID\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        template_summary = enhanced_template_manager.get_template_summary(template_id)\n",
        "\n",
        "        # Get template steps for preview\n",
        "        template_json_path = os.path.join(\"rent_roll_templates\", f\"{template_id}.json\")\n",
        "        if os.path.exists(template_json_path):\n",
        "            with open(template_json_path, 'r') as f:\n",
        "                template_data = json.load(f)\n",
        "\n",
        "            steps_preview = \"\"\n",
        "            workflow_steps = template_data.get(\"raw_workflow_steps\", [])\n",
        "            for i, step in enumerate(workflow_steps[:5], 1):\n",
        "                steps_preview += f\"{i}. {step.get('user_message', 'N/A')[:80]}...\\n\"\n",
        "\n",
        "            if len(workflow_steps) > 5:\n",
        "                steps_preview += f\"... and {len(workflow_steps) - 5} more steps\\n\"\n",
        "\n",
        "            return template_summary, steps_preview, f\"✅ Template loaded: {len(workflow_steps)} steps found\"\n",
        "        else:\n",
        "            return template_summary, \"\", \"❌ Template file not found\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error loading template: {str(e)}\", \"\", \"Failed to load\"\n",
        "\n",
        "def start_enhanced_template_application_session(template_id, new_rent_roll_file):\n",
        "    \"\"\"Start applying template to new rent roll with enhanced features\"\"\"\n",
        "    if not template_id:\n",
        "        return \"❌ Please select a template first\"\n",
        "\n",
        "    if not new_rent_roll_file:\n",
        "        return \"❌ Please upload a new rent roll file\"\n",
        "\n",
        "    try:\n",
        "        import tempfile\n",
        "\n",
        "        # Use the specialized loader\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx')\n",
        "        temp_file.close()\n",
        "        file_path = temp_file.name\n",
        "\n",
        "        # Copy the uploaded file to temporary location\n",
        "        with open(new_rent_roll_file.name, 'rb') as src_file, open(file_path, 'wb') as dst_file:\n",
        "            dst_file.write(src_file.read())\n",
        "\n",
        "        # Use the specialized rent roll loader\n",
        "        try:\n",
        "            print(\"Loading rent roll with specialized loader for enhanced template application...\")\n",
        "            new_df = read_rent_roll_simple(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error with specialized loader: {e}. Falling back to standard loading.\")\n",
        "            new_df = pd.read_excel(file_path)\n",
        "\n",
        "        # Clean up temp file\n",
        "        os.unlink(file_path)\n",
        "\n",
        "        # Start enhanced application session\n",
        "        session_id, status = enhanced_template_app_engine.start_template_application(\n",
        "            template_id=template_id,\n",
        "            new_rent_roll_file=new_rent_roll_file.name,\n",
        "            new_rent_roll_df=new_df\n",
        "        )\n",
        "\n",
        "        if session_id:\n",
        "            return f\"\"\"✅ Enhanced Session Started: {session_id}\n",
        "\n",
        "{status}\n",
        "\n",
        "📊 New Rent Roll Info:\n",
        "• Shape: {new_df.shape}\n",
        "• Columns: {list(new_df.columns)}\n",
        "\n",
        "🎯 Ready to execute {enhanced_template_app_engine.current_application['total_steps']} template steps!\n",
        "\n",
        "🆕 Enhanced Features:\n",
        "• 👤 Provide input for each step\n",
        "• 🔧 Override code if needed\n",
        "• 🔄 Go back to previous steps\n",
        "• 📝 Step-by-step execution control\"\"\"\n",
        "        else:\n",
        "            return status\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error starting enhanced application: {str(e)}\"\n",
        "\n",
        "def get_next_step_details():\n",
        "    \"\"\"Get detailed information about the next step\"\"\"\n",
        "    try:\n",
        "        step_info, step_description = enhanced_template_app_engine.get_next_step_info()\n",
        "\n",
        "        if step_info is None:\n",
        "            return \"🎉 All steps completed!\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        # Format the step information for display\n",
        "        step_display = f\"\"\"📋 Step {step_info['step_number']} of {step_info['total_steps']}\n",
        "\n",
        "🎯 Original Intent:\n",
        "{step_info['original_message']}\n",
        "\n",
        "📊 Current Data:\n",
        "• Shape: {step_info['current_shape']}\n",
        "• Columns: {len(step_info['current_columns'])} columns\n",
        "\n",
        "🔄 Navigation:\n",
        "• Can go back: {'✅ Yes' if step_info['can_go_back'] else '❌ No previous steps'}\"\"\"\n",
        "\n",
        "        return (\n",
        "            step_display,\n",
        "            step_info['original_code'],\n",
        "            step_info['ai_response'][:500] + \"...\" if len(step_info['ai_response']) > 500 else step_info['ai_response'],\n",
        "            \"\", # Clear user input field\n",
        "            \"\"  # Clear code override field\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error getting step details: {str(e)}\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "def execute_step_with_user_input(user_input, code_override):\n",
        "    \"\"\"Execute the current step with user input\"\"\"\n",
        "    try:\n",
        "        result = enhanced_template_app_engine.execute_step_with_user_input(\n",
        "            user_additional_input=user_input or \"\",\n",
        "            override_code=code_override or \"\"\n",
        "        )\n",
        "\n",
        "        if result[\"success\"]:\n",
        "            status_msg = f\"\"\"✅ {result['message']}\n",
        "\n",
        "📊 Execution Details:\n",
        "• User input applied: {'Yes' if result['execution_result'].get('user_input_applied') else 'No'}\n",
        "• Code override used: {'Yes' if result['execution_result'].get('code_override_used') else 'No'}\n",
        "\n",
        "🔄 Navigation Options:\n",
        "• Next step available: {'✅ Yes' if result.get('next_step_available') else '❌ All complete'}\n",
        "• Can go back: {'✅ Yes' if result.get('can_go_back') else '❌ No'}\n",
        "\n",
        "📈 Output:\n",
        "{result['execution_result']['execution_output'][:500]}...\"\"\"\n",
        "\n",
        "        else:\n",
        "            status_msg = f\"\"\"❌ Step {result['step_number']} Failed\n",
        "\n",
        "🚨 Error: {result['error']}\n",
        "\n",
        "🔄 Options:\n",
        "• Can retry: {'✅ Yes' if result.get('can_retry') else '❌ No'}\n",
        "• Can go back: {'✅ Yes' if result.get('can_go_back') else '❌ No'}\n",
        "\n",
        "💡 Suggestions:\n",
        "• Modify your input and try again\n",
        "• Use code override to fix the issue\n",
        "• Go back to a previous step\"\"\"\n",
        "\n",
        "        return status_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error executing step: {str(e)}\"\n",
        "\n",
        "def get_back_navigation_options():\n",
        "    \"\"\"Get available steps for back navigation\"\"\"\n",
        "    try:\n",
        "        back_steps = enhanced_template_app_engine.get_available_back_steps()\n",
        "\n",
        "        if not back_steps:\n",
        "            return \"🔄 No previous steps available for back navigation\"\n",
        "\n",
        "        options_text = \"🔄 Available Steps for Back Navigation:\\n\\n\"\n",
        "        for step in back_steps:\n",
        "            options_text += f\"Step {step['step_number']}: {step['description']}\\n\"\n",
        "            options_text += f\"   📅 {step['timestamp']} | 📊 Shape: {step['shape']}\\n\\n\"\n",
        "\n",
        "        return options_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error getting back navigation options: {str(e)}\"\n",
        "\n",
        "def go_back_to_step(step_number):\n",
        "    \"\"\"Navigate back to a specific step\"\"\"\n",
        "    try:\n",
        "        if not step_number:\n",
        "            return \"❌ Please enter a step number\"\n",
        "\n",
        "        result = enhanced_template_app_engine.go_back_to_step(int(step_number))\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error going back to step: {str(e)}\"\n",
        "\n",
        "def get_enhanced_application_status():\n",
        "    \"\"\"Get enhanced application status\"\"\"\n",
        "    return enhanced_template_app_engine.get_application_status()\n",
        "\n",
        "\n",
        "# Enhanced Gradio Interface\n",
        "def create_enhanced_template_application_tab():\n",
        "    \"\"\"Create the enhanced Template Application tab with user input and back navigation\"\"\"\n",
        "\n",
        "    with gr.Tab(\"🆕 Enhanced Apply Template\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 🎯 Enhanced Template Application with User Input & Back Navigation\n",
        "\n",
        "        **New Features:**\n",
        "        - 👤 **User Input per Step**: Provide additional instructions for each step\n",
        "        - 🔧 **Code Override**: Replace template code with your own\n",
        "        - 🔄 **Back Navigation**: Go back to any previous step and continue from there\n",
        "        - 📝 **Step-by-Step Control**: Execute one step at a time with full control\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"#### 1. Setup Template Application\")\n",
        "\n",
        "                template_id_enhanced = gr.Textbox(\n",
        "                    label=\"Template ID\",\n",
        "                    placeholder=\"e.g., template_20250526_143022\",\n",
        "                    lines=1\n",
        "                )\n",
        "\n",
        "                load_template_enhanced_btn = gr.Button(\"📂 Load Template\", variant=\"secondary\")\n",
        "\n",
        "                new_rent_roll_enhanced = gr.File(\n",
        "                    label=\"New Rent Roll File (.xlsx, .xls)\",\n",
        "                    file_types=[\".xlsx\", \".xls\"]\n",
        "                )\n",
        "\n",
        "                start_enhanced_btn = gr.Button(\"🚀 Start Enhanced Session\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### Template Details\")\n",
        "                template_details_enhanced = gr.HTML(label=\"Template Information\")\n",
        "\n",
        "                template_steps_enhanced = gr.Textbox(\n",
        "                    label=\"Steps Preview\",\n",
        "                    lines=6,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### 📝 Step-by-Step Execution with User Input\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### Current Step Information\")\n",
        "\n",
        "                get_step_btn = gr.Button(\"📋 Get Next Step Details\", variant=\"secondary\")\n",
        "\n",
        "                current_step_info = gr.Textbox(\n",
        "                    label=\"Step Information\",\n",
        "                    lines=8,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    original_code_display = gr.Textbox(\n",
        "                        label=\"Original Template Code\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "                    ai_response_display = gr.Textbox(\n",
        "                        label=\"Original AI Response Context\",\n",
        "                        lines=8,\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"#### 👤 Your Input for This Step\")\n",
        "\n",
        "                user_step_input = gr.Textbox(\n",
        "                    label=\"Additional Instructions\",\n",
        "                    placeholder=\"Optional: Add specific requirements, modifications, or instructions for this step...\",\n",
        "                    lines=4\n",
        "                )\n",
        "\n",
        "                user_code_override = gr.Textbox(\n",
        "                    label=\"Code Override (Optional)\",\n",
        "                    placeholder=\"Optional: Provide your own Python code to use instead of the template code...\",\n",
        "                    lines=6\n",
        "                )\n",
        "\n",
        "                execute_step_btn = gr.Button(\"▶️ Execute Step\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"#### 🔄 Back Navigation\")\n",
        "\n",
        "                get_back_options_btn = gr.Button(\"🔄 Show Back Options\", variant=\"secondary\")\n",
        "\n",
        "                back_options_display = gr.Textbox(\n",
        "                    label=\"Available Previous Steps\",\n",
        "                    lines=6,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                back_step_number = gr.Number(\n",
        "                    label=\"Go Back to Step #\",\n",
        "                    precision=0,      # Integer only\n",
        "                    minimum=0,        # ✅ Allow step 0\n",
        "                    maximum=100,      # Reasonable maximum\n",
        "                    value=None,       # Start with no value\n",
        "                    info=\"Enter 0 to go back to the beginning\"\n",
        "                )\n",
        "\n",
        "                go_back_btn = gr.Button(\"⏪ Go Back to Step\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### 📊 Execution Results & Status\")\n",
        "\n",
        "                execution_results = gr.Textbox(\n",
        "                    label=\"Step Execution Results\",\n",
        "                    lines=12,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                status_enhanced_btn = gr.Button(\"📊 Check Status\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            enhanced_status_display = gr.Textbox(\n",
        "                label=\"Enhanced Application Status\",\n",
        "                lines=8,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        # Event handlers for Enhanced Template Application\n",
        "        load_template_enhanced_btn.click(\n",
        "            load_template_for_enhanced_application,\n",
        "            inputs=[template_id_enhanced],\n",
        "            outputs=[template_details_enhanced, template_steps_enhanced, execution_results]\n",
        "        )\n",
        "\n",
        "        start_enhanced_btn.click(\n",
        "            start_enhanced_template_application_session,\n",
        "            inputs=[template_id_enhanced, new_rent_roll_enhanced],\n",
        "            outputs=[execution_results]\n",
        "        ).then(\n",
        "            get_enhanced_application_status,\n",
        "            outputs=[enhanced_status_display]\n",
        "        )\n",
        "\n",
        "        get_step_btn.click(\n",
        "            get_next_step_details,\n",
        "            outputs=[current_step_info, original_code_display, ai_response_display, user_step_input, user_code_override]\n",
        "        )\n",
        "\n",
        "        execute_step_btn.click(\n",
        "            execute_step_with_user_input,\n",
        "            inputs=[user_step_input, user_code_override],\n",
        "            outputs=[execution_results]\n",
        "        ).then(\n",
        "            get_enhanced_application_status,\n",
        "            outputs=[enhanced_status_display]\n",
        "        )\n",
        "\n",
        "        get_back_options_btn.click(\n",
        "            get_back_navigation_options,\n",
        "            outputs=[back_options_display]\n",
        "        )\n",
        "\n",
        "        go_back_btn.click(\n",
        "            go_back_to_step,\n",
        "            inputs=[back_step_number],\n",
        "            outputs=[execution_results]\n",
        "        ).then(\n",
        "            get_enhanced_application_status,\n",
        "            outputs=[enhanced_status_display]\n",
        "        )\n",
        "\n",
        "        status_enhanced_btn.click(\n",
        "            get_enhanced_application_status,\n",
        "            outputs=[enhanced_status_display]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### 💡 Enhanced Usage Guide:\n",
        "\n",
        "        1. **Setup**: Load template and upload new rent roll file\n",
        "        2. **Start Session**: Begin enhanced template application\n",
        "        3. **Step Details**: Click \"Get Next Step Details\" to see what the step will do\n",
        "        4. **Add Input**: Optionally provide additional instructions or code overrides\n",
        "        5. **Execute**: Run the step with your customizations\n",
        "        6. **Navigate**: Use back navigation to return to previous steps if needed\n",
        "        7. **Repeat**: Continue step-by-step until completion\n",
        "\n",
        "        **🆕 Key Features:**\n",
        "        - **User Input**: Customize each step with additional requirements\n",
        "        - **Code Override**: Replace template code with your own\n",
        "        - **Back Navigation**: Return to any previous step and continue from there\n",
        "        - **Full Control**: Execute steps one at a time with complete oversight\n",
        "        \"\"\")\n",
        "\n",
        "# Replace the old tab with this enhanced version\n",
        "# Use: create_enhanced_template_application_tab() instead of add_template_application_tab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9ilCjPA59aH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4ffj2xbKMzK"
      },
      "outputs": [],
      "source": [
        "def chat_with_compression_status(message, history, selected_message_content=None, selected_message_index=None):\n",
        "    \"\"\"Enhanced wrapper that returns compression status along with chat result and handles version-specific requests\"\"\"\n",
        "    result = chat_with_selected_message(message, history, selected_message_content, selected_message_index)\n",
        "\n",
        "    # Calculate compression info\n",
        "    if history:\n",
        "        total_messages = len(history) * 2 + 1\n",
        "        total_chars = sum(len(msg[0]) + len(msg[1]) for msg in history) + len(message)\n",
        "\n",
        "        # Check if version was mentioned\n",
        "        version_pattern = r'v_\\d{8}_\\d{6}'\n",
        "        version_mentioned = bool(re.search(version_pattern, message))\n",
        "\n",
        "        if total_messages > 40:\n",
        "            status = f\"🔄 Compressed - Managing {total_messages} messages (~{total_chars:,} chars)\"\n",
        "        else:\n",
        "            status = f\"✅ Normal - {total_messages} messages (~{total_chars:,} chars)\"\n",
        "\n",
        "        if version_mentioned:\n",
        "            status += \" | 📋 Version-specific analysis\"\n",
        "    else:\n",
        "        status = \"✅ New conversation\"\n",
        "\n",
        "    return result, status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrK2fThLnbEE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eYiRx9fvejLP",
        "outputId": "a5316e93-2682-47a6-c855-5eb7877341af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-36-1813267413>:363: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Agentic Rent Roll Analysis Chat\", height=500)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3f858d945a3ae533d6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://3f858d945a3ae533d6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:rent_roll_analyzer:Could not find header row with 'Current' marker. Falling back to standard loading.\n",
            "WARNING:rent_roll_analyzer:Could not find header row with 'Current' marker. Falling back to standard loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved dataframe version v_20250617_014250: Initial upload - original dataset from Rent Roll 9-1-24 Excel 1 1.xlsx\n",
            "  - CSV: rent_roll_versions/rent_roll_v_20250617_014250.csv\n",
            "  - Excel: rent_roll_versions/rent_roll_v_20250617_014250.xlsx\n",
            "  - Shape: (30, 9)\n",
            "  - Registry updated: 1 total versions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-211692193>:145: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  {rent_roll_df.head(5).fillna('').to_html(index=False)}\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1965: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Started session recording: session_20250617_014343\n",
            "🔧 Conversation size: 350 tokens\n",
            "Starting code generation with version support...\n",
            "\n",
            "==== STARTING CODE GENERATION WITH VERSION SUPPORT ====\n",
            "User query: \n",
            "USING SPECIFIC VERSION: v_20250617_014250\n",
            "==================================\n",
            "Version Description: Initial upload - original dataset from Rent Roll 9-1-24 Excel 1 1.xlsx\n",
            "Version Created: 20250617_014250\n",
            "Version Shape: (30, 9)\n",
            "Current/Latest Version Shape: (30, 9)\n",
            "\n",
            "IMPORTANT: All analysis will be performed on version v_20250617_014250, NOT the current/latest version.\n",
            "\n",
            "Version Differences:\n",
            "- Row count difference: 0 rows\n",
            "- Column count difference: 0 columns\n",
            "- Version is the latest version\n",
            "\n",
            "Data from Version v_20250617_014250:\n",
            "                Tenant Floor Lease Start Date Lease Expiration     RSF    RSF%  Annual Rent  Monthly Rent Rent PSF\n",
            "0                 AT&T    LL       2024-09-01       2029-08-31   295.0  0.0007      10110.0        842.52    34.27\n",
            "1  Care Tech Solutions    LL       2023-06-01       2028-05-31  8472.0  0.0206     160968.0      13414.00       19\n",
            "2  Care Tech Solutions    LL       2023-06-01       2028-05-31  1847.0  0.0045      23088.0       1923.96     12.5\n",
            "\n",
            "\n",
            "use the table v_20250617_014250 and delete rows where tenant is empty \n",
            "\n",
            "NOTE: Analysis will be performed on the specific version loaded above.\n",
            "Dataframe has 30 rows and 9 columns\n",
            "Using specific version: False\n",
            "Sending FIRST 5 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH VERSION CONTEXT) ====\n",
            "Original conversation size: 1 messages\n",
            "🔧 Applying conversation history compression...\n",
            "Initial token count: 350\n",
            "🔧 Conversation size: 350 tokens\n",
            "Compressed token count: 350\n",
            "Compression ratio: 100.00% of original size\n",
            "Messages after compression: 1\n",
            "Final GPT-4.1 request token estimate: 1891\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE (WITH VERSION CONTEXT) ====\n",
            "You are a Python expert working with a rent roll dataframe that is already loaded as df and contains ALL 30 rows from the latest version (v_20250617_014250). You have access to a representative sample of the data structure, but your code should operate on the full dataframe.\n",
            "\n",
            "Your tasks and requirements:\n",
            "\n",
            "- The dataframe df is already defined and contains all 30 rows of the current/latest version.\n",
            "- Do NOT reload or redefine df; use it as-is.\n",
            "- When displaying tables or results, ALWAYS show ALL ...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION (CURRENT) ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE (CURRENT VERSION) ====\n",
            "# Removing Rows with Empty Tenant Values\n",
            "\n",
            "I'll delete all rows where the Tenant column contains empty values. Here's my approach:\n",
            "\n",
            "1. First, I'll examine the current dataframe to understand what we're working with\n",
            "2. Then I'll filter out rows where the Tenant column is empty (NaN/None)\n",
            "3. Finally, I'll save the new version with the appropriate description\n",
            "\n",
            "Let's start by examining the current dataframe:\n",
            "\n",
            "```python\n",
            "# Display the current dataframe\n",
            "print_formatted_table(df, \"Current Dataframe (Befo...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 2 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Display the current dataframe\n",
            "print_formatted_table(df, \"Current Dataframe (Before Removing Empty Tenant Rows)\")\n",
            "\n",
            "-- Code Block 2 --\n",
            "# Filter out rows where Tenant is empty (NaN/None)\n",
            "df_cleaned = df.dropna(subset=['Tenant'])\n",
            "\n",
            "# Display the cleaned dataframe\n",
            "print_formatted_table(df_cleaned, \"Cleaned Dataframe (After Removing Empty...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE (CURRENT VERSION) ====\n",
            "\n",
            "--- Executing Code Block 1/2 ---\n",
            "\n",
            "Current Dataframe (Before Removing Empty Tenant Rows)\n",
            "================================================================================\n",
            "   Tenant                                             Floor   Lease Start Date Lease Expiration  RSF       RSF%      Annual Rent  Monthly Rent Rent PSF\n",
            "0                                                AT&T      LL  2024-09-01       2029-08-31          295.0  7.00e-04  1.01e+04        842.52     34.27  \n",
            "1                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         8472.0  2.06e-02  1.61e+05      13414.00        19  \n",
            "2                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         1847.0  4.50e-03  2.31e+04       1923.96      12.5  \n",
            "3                            Clear Rate Communication      LL  2017-06-27       2026-08-26         5212.0  1.27e-02  7.69e+04       6406.42     14.75  \n",
            "4                                                Aras  1st Fl  2019-09-05       2028-03-31        16759.0  4.08e-02  4.65e+05      38755.19     27.75  \n",
            "5                                    Dickinson Wright  3rd Fl  2011-08-01       2029-07-31        67798.0  1.65e-01  1.92e+06     160285.77     28.37  \n",
            "6                                    Dickinson Wright  4th Fl  2016-06-01       2029-07-31        19310.0  4.70e-02  5.89e+05      49079.58      30.5  \n",
            "7                                    Dickinson Wright  4th Fl  2018-04-01       2029-07-31         8906.0  2.17e-02  2.72e+05      22636.08      30.5  \n",
            "8                            Clear Rate Communication  4th Fl  2017-06-27       2026-08-26        10989.0  2.67e-02  3.30e+05      27472.50        30  \n",
            "9                                    Horizon Global 2  4th Fl  2017-04-01       2027-10-31         8885.0  2.16e-02  2.67e+05      22212.50        30  \n",
            "10                                     Horizon Global  5th Fl  2016-06-01       2027-10-31        15257.0  3.71e-02  4.50e+05      37506.79      29.5  \n",
            "11                                     Dart Appraisal  5th Fl  2023-06-01       2028-05-31        11954.0  2.91e-02  3.17e+05      26398.42      26.5  \n",
            "12                                                BDO  6th Fl  2016-12-01       2027-04-30        21418.0  5.21e-02  6.64e+05      55329.83        31  \n",
            "13                                   Abraham and Rose      LL  2020-07-13       2030-07-31         5500.0  1.34e-02  1.21e+05      10083.33        22  \n",
            "14                                                USI  1st Fl  2021-09-20       2028-12-31        13199.0  3.21e-02  3.63e+05      30247.71      27.5  \n",
            "15                                 Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31         3146.0  7.70e-03  8.81e+04       7340.68      21.3  \n",
            "16                                                NaN     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "17                   Total Monthly & Annual Base Rent     NaN         NaN              NaN       218947.0  5.32e-01  6.12e+06     509935.28            \n",
            "18  The annual rent does not include the increases...     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "19                                                NaN     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "20                             Available Vacant Space     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "21                                           Pavilion     NaN         NaN              NaN        14000.0  3.40e-02       NaN           NaN       NaN  \n",
            "22                                          Vacant LL     NaN         NaN              NaN        24950.0  6.07e-02       NaN           NaN       NaN  \n",
            "23                                      Vacant 1st Fl     NaN         NaN              NaN        35552.0  8.65e-02       NaN           NaN       NaN  \n",
            "24                                      Vacant 2nd Fl     NaN         NaN              NaN        86358.0  2.10e-01       NaN           NaN       NaN  \n",
            "25                                      Vacant 4th Fl     NaN         NaN              NaN         8750.0  2.13e-02       NaN           NaN       NaN  \n",
            "26                                      Vacant 5th Fl     NaN         NaN              NaN        22625.0  5.50e-02       NaN           NaN       NaN  \n",
            "27                                                NaN     NaN         NaN              NaN       192235.0  4.68e-01       NaN           NaN       NaN  \n",
            "28                                                NaN     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "29                               Total Building Sq ft     NaN         NaN              NaN       411182.0  1.00e+00       NaN           NaN       NaN  \n",
            "\n",
            "✓ Code Block 1 executed successfully\n",
            "\n",
            "--- Executing Code Block 2/2 ---\n",
            "\n",
            "Cleaned Dataframe (After Removing Empty Tenant Rows)\n",
            "================================================================================\n",
            "   Tenant                                             Floor   Lease Start Date Lease Expiration  RSF       RSF%      Annual Rent  Monthly Rent Rent PSF\n",
            "0                                                AT&T      LL  2024-09-01       2029-08-31          295.0  7.00e-04  1.01e+04        842.52     34.27  \n",
            "1                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         8472.0  2.06e-02  1.61e+05      13414.00        19  \n",
            "2                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         1847.0  4.50e-03  2.31e+04       1923.96      12.5  \n",
            "3                            Clear Rate Communication      LL  2017-06-27       2026-08-26         5212.0  1.27e-02  7.69e+04       6406.42     14.75  \n",
            "4                                                Aras  1st Fl  2019-09-05       2028-03-31        16759.0  4.08e-02  4.65e+05      38755.19     27.75  \n",
            "5                                    Dickinson Wright  3rd Fl  2011-08-01       2029-07-31        67798.0  1.65e-01  1.92e+06     160285.77     28.37  \n",
            "6                                    Dickinson Wright  4th Fl  2016-06-01       2029-07-31        19310.0  4.70e-02  5.89e+05      49079.58      30.5  \n",
            "7                                    Dickinson Wright  4th Fl  2018-04-01       2029-07-31         8906.0  2.17e-02  2.72e+05      22636.08      30.5  \n",
            "8                            Clear Rate Communication  4th Fl  2017-06-27       2026-08-26        10989.0  2.67e-02  3.30e+05      27472.50        30  \n",
            "9                                    Horizon Global 2  4th Fl  2017-04-01       2027-10-31         8885.0  2.16e-02  2.67e+05      22212.50        30  \n",
            "10                                     Horizon Global  5th Fl  2016-06-01       2027-10-31        15257.0  3.71e-02  4.50e+05      37506.79      29.5  \n",
            "11                                     Dart Appraisal  5th Fl  2023-06-01       2028-05-31        11954.0  2.91e-02  3.17e+05      26398.42      26.5  \n",
            "12                                                BDO  6th Fl  2016-12-01       2027-04-30        21418.0  5.21e-02  6.64e+05      55329.83        31  \n",
            "13                                   Abraham and Rose      LL  2020-07-13       2030-07-31         5500.0  1.34e-02  1.21e+05      10083.33        22  \n",
            "14                                                USI  1st Fl  2021-09-20       2028-12-31        13199.0  3.21e-02  3.63e+05      30247.71      27.5  \n",
            "15                                 Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31         3146.0  7.70e-03  8.81e+04       7340.68      21.3  \n",
            "17                   Total Monthly & Annual Base Rent     NaN         NaN              NaN       218947.0  5.32e-01  6.12e+06     509935.28            \n",
            "18  The annual rent does not include the increases...     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "20                             Available Vacant Space     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "21                                           Pavilion     NaN         NaN              NaN        14000.0  3.40e-02       NaN           NaN       NaN  \n",
            "22                                          Vacant LL     NaN         NaN              NaN        24950.0  6.07e-02       NaN           NaN       NaN  \n",
            "23                                      Vacant 1st Fl     NaN         NaN              NaN        35552.0  8.65e-02       NaN           NaN       NaN  \n",
            "24                                      Vacant 2nd Fl     NaN         NaN              NaN        86358.0  2.10e-01       NaN           NaN       NaN  \n",
            "25                                      Vacant 4th Fl     NaN         NaN              NaN         8750.0  2.13e-02       NaN           NaN       NaN  \n",
            "26                                      Vacant 5th Fl     NaN         NaN              NaN        22625.0  5.50e-02       NaN           NaN       NaN  \n",
            "29                               Total Building Sq ft     NaN         NaN              NaN       411182.0  1.00e+00       NaN           NaN       NaN  \n",
            "✓ Saved dataframe version v_20250617_014408: Removed rows with empty Tenant values\n",
            "  - CSV: rent_roll_versions/rent_roll_v_20250617_014408.csv\n",
            "  - Excel: rent_roll_versions/rent_roll_v_20250617_014408.xlsx\n",
            "  - Shape: (26, 9)\n",
            "  - Registry updated: 2 total versions\n",
            "\n",
            "✓ Code Block 2 executed successfully\n",
            "\n",
            "==== STEP 4: VALIDATING CHANGES (CURRENT VERSION) ====\n",
            "🔍 Validating changes with GPT-4.1...\n",
            "📝 Logging validation session to: validation_logs/validation_20250617_014408_473.log\n",
            "🔍 Validation: FAIL\n",
            "\n",
            "==== CODE GENERATION COMPLETE (CURRENT VERSION) ====\n",
            "🔧 Conversation size: 3928 tokens\n",
            "Starting code generation with version support...\n",
            "\n",
            "==== STARTING CODE GENERATION WITH VERSION SUPPORT ====\n",
            "User query: \n",
            "USING SPECIFIC VERSION: v_20250617_014408\n",
            "==================================\n",
            "Version Description: Removed rows with empty Tenant values\n",
            "Version Created: 20250617_014408\n",
            "Version Shape: (26, 9)\n",
            "Current/Latest Version Shape: (30, 9)\n",
            "\n",
            "IMPORTANT: All analysis will be performed on version v_20250617_014408, NOT the current/latest version.\n",
            "\n",
            "Version Differences:\n",
            "- Row count difference: -4 rows\n",
            "- Column count difference: 0 columns\n",
            "- Version is the latest version\n",
            "\n",
            "Data from Version v_20250617_014408:\n",
            "                Tenant Floor Lease Start Date Lease Expiration     RSF    RSF%  Annual Rent  Monthly Rent Rent PSF\n",
            "0                 AT&T    LL       2024-09-01       2029-08-31   295.0  0.0007      10110.0        842.52    34.27\n",
            "1  Care Tech Solutions    LL       2023-06-01       2028-05-31  8472.0  0.0206     160968.0      13414.00       19\n",
            "2  Care Tech Solutions    LL       2023-06-01       2028-05-31  1847.0  0.0045      23088.0       1923.96     12.5\n",
            "\n",
            "\n",
            "Using rent roll dataset version v_20250617_014408, create a new categorical column titled \"Residents\" that classifies each tenant record as either \"Occupied\" or \"Vacant\" based on the presence of valid tenant information in the \"Tenant\" column. Implement this classification by applying a conditional logic operation that assigns \"Occupied\" status to all rows in this version, as the dataset has already undergone preprocessing to remove entries with empty Tenant values (as evidenced by the version description). After creating the column, validate the classification by cross-referencing with lease dates to ensure all active leases are properly categorized, calculate the occupancy rate as a percentage of total rentable square footage (RSF), and generate summary statistics showing the distribution of occupied versus vacant space by floor. This enhancement will provide critical occupancy metrics for portfolio performance analysis and enable more accurate vacancy reporting for the property.\n",
            "\n",
            "NOTE: Analysis will be performed on the specific version loaded above.\n",
            "Dataframe has 26 rows and 9 columns\n",
            "Using specific version: False\n",
            "Sending FIRST 5 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH VERSION CONTEXT) ====\n",
            "Original conversation size: 3 messages\n",
            "🔧 Applying conversation history compression...\n",
            "Initial token count: 3928\n",
            "🔧 Conversation size: 3928 tokens\n",
            "Compressed token count: 3928\n",
            "Compression ratio: 100.00% of original size\n",
            "Messages after compression: 3\n",
            "Final GPT-4.1 request token estimate: 5487\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE (WITH VERSION CONTEXT) ====\n",
            "You are an expert Python analyst working with a rent roll dataframe that is already loaded as the variable df. This dataframe contains ALL 26 rows from the latest version (v_20250617_014408), which has already had rows with empty Tenant values removed. The dataframe columns are: ['Tenant', 'Floor', 'Lease Start Date', 'Lease Expiration', 'RSF', 'RSF%', 'Annual Rent', 'Monthly Rent', 'Rent PSF']. You have access to a representative sample of the data structure, but your code should always operate...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION (CURRENT) ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE (CURRENT VERSION) ====\n",
            "# Creating a \"Residents\" Classification Column and Analyzing Occupancy\n",
            "\n",
            "I'll create a new categorical column called \"Residents\" that classifies each tenant record as either \"Occupied\" or \"Vacant\" based on the tenant information. Since we're working with version v_20250617_014408 which has already had rows with empty Tenant values removed, I'll need to analyze the data to determine which entries represent actual tenants versus summary rows or vacant spaces.\n",
            "\n",
            "## Approach:\n",
            "1. Examine the dataframe ...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 3 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Display the current dataframe\n",
            "print_formatted_table(df, \"Current Dataframe (v_20250617_014408)\")\n",
            "\n",
            "-- Code Block 2 --\n",
            "# First, let's identify patterns in the data to determine occupancy status\n",
            "# Rows with \"Vacant\" in the Tenant name are clearly vacant spaces\n",
            "# Summary rows like \"Total Building Sq ft\" are not tenant s...\n",
            "\n",
            "-- Code Block 3 --\n",
            "# Calculate occupancy rate based on RSF\n",
            "total_rsf = df[df['Tenant'] == 'Total Building Sq ft']['RSF'].values[0]\n",
            "occupied_rsf = df[df['Residents'] == 'Occupied']['RSF'].sum()\n",
            "vacant_rsf = df[df['Reside...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE (CURRENT VERSION) ====\n",
            "\n",
            "--- Executing Code Block 1/3 ---\n",
            "\n",
            "Current Dataframe (v_20250617_014408)\n",
            "================================================================================\n",
            "   Tenant                                             Floor   Lease Start Date Lease Expiration  RSF       RSF%      Annual Rent  Monthly Rent Rent PSF\n",
            "0                                                AT&T      LL  2024-09-01       2029-08-31          295.0  7.00e-04  1.01e+04        842.52     34.27  \n",
            "1                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         8472.0  2.06e-02  1.61e+05      13414.00        19  \n",
            "2                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         1847.0  4.50e-03  2.31e+04       1923.96      12.5  \n",
            "3                            Clear Rate Communication      LL  2017-06-27       2026-08-26         5212.0  1.27e-02  7.69e+04       6406.42     14.75  \n",
            "4                                                Aras  1st Fl  2019-09-05       2028-03-31        16759.0  4.08e-02  4.65e+05      38755.19     27.75  \n",
            "5                                    Dickinson Wright  3rd Fl  2011-08-01       2029-07-31        67798.0  1.65e-01  1.92e+06     160285.77     28.37  \n",
            "6                                    Dickinson Wright  4th Fl  2016-06-01       2029-07-31        19310.0  4.70e-02  5.89e+05      49079.58      30.5  \n",
            "7                                    Dickinson Wright  4th Fl  2018-04-01       2029-07-31         8906.0  2.17e-02  2.72e+05      22636.08      30.5  \n",
            "8                            Clear Rate Communication  4th Fl  2017-06-27       2026-08-26        10989.0  2.67e-02  3.30e+05      27472.50        30  \n",
            "9                                    Horizon Global 2  4th Fl  2017-04-01       2027-10-31         8885.0  2.16e-02  2.67e+05      22212.50        30  \n",
            "10                                     Horizon Global  5th Fl  2016-06-01       2027-10-31        15257.0  3.71e-02  4.50e+05      37506.79      29.5  \n",
            "11                                     Dart Appraisal  5th Fl  2023-06-01       2028-05-31        11954.0  2.91e-02  3.17e+05      26398.42      26.5  \n",
            "12                                                BDO  6th Fl  2016-12-01       2027-04-30        21418.0  5.21e-02  6.64e+05      55329.83        31  \n",
            "13                                   Abraham and Rose      LL  2020-07-13       2030-07-31         5500.0  1.34e-02  1.21e+05      10083.33        22  \n",
            "14                                                USI  1st Fl  2021-09-20       2028-12-31        13199.0  3.21e-02  3.63e+05      30247.71      27.5  \n",
            "15                                 Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31         3146.0  7.70e-03  8.81e+04       7340.68      21.3  \n",
            "16                   Total Monthly & Annual Base Rent     NaN         NaN              NaN       218947.0  5.32e-01  6.12e+06     509935.28            \n",
            "17  The annual rent does not include the increases...     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "18                             Available Vacant Space     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN  \n",
            "19                                           Pavilion     NaN         NaN              NaN        14000.0  3.40e-02       NaN           NaN       NaN  \n",
            "20                                          Vacant LL     NaN         NaN              NaN        24950.0  6.07e-02       NaN           NaN       NaN  \n",
            "21                                      Vacant 1st Fl     NaN         NaN              NaN        35552.0  8.65e-02       NaN           NaN       NaN  \n",
            "22                                      Vacant 2nd Fl     NaN         NaN              NaN        86358.0  2.10e-01       NaN           NaN       NaN  \n",
            "23                                      Vacant 4th Fl     NaN         NaN              NaN         8750.0  2.13e-02       NaN           NaN       NaN  \n",
            "24                                      Vacant 5th Fl     NaN         NaN              NaN        22625.0  5.50e-02       NaN           NaN       NaN  \n",
            "25                               Total Building Sq ft     NaN         NaN              NaN       411182.0  1.00e+00       NaN           NaN       NaN  \n",
            "\n",
            "✓ Code Block 1 executed successfully\n",
            "\n",
            "--- Executing Code Block 2/3 ---\n",
            "\n",
            "Dataframe with Residents Classification\n",
            "================================================================================\n",
            "   Tenant                                             Floor   Lease Start Date Lease Expiration  RSF       RSF%      Annual Rent  Monthly Rent Rent PSF Residents\n",
            "0                                                AT&T      LL  2024-09-01       2029-08-31          295.0  7.00e-04  1.01e+04        842.52     34.27    Occupied\n",
            "1                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         8472.0  2.06e-02  1.61e+05      13414.00        19    Occupied\n",
            "2                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         1847.0  4.50e-03  2.31e+04       1923.96      12.5    Occupied\n",
            "3                            Clear Rate Communication      LL  2017-06-27       2026-08-26         5212.0  1.27e-02  7.69e+04       6406.42     14.75    Occupied\n",
            "4                                                Aras  1st Fl  2019-09-05       2028-03-31        16759.0  4.08e-02  4.65e+05      38755.19     27.75    Occupied\n",
            "5                                    Dickinson Wright  3rd Fl  2011-08-01       2029-07-31        67798.0  1.65e-01  1.92e+06     160285.77     28.37    Occupied\n",
            "6                                    Dickinson Wright  4th Fl  2016-06-01       2029-07-31        19310.0  4.70e-02  5.89e+05      49079.58      30.5    Occupied\n",
            "7                                    Dickinson Wright  4th Fl  2018-04-01       2029-07-31         8906.0  2.17e-02  2.72e+05      22636.08      30.5    Occupied\n",
            "8                            Clear Rate Communication  4th Fl  2017-06-27       2026-08-26        10989.0  2.67e-02  3.30e+05      27472.50        30    Occupied\n",
            "9                                    Horizon Global 2  4th Fl  2017-04-01       2027-10-31         8885.0  2.16e-02  2.67e+05      22212.50        30    Occupied\n",
            "10                                     Horizon Global  5th Fl  2016-06-01       2027-10-31        15257.0  3.71e-02  4.50e+05      37506.79      29.5    Occupied\n",
            "11                                     Dart Appraisal  5th Fl  2023-06-01       2028-05-31        11954.0  2.91e-02  3.17e+05      26398.42      26.5    Occupied\n",
            "12                                                BDO  6th Fl  2016-12-01       2027-04-30        21418.0  5.21e-02  6.64e+05      55329.83        31    Occupied\n",
            "13                                   Abraham and Rose      LL  2020-07-13       2030-07-31         5500.0  1.34e-02  1.21e+05      10083.33        22    Occupied\n",
            "14                                                USI  1st Fl  2021-09-20       2028-12-31        13199.0  3.21e-02  3.63e+05      30247.71      27.5    Occupied\n",
            "15                                 Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31         3146.0  7.70e-03  8.81e+04       7340.68      21.3    Occupied\n",
            "16                   Total Monthly & Annual Base Rent     NaN         NaN              NaN       218947.0  5.32e-01  6.12e+06     509935.28               Summary\n",
            "17  The annual rent does not include the increases...     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN     Summary\n",
            "18                             Available Vacant Space     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN      Vacant\n",
            "19                                           Pavilion     NaN         NaN              NaN        14000.0  3.40e-02       NaN           NaN       NaN     Summary\n",
            "20                                          Vacant LL     NaN         NaN              NaN        24950.0  6.07e-02       NaN           NaN       NaN      Vacant\n",
            "21                                      Vacant 1st Fl     NaN         NaN              NaN        35552.0  8.65e-02       NaN           NaN       NaN      Vacant\n",
            "22                                      Vacant 2nd Fl     NaN         NaN              NaN        86358.0  2.10e-01       NaN           NaN       NaN      Vacant\n",
            "23                                      Vacant 4th Fl     NaN         NaN              NaN         8750.0  2.13e-02       NaN           NaN       NaN      Vacant\n",
            "24                                      Vacant 5th Fl     NaN         NaN              NaN        22625.0  5.50e-02       NaN           NaN       NaN      Vacant\n",
            "25                               Total Building Sq ft     NaN         NaN              NaN       411182.0  1.00e+00       NaN           NaN       NaN     Summary\n",
            "\n",
            "Validation of Residents Classification\n",
            "================================================================================\n",
            "   Tenant                                             Lease Start Date Lease Expiration Residents\n",
            "0                                                AT&T  2024-09-01       2029-08-31       Occupied\n",
            "1                                 Care Tech Solutions  2023-06-01       2028-05-31       Occupied\n",
            "2                                 Care Tech Solutions  2023-06-01       2028-05-31       Occupied\n",
            "3                            Clear Rate Communication  2017-06-27       2026-08-26       Occupied\n",
            "4                                                Aras  2019-09-05       2028-03-31       Occupied\n",
            "5                                    Dickinson Wright  2011-08-01       2029-07-31       Occupied\n",
            "6                                    Dickinson Wright  2016-06-01       2029-07-31       Occupied\n",
            "7                                    Dickinson Wright  2018-04-01       2029-07-31       Occupied\n",
            "8                            Clear Rate Communication  2017-06-27       2026-08-26       Occupied\n",
            "9                                    Horizon Global 2  2017-04-01       2027-10-31       Occupied\n",
            "10                                     Horizon Global  2016-06-01       2027-10-31       Occupied\n",
            "11                                     Dart Appraisal  2023-06-01       2028-05-31       Occupied\n",
            "12                                                BDO  2016-12-01       2027-04-30       Occupied\n",
            "13                                   Abraham and Rose  2020-07-13       2030-07-31       Occupied\n",
            "14                                                USI  2021-09-20       2028-12-31       Occupied\n",
            "15                                 Power Mortgage LLC  2023-06-01       2028-07-31       Occupied\n",
            "16                   Total Monthly & Annual Base Rent         NaN              NaN        Summary\n",
            "17  The annual rent does not include the increases...         NaN              NaN        Summary\n",
            "18                             Available Vacant Space         NaN              NaN         Vacant\n",
            "19                                           Pavilion         NaN              NaN        Summary\n",
            "20                                          Vacant LL         NaN              NaN         Vacant\n",
            "21                                      Vacant 1st Fl         NaN              NaN         Vacant\n",
            "22                                      Vacant 2nd Fl         NaN              NaN         Vacant\n",
            "23                                      Vacant 4th Fl         NaN              NaN         Vacant\n",
            "24                                      Vacant 5th Fl         NaN              NaN         Vacant\n",
            "25                               Total Building Sq ft         NaN              NaN        Summary\n",
            "\n",
            "✓ Code Block 2 executed successfully\n",
            "\n",
            "--- Executing Code Block 3/3 ---\n",
            "\n",
            "Occupancy Metrics:\n",
            "Total Rentable Square Footage: 411,182.00 RSF\n",
            "Occupied Space: 218,947.00 RSF (53.25%)\n",
            "Vacant Space: 178,235.00 RSF (43.35%)\n",
            "\n",
            "Occupancy Statistics by Floor:\n",
            "Could not generate complete floor statistics due to missing data categories.\n",
            "\n",
            "Available Floor Statistics\n",
            "================================================================================\n",
            "Residents  Occupied\n",
            "Floor              \n",
            "1st Fl     29958.0 \n",
            "3rd Fl     67798.0 \n",
            "4th Fl     48090.0 \n",
            "5th Fl     30357.0 \n",
            "6th Fl     21418.0 \n",
            "LL         21326.0 \n",
            "✓ Saved dataframe version v_20250617_014746: Added Residents classification column and occupancy analysis\n",
            "  - CSV: rent_roll_versions/rent_roll_v_20250617_014746.csv\n",
            "  - Excel: rent_roll_versions/rent_roll_v_20250617_014746.xlsx\n",
            "  - Shape: (26, 10)\n",
            "  - Registry updated: 3 total versions\n",
            "\n",
            "✓ Code Block 3 executed successfully\n",
            "\n",
            "==== STEP 4: VALIDATING CHANGES (CURRENT VERSION) ====\n",
            "🔍 Validating changes with GPT-4.1...\n",
            "📝 Logging validation session to: validation_logs/validation_20250617_014746_107.log\n",
            "🔍 Validation: PASS\n",
            "\n",
            "==== CODE GENERATION COMPLETE (CURRENT VERSION) ====\n",
            "🔧 Conversation size: 11625 tokens\n",
            "🔄 Applying compression/truncation...\n",
            "✅ Compressed: 11625 -> 11625 tokens\n",
            "Starting code generation with version support...\n",
            "\n",
            "==== STARTING CODE GENERATION WITH VERSION SUPPORT ====\n",
            "User query: \n",
            "USING SPECIFIC VERSION: v_20250617_014746\n",
            "==================================\n",
            "Version Description: Added Residents classification column and occupancy analysis\n",
            "Version Created: 20250617_014746\n",
            "Version Shape: (26, 10)\n",
            "Current/Latest Version Shape: (30, 9)\n",
            "\n",
            "IMPORTANT: All analysis will be performed on version v_20250617_014746, NOT the current/latest version.\n",
            "\n",
            "Version Differences:\n",
            "- Row count difference: -4 rows\n",
            "- Column count difference: 1 columns\n",
            "- Version is the latest version\n",
            "\n",
            "Data from Version v_20250617_014746:\n",
            "                Tenant Floor Lease Start Date Lease Expiration     RSF    RSF%  Annual Rent  Monthly Rent Rent PSF Residents\n",
            "0                 AT&T    LL       2024-09-01       2029-08-31   295.0  0.0007      10110.0        842.52    34.27  Occupied\n",
            "1  Care Tech Solutions    LL       2023-06-01       2028-05-31  8472.0  0.0206     160968.0      13414.00       19  Occupied\n",
            "2  Care Tech Solutions    LL       2023-06-01       2028-05-31  1847.0  0.0045      23088.0       1923.96     12.5  Occupied\n",
            "\n",
            "Columns removed since this version: ['Residents']\n",
            "\n",
            "use the table v_20250617_014746 and for the tenant Pavilion change residents value from summary to Vacant\n",
            "\n",
            "NOTE: Analysis will be performed on the specific version loaded above.\n",
            "Dataframe has 26 rows and 10 columns\n",
            "Using specific version: False\n",
            "Sending FIRST 5 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH VERSION CONTEXT) ====\n",
            "Original conversation size: 5 messages\n",
            "🔧 Applying conversation history compression...\n",
            "Initial token count: 11625\n",
            "🔧 Conversation size: 11625 tokens\n",
            "🔄 Applying compression/truncation...\n",
            "✅ Compressed: 11625 -> 11625 tokens\n",
            "Compressed token count: 11625\n",
            "Compression ratio: 100.00% of original size\n",
            "Messages after compression: 5\n",
            "Final GPT-4.1 request token estimate: 13233\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE (WITH VERSION CONTEXT) ====\n",
            "You are an expert Python analyst working with a rent roll dataset. The dataframe df is ALREADY LOADED and contains ALL 26 rows from the current/latest version (v_20250617_014746). You have access to a representative sample of the data structure and columns, as shown below:\n",
            "\n",
            "Sample Data (first 5 rows):\n",
            "\n",
            "| Tenant                  | Floor   | Lease Start Date | Lease Expiration |    RSF |  RSF%   | Annual Rent | Monthly Rent | Rent PSF | Residents |\n",
            "|-------------------------|---------|------------...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION (CURRENT) ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE (CURRENT VERSION) ====\n",
            "# Changing Residents Value for Pavilion from Summary to Vacant\n",
            "\n",
            "I'll update the Residents value for the tenant \"Pavilion\" from \"Summary\" to \"Vacant\" in the specified version of the dataframe. Here's my approach:\n",
            "\n",
            "1. First, I'll examine the current dataframe to locate the Pavilion tenant row\n",
            "2. Then I'll update the Residents value for that specific row\n",
            "3. Finally, I'll save the new version with an appropriate description\n",
            "\n",
            "Let's start by examining the current dataframe to locate the Pavilion tenan...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 2 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Display the current dataframe to locate the Pavilion tenant\n",
            "print_formatted_table(df, \"Current Dataframe (Before Update)\")\n",
            "\n",
            "-- Code Block 2 --\n",
            "# Find the row index for the Pavilion tenant\n",
            "pavilion_index = df[df['Tenant'] == 'Pavilion'].index\n",
            "\n",
            "# Check if Pavilion tenant exists in the dataframe\n",
            "if len(pavilion_index) > 0:\n",
            "    # Update the Resi...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE (CURRENT VERSION) ====\n",
            "\n",
            "--- Executing Code Block 1/2 ---\n",
            "\n",
            "Current Dataframe (Before Update)\n",
            "================================================================================\n",
            "   Tenant                                             Floor   Lease Start Date Lease Expiration  RSF       RSF%      Annual Rent  Monthly Rent Rent PSF Residents\n",
            "0                                                AT&T      LL  2024-09-01       2029-08-31          295.0  7.00e-04  1.01e+04        842.52     34.27    Occupied\n",
            "1                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         8472.0  2.06e-02  1.61e+05      13414.00        19    Occupied\n",
            "2                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         1847.0  4.50e-03  2.31e+04       1923.96      12.5    Occupied\n",
            "3                            Clear Rate Communication      LL  2017-06-27       2026-08-26         5212.0  1.27e-02  7.69e+04       6406.42     14.75    Occupied\n",
            "4                                                Aras  1st Fl  2019-09-05       2028-03-31        16759.0  4.08e-02  4.65e+05      38755.19     27.75    Occupied\n",
            "5                                    Dickinson Wright  3rd Fl  2011-08-01       2029-07-31        67798.0  1.65e-01  1.92e+06     160285.77     28.37    Occupied\n",
            "6                                    Dickinson Wright  4th Fl  2016-06-01       2029-07-31        19310.0  4.70e-02  5.89e+05      49079.58      30.5    Occupied\n",
            "7                                    Dickinson Wright  4th Fl  2018-04-01       2029-07-31         8906.0  2.17e-02  2.72e+05      22636.08      30.5    Occupied\n",
            "8                            Clear Rate Communication  4th Fl  2017-06-27       2026-08-26        10989.0  2.67e-02  3.30e+05      27472.50        30    Occupied\n",
            "9                                    Horizon Global 2  4th Fl  2017-04-01       2027-10-31         8885.0  2.16e-02  2.67e+05      22212.50        30    Occupied\n",
            "10                                     Horizon Global  5th Fl  2016-06-01       2027-10-31        15257.0  3.71e-02  4.50e+05      37506.79      29.5    Occupied\n",
            "11                                     Dart Appraisal  5th Fl  2023-06-01       2028-05-31        11954.0  2.91e-02  3.17e+05      26398.42      26.5    Occupied\n",
            "12                                                BDO  6th Fl  2016-12-01       2027-04-30        21418.0  5.21e-02  6.64e+05      55329.83        31    Occupied\n",
            "13                                   Abraham and Rose      LL  2020-07-13       2030-07-31         5500.0  1.34e-02  1.21e+05      10083.33        22    Occupied\n",
            "14                                                USI  1st Fl  2021-09-20       2028-12-31        13199.0  3.21e-02  3.63e+05      30247.71      27.5    Occupied\n",
            "15                                 Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31         3146.0  7.70e-03  8.81e+04       7340.68      21.3    Occupied\n",
            "16                   Total Monthly & Annual Base Rent     NaN         NaN              NaN       218947.0  5.32e-01  6.12e+06     509935.28               Summary\n",
            "17  The annual rent does not include the increases...     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN     Summary\n",
            "18                             Available Vacant Space     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN      Vacant\n",
            "19                                           Pavilion     NaN         NaN              NaN        14000.0  3.40e-02       NaN           NaN       NaN     Summary\n",
            "20                                          Vacant LL     NaN         NaN              NaN        24950.0  6.07e-02       NaN           NaN       NaN      Vacant\n",
            "21                                      Vacant 1st Fl     NaN         NaN              NaN        35552.0  8.65e-02       NaN           NaN       NaN      Vacant\n",
            "22                                      Vacant 2nd Fl     NaN         NaN              NaN        86358.0  2.10e-01       NaN           NaN       NaN      Vacant\n",
            "23                                      Vacant 4th Fl     NaN         NaN              NaN         8750.0  2.13e-02       NaN           NaN       NaN      Vacant\n",
            "24                                      Vacant 5th Fl     NaN         NaN              NaN        22625.0  5.50e-02       NaN           NaN       NaN      Vacant\n",
            "25                               Total Building Sq ft     NaN         NaN              NaN       411182.0  1.00e+00       NaN           NaN       NaN     Summary\n",
            "\n",
            "✓ Code Block 1 executed successfully\n",
            "\n",
            "--- Executing Code Block 2/2 ---\n",
            "\n",
            "Updated Residents value for Pavilion tenant from 'Summary' to 'Vacant'\n",
            "\n",
            "Updated Dataframe (After Changing Pavilion's Residents Value)\n",
            "================================================================================\n",
            "   Tenant                                             Floor   Lease Start Date Lease Expiration  RSF       RSF%      Annual Rent  Monthly Rent Rent PSF Residents\n",
            "0                                                AT&T      LL  2024-09-01       2029-08-31          295.0  7.00e-04  1.01e+04        842.52     34.27    Occupied\n",
            "1                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         8472.0  2.06e-02  1.61e+05      13414.00        19    Occupied\n",
            "2                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         1847.0  4.50e-03  2.31e+04       1923.96      12.5    Occupied\n",
            "3                            Clear Rate Communication      LL  2017-06-27       2026-08-26         5212.0  1.27e-02  7.69e+04       6406.42     14.75    Occupied\n",
            "4                                                Aras  1st Fl  2019-09-05       2028-03-31        16759.0  4.08e-02  4.65e+05      38755.19     27.75    Occupied\n",
            "5                                    Dickinson Wright  3rd Fl  2011-08-01       2029-07-31        67798.0  1.65e-01  1.92e+06     160285.77     28.37    Occupied\n",
            "6                                    Dickinson Wright  4th Fl  2016-06-01       2029-07-31        19310.0  4.70e-02  5.89e+05      49079.58      30.5    Occupied\n",
            "7                                    Dickinson Wright  4th Fl  2018-04-01       2029-07-31         8906.0  2.17e-02  2.72e+05      22636.08      30.5    Occupied\n",
            "8                            Clear Rate Communication  4th Fl  2017-06-27       2026-08-26        10989.0  2.67e-02  3.30e+05      27472.50        30    Occupied\n",
            "9                                    Horizon Global 2  4th Fl  2017-04-01       2027-10-31         8885.0  2.16e-02  2.67e+05      22212.50        30    Occupied\n",
            "10                                     Horizon Global  5th Fl  2016-06-01       2027-10-31        15257.0  3.71e-02  4.50e+05      37506.79      29.5    Occupied\n",
            "11                                     Dart Appraisal  5th Fl  2023-06-01       2028-05-31        11954.0  2.91e-02  3.17e+05      26398.42      26.5    Occupied\n",
            "12                                                BDO  6th Fl  2016-12-01       2027-04-30        21418.0  5.21e-02  6.64e+05      55329.83        31    Occupied\n",
            "13                                   Abraham and Rose      LL  2020-07-13       2030-07-31         5500.0  1.34e-02  1.21e+05      10083.33        22    Occupied\n",
            "14                                                USI  1st Fl  2021-09-20       2028-12-31        13199.0  3.21e-02  3.63e+05      30247.71      27.5    Occupied\n",
            "15                                 Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31         3146.0  7.70e-03  8.81e+04       7340.68      21.3    Occupied\n",
            "16                   Total Monthly & Annual Base Rent     NaN         NaN              NaN       218947.0  5.32e-01  6.12e+06     509935.28               Summary\n",
            "17  The annual rent does not include the increases...     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN     Summary\n",
            "18                             Available Vacant Space     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN      Vacant\n",
            "19                                           Pavilion     NaN         NaN              NaN        14000.0  3.40e-02       NaN           NaN       NaN      Vacant\n",
            "20                                          Vacant LL     NaN         NaN              NaN        24950.0  6.07e-02       NaN           NaN       NaN      Vacant\n",
            "21                                      Vacant 1st Fl     NaN         NaN              NaN        35552.0  8.65e-02       NaN           NaN       NaN      Vacant\n",
            "22                                      Vacant 2nd Fl     NaN         NaN              NaN        86358.0  2.10e-01       NaN           NaN       NaN      Vacant\n",
            "23                                      Vacant 4th Fl     NaN         NaN              NaN         8750.0  2.13e-02       NaN           NaN       NaN      Vacant\n",
            "24                                      Vacant 5th Fl     NaN         NaN              NaN        22625.0  5.50e-02       NaN           NaN       NaN      Vacant\n",
            "25                               Total Building Sq ft     NaN         NaN              NaN       411182.0  1.00e+00       NaN           NaN       NaN     Summary\n",
            "✓ Saved dataframe version v_20250617_015006: Changed Residents value for Pavilion from Summary to Vacant\n",
            "  - CSV: rent_roll_versions/rent_roll_v_20250617_015006.csv\n",
            "  - Excel: rent_roll_versions/rent_roll_v_20250617_015006.xlsx\n",
            "  - Shape: (26, 10)\n",
            "  - Registry updated: 4 total versions\n",
            "\n",
            "✓ Code Block 2 executed successfully\n",
            "\n",
            "==== STEP 4: VALIDATING CHANGES (CURRENT VERSION) ====\n",
            "🔍 Validating changes with GPT-4.1...\n",
            "📝 Logging validation session to: validation_logs/validation_20250617_015006_450.log\n",
            "🔍 Validation: FAIL\n",
            "\n",
            "==== CODE GENERATION COMPLETE (CURRENT VERSION) ====\n",
            "🔧 Conversation size: 18306 tokens\n",
            "🔄 Applying compression/truncation...\n",
            "✅ Compressed: 18306 -> 14873 tokens\n",
            "Starting code generation with version support...\n",
            "\n",
            "==== STARTING CODE GENERATION WITH VERSION SUPPORT ====\n",
            "User query: \n",
            "USING SPECIFIC VERSION: v_20250617_015006\n",
            "==================================\n",
            "Version Description: Changed Residents value for Pavilion from Summary to Vacant\n",
            "Version Created: 20250617_015006\n",
            "Version Shape: (26, 10)\n",
            "Current/Latest Version Shape: (30, 9)\n",
            "\n",
            "IMPORTANT: All analysis will be performed on version v_20250617_015006, NOT the current/latest version.\n",
            "\n",
            "Version Differences:\n",
            "- Row count difference: -4 rows\n",
            "- Column count difference: 1 columns\n",
            "- Version is the latest version\n",
            "\n",
            "Data from Version v_20250617_015006:\n",
            "                Tenant Floor Lease Start Date Lease Expiration     RSF    RSF%  Annual Rent  Monthly Rent Rent PSF Residents\n",
            "0                 AT&T    LL       2024-09-01       2029-08-31   295.0  0.0007      10110.0        842.52    34.27  Occupied\n",
            "1  Care Tech Solutions    LL       2023-06-01       2028-05-31  8472.0  0.0206     160968.0      13414.00       19  Occupied\n",
            "2  Care Tech Solutions    LL       2023-06-01       2028-05-31  1847.0  0.0045      23088.0       1923.96     12.5  Occupied\n",
            "\n",
            "Columns removed since this version: ['Residents']\n",
            "\n",
            "use the table v_20250617_015006 and delete all rows where Residents is Summary\n",
            "\n",
            "NOTE: Analysis will be performed on the specific version loaded above.\n",
            "Dataframe has 26 rows and 10 columns\n",
            "Using specific version: False\n",
            "Sending FIRST 5 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH VERSION CONTEXT) ====\n",
            "Original conversation size: 5 messages\n",
            "🔧 Applying conversation history compression...\n",
            "Initial token count: 14873\n",
            "🔧 Conversation size: 14873 tokens\n",
            "🔄 Applying compression/truncation...\n",
            "✅ Compressed: 14873 -> 14873 tokens\n",
            "Compressed token count: 14873\n",
            "Compression ratio: 100.00% of original size\n",
            "Messages after compression: 5\n",
            "Final GPT-4.1 request token estimate: 16503\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE (WITH VERSION CONTEXT) ====\n",
            "You are an expert Python analyst working with a rent roll dataset. The dataframe df is ALREADY LOADED and contains ALL 26 rows from the latest version (v_20250617_015006). This is the current and active version for all analysis. You have access to a representative sample of the data structure, but your code must operate on the entire dataframe.\n",
            "\n",
            "**Instructions for all code you generate:**\n",
            "\n",
            "- The dataframe variable is df and is already defined.\n",
            "- Do NOT reload or redefine df.\n",
            "- When displaying ta...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION (CURRENT) ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE (CURRENT VERSION) ====\n",
            "# Deleting Rows with \"Summary\" in the Residents Column\n",
            "\n",
            "I'll delete all rows where the \"Residents\" column has a value of \"Summary\" from the specified version v_20250617_015006. Here's my approach:\n",
            "\n",
            "1. First, I'll examine the current dataframe to identify rows with \"Summary\" in the Residents column\n",
            "2. Then I'll filter out these rows to create a new dataframe\n",
            "3. Finally, I'll save the new version with an appropriate description\n",
            "\n",
            "Let's start by examining the current dataframe to identify rows with ...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 2 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Display the current dataframe\n",
            "print_formatted_table(df, \"Current Dataframe (Before Deletion)\")\n",
            "\n",
            "# Count rows with \"Summary\" in the Residents column\n",
            "summary_count = df[df['Residents'] == 'Summary'].s...\n",
            "\n",
            "-- Code Block 2 --\n",
            "# Filter out rows where Residents is \"Summary\"\n",
            "df_filtered = df[df['Residents'] != 'Summary'].copy()\n",
            "\n",
            "# Display the filtered dataframe\n",
            "print_formatted_table(df_filtered, \"Filtered Dataframe (After Del...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE (CURRENT VERSION) ====\n",
            "\n",
            "--- Executing Code Block 1/2 ---\n",
            "\n",
            "Current Dataframe (Before Deletion)\n",
            "================================================================================\n",
            "   Tenant                                             Floor   Lease Start Date Lease Expiration  RSF       RSF%      Annual Rent  Monthly Rent Rent PSF Residents\n",
            "0                                                AT&T      LL  2024-09-01       2029-08-31          295.0  7.00e-04  1.01e+04        842.52     34.27    Occupied\n",
            "1                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         8472.0  2.06e-02  1.61e+05      13414.00        19    Occupied\n",
            "2                                 Care Tech Solutions      LL  2023-06-01       2028-05-31         1847.0  4.50e-03  2.31e+04       1923.96      12.5    Occupied\n",
            "3                            Clear Rate Communication      LL  2017-06-27       2026-08-26         5212.0  1.27e-02  7.69e+04       6406.42     14.75    Occupied\n",
            "4                                                Aras  1st Fl  2019-09-05       2028-03-31        16759.0  4.08e-02  4.65e+05      38755.19     27.75    Occupied\n",
            "5                                    Dickinson Wright  3rd Fl  2011-08-01       2029-07-31        67798.0  1.65e-01  1.92e+06     160285.77     28.37    Occupied\n",
            "6                                    Dickinson Wright  4th Fl  2016-06-01       2029-07-31        19310.0  4.70e-02  5.89e+05      49079.58      30.5    Occupied\n",
            "7                                    Dickinson Wright  4th Fl  2018-04-01       2029-07-31         8906.0  2.17e-02  2.72e+05      22636.08      30.5    Occupied\n",
            "8                            Clear Rate Communication  4th Fl  2017-06-27       2026-08-26        10989.0  2.67e-02  3.30e+05      27472.50        30    Occupied\n",
            "9                                    Horizon Global 2  4th Fl  2017-04-01       2027-10-31         8885.0  2.16e-02  2.67e+05      22212.50        30    Occupied\n",
            "10                                     Horizon Global  5th Fl  2016-06-01       2027-10-31        15257.0  3.71e-02  4.50e+05      37506.79      29.5    Occupied\n",
            "11                                     Dart Appraisal  5th Fl  2023-06-01       2028-05-31        11954.0  2.91e-02  3.17e+05      26398.42      26.5    Occupied\n",
            "12                                                BDO  6th Fl  2016-12-01       2027-04-30        21418.0  5.21e-02  6.64e+05      55329.83        31    Occupied\n",
            "13                                   Abraham and Rose      LL  2020-07-13       2030-07-31         5500.0  1.34e-02  1.21e+05      10083.33        22    Occupied\n",
            "14                                                USI  1st Fl  2021-09-20       2028-12-31        13199.0  3.21e-02  3.63e+05      30247.71      27.5    Occupied\n",
            "15                                 Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31         3146.0  7.70e-03  8.81e+04       7340.68      21.3    Occupied\n",
            "16                   Total Monthly & Annual Base Rent     NaN         NaN              NaN       218947.0  5.32e-01  6.12e+06     509935.28               Summary\n",
            "17  The annual rent does not include the increases...     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN     Summary\n",
            "18                             Available Vacant Space     NaN         NaN              NaN            NaN       NaN       NaN           NaN       NaN      Vacant\n",
            "19                                           Pavilion     NaN         NaN              NaN        14000.0  3.40e-02       NaN           NaN       NaN      Vacant\n",
            "20                                          Vacant LL     NaN         NaN              NaN        24950.0  6.07e-02       NaN           NaN       NaN      Vacant\n",
            "21                                      Vacant 1st Fl     NaN         NaN              NaN        35552.0  8.65e-02       NaN           NaN       NaN      Vacant\n",
            "22                                      Vacant 2nd Fl     NaN         NaN              NaN        86358.0  2.10e-01       NaN           NaN       NaN      Vacant\n",
            "23                                      Vacant 4th Fl     NaN         NaN              NaN         8750.0  2.13e-02       NaN           NaN       NaN      Vacant\n",
            "24                                      Vacant 5th Fl     NaN         NaN              NaN        22625.0  5.50e-02       NaN           NaN       NaN      Vacant\n",
            "25                               Total Building Sq ft     NaN         NaN              NaN       411182.0  1.00e+00       NaN           NaN       NaN     Summary\n",
            "\n",
            "Number of rows with 'Summary' in Residents column: 3\n",
            "\n",
            "✓ Code Block 1 executed successfully\n",
            "\n",
            "--- Executing Code Block 2/2 ---\n",
            "\n",
            "Filtered Dataframe (After Deleting Summary Rows)\n",
            "================================================================================\n",
            "   Tenant                    Floor   Lease Start Date Lease Expiration  RSF      RSF%      Annual Rent  Monthly Rent Rent PSF Residents\n",
            "0                       AT&T      LL  2024-09-01       2029-08-31         295.0  7.00e-04  1.01e+04        842.52     34.27    Occupied\n",
            "1        Care Tech Solutions      LL  2023-06-01       2028-05-31        8472.0  2.06e-02  1.61e+05      13414.00        19    Occupied\n",
            "2        Care Tech Solutions      LL  2023-06-01       2028-05-31        1847.0  4.50e-03  2.31e+04       1923.96      12.5    Occupied\n",
            "3   Clear Rate Communication      LL  2017-06-27       2026-08-26        5212.0  1.27e-02  7.69e+04       6406.42     14.75    Occupied\n",
            "4                       Aras  1st Fl  2019-09-05       2028-03-31       16759.0  4.08e-02  4.65e+05      38755.19     27.75    Occupied\n",
            "5           Dickinson Wright  3rd Fl  2011-08-01       2029-07-31       67798.0  1.65e-01  1.92e+06     160285.77     28.37    Occupied\n",
            "6           Dickinson Wright  4th Fl  2016-06-01       2029-07-31       19310.0  4.70e-02  5.89e+05      49079.58      30.5    Occupied\n",
            "7           Dickinson Wright  4th Fl  2018-04-01       2029-07-31        8906.0  2.17e-02  2.72e+05      22636.08      30.5    Occupied\n",
            "8   Clear Rate Communication  4th Fl  2017-06-27       2026-08-26       10989.0  2.67e-02  3.30e+05      27472.50        30    Occupied\n",
            "9           Horizon Global 2  4th Fl  2017-04-01       2027-10-31        8885.0  2.16e-02  2.67e+05      22212.50        30    Occupied\n",
            "10            Horizon Global  5th Fl  2016-06-01       2027-10-31       15257.0  3.71e-02  4.50e+05      37506.79      29.5    Occupied\n",
            "11            Dart Appraisal  5th Fl  2023-06-01       2028-05-31       11954.0  2.91e-02  3.17e+05      26398.42      26.5    Occupied\n",
            "12                       BDO  6th Fl  2016-12-01       2027-04-30       21418.0  5.21e-02  6.64e+05      55329.83        31    Occupied\n",
            "13          Abraham and Rose      LL  2020-07-13       2030-07-31        5500.0  1.34e-02  1.21e+05      10083.33        22    Occupied\n",
            "14                       USI  1st Fl  2021-09-20       2028-12-31       13199.0  3.21e-02  3.63e+05      30247.71      27.5    Occupied\n",
            "15        Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31        3146.0  7.70e-03  8.81e+04       7340.68      21.3    Occupied\n",
            "18    Available Vacant Space     NaN         NaN              NaN           NaN       NaN       NaN           NaN       NaN      Vacant\n",
            "19                  Pavilion     NaN         NaN              NaN       14000.0  3.40e-02       NaN           NaN       NaN      Vacant\n",
            "20                 Vacant LL     NaN         NaN              NaN       24950.0  6.07e-02       NaN           NaN       NaN      Vacant\n",
            "21             Vacant 1st Fl     NaN         NaN              NaN       35552.0  8.65e-02       NaN           NaN       NaN      Vacant\n",
            "22             Vacant 2nd Fl     NaN         NaN              NaN       86358.0  2.10e-01       NaN           NaN       NaN      Vacant\n",
            "23             Vacant 4th Fl     NaN         NaN              NaN        8750.0  2.13e-02       NaN           NaN       NaN      Vacant\n",
            "24             Vacant 5th Fl     NaN         NaN              NaN       22625.0  5.50e-02       NaN           NaN       NaN      Vacant\n",
            "\n",
            "Number of rows removed: 3\n",
            "✓ Saved dataframe version v_20250617_015217: Deleted all rows where Residents is Summary\n",
            "  - CSV: rent_roll_versions/rent_roll_v_20250617_015217.csv\n",
            "  - Excel: rent_roll_versions/rent_roll_v_20250617_015217.xlsx\n",
            "  - Shape: (23, 10)\n",
            "  - Registry updated: 5 total versions\n",
            "\n",
            "✓ Code Block 2 executed successfully\n",
            "\n",
            "==== STEP 4: VALIDATING CHANGES (CURRENT VERSION) ====\n",
            "🔍 Validating changes with GPT-4.1...\n",
            "📝 Logging validation session to: validation_logs/validation_20250617_015217_307.log\n",
            "🔍 Validation: PASS\n",
            "\n",
            "==== CODE GENERATION COMPLETE (CURRENT VERSION) ====\n",
            "🔧 Conversation size: 24840 tokens\n",
            "🔄 Applying compression/truncation...\n",
            "✅ Compressed: 24840 -> 13577 tokens\n",
            "Starting code generation with version support...\n",
            "\n",
            "==== STARTING CODE GENERATION WITH VERSION SUPPORT ====\n",
            "User query: \n",
            "USING SPECIFIC VERSION: v_20250617_015217\n",
            "==================================\n",
            "Version Description: Deleted all rows where Residents is Summary\n",
            "Version Created: 20250617_015217\n",
            "Version Shape: (23, 10)\n",
            "Current/Latest Version Shape: (30, 9)\n",
            "\n",
            "IMPORTANT: All analysis will be performed on version v_20250617_015217, NOT the current/latest version.\n",
            "\n",
            "Version Differences:\n",
            "- Row count difference: -7 rows\n",
            "- Column count difference: 1 columns\n",
            "- Version is the latest version\n",
            "\n",
            "Data from Version v_20250617_015217:\n",
            "                Tenant Floor Lease Start Date Lease Expiration     RSF    RSF%  Annual Rent  Monthly Rent  Rent PSF Residents\n",
            "0                 AT&T    LL       2024-09-01       2029-08-31   295.0  0.0007      10110.0        842.52     34.27  Occupied\n",
            "1  Care Tech Solutions    LL       2023-06-01       2028-05-31  8472.0  0.0206     160968.0      13414.00     19.00  Occupied\n",
            "2  Care Tech Solutions    LL       2023-06-01       2028-05-31  1847.0  0.0045      23088.0       1923.96     12.50  Occupied\n",
            "\n",
            "Columns removed since this version: ['Residents']\n",
            "\n",
            "use the table v_20250617_015217 and delete the row with the tenant name \"Available Vacant Space\".\n",
            "\n",
            "NOTE: Analysis will be performed on the specific version loaded above.\n",
            "Dataframe has 23 rows and 10 columns\n",
            "Using specific version: False\n",
            "Sending FIRST 5 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH VERSION CONTEXT) ====\n",
            "Original conversation size: 5 messages\n",
            "🔧 Applying conversation history compression...\n",
            "Initial token count: 13577\n",
            "🔧 Conversation size: 13577 tokens\n",
            "🔄 Applying compression/truncation...\n",
            "✅ Compressed: 13577 -> 13577 tokens\n",
            "Compressed token count: 13577\n",
            "Compression ratio: 100.00% of original size\n",
            "Messages after compression: 5\n",
            "Final GPT-4.1 request token estimate: 15229\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE (WITH VERSION CONTEXT) ====\n",
            "You are working with a rent roll dataframe that is ALREADY LOADED as the variable **df**. This dataframe contains ALL 23 rows from the current/latest version (v_20250617_015217), and its structure matches the sample provided below.\n",
            "\n",
            "**Key Instructions:**\n",
            "\n",
            "- The dataframe **df** is already defined and contains the full, current data.\n",
            "- Do NOT reload or clean the data unless specifically instructed.\n",
            "- When displaying tables, ALWAYS show ALL rows (do not truncate output).\n",
            "- After any analysis or mo...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION (CURRENT) ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE (CURRENT VERSION) ====\n",
            "# Deleting the Row with Tenant Name \"Available Vacant Space\"\n",
            "\n",
            "I'll delete the row where the \"Tenant\" column has the value \"Available Vacant Space\" from the specified version v_20250617_015217. Here's my approach:\n",
            "\n",
            "1. First, I'll examine the current dataframe to identify the row with \"Available Vacant Space\" in the Tenant column\n",
            "2. Then I'll filter out this row to create a new dataframe\n",
            "3. Finally, I'll save the new version with an appropriate description\n",
            "\n",
            "Let's start by examining the current dat...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 2 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Display the current dataframe\n",
            "print_formatted_table(df, \"Current Dataframe (Before Deletion)\")\n",
            "\n",
            "# Check if \"Available Vacant Space\" exists in the Tenant column\n",
            "available_vacant_space_exists = \"Avail...\n",
            "\n",
            "-- Code Block 2 --\n",
            "# Filter out the row where Tenant is \"Available Vacant Space\"\n",
            "df_filtered = df[df['Tenant'] != \"Available Vacant Space\"].copy()\n",
            "\n",
            "# Display the filtered dataframe\n",
            "print_formatted_table(df_filtered, \"Fi...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE (CURRENT VERSION) ====\n",
            "\n",
            "--- Executing Code Block 1/2 ---\n",
            "\n",
            "Current Dataframe (Before Deletion)\n",
            "================================================================================\n",
            "   Tenant                    Floor   Lease Start Date Lease Expiration  RSF      RSF%      Annual Rent  Monthly Rent  Rent PSF Residents\n",
            "0                       AT&T      LL  2024-09-01       2029-08-31         295.0  7.00e-04  1.01e+04        842.52     34.27     Occupied\n",
            "1        Care Tech Solutions      LL  2023-06-01       2028-05-31        8472.0  2.06e-02  1.61e+05      13414.00     19.00     Occupied\n",
            "2        Care Tech Solutions      LL  2023-06-01       2028-05-31        1847.0  4.50e-03  2.31e+04       1923.96     12.50     Occupied\n",
            "3   Clear Rate Communication      LL  2017-06-27       2026-08-26        5212.0  1.27e-02  7.69e+04       6406.42     14.75     Occupied\n",
            "4                       Aras  1st Fl  2019-09-05       2028-03-31       16759.0  4.08e-02  4.65e+05      38755.19     27.75     Occupied\n",
            "5           Dickinson Wright  3rd Fl  2011-08-01       2029-07-31       67798.0  1.65e-01  1.92e+06     160285.77     28.37     Occupied\n",
            "6           Dickinson Wright  4th Fl  2016-06-01       2029-07-31       19310.0  4.70e-02  5.89e+05      49079.58     30.50     Occupied\n",
            "7           Dickinson Wright  4th Fl  2018-04-01       2029-07-31        8906.0  2.17e-02  2.72e+05      22636.08     30.50     Occupied\n",
            "8   Clear Rate Communication  4th Fl  2017-06-27       2026-08-26       10989.0  2.67e-02  3.30e+05      27472.50     30.00     Occupied\n",
            "9           Horizon Global 2  4th Fl  2017-04-01       2027-10-31        8885.0  2.16e-02  2.67e+05      22212.50     30.00     Occupied\n",
            "10            Horizon Global  5th Fl  2016-06-01       2027-10-31       15257.0  3.71e-02  4.50e+05      37506.79     29.50     Occupied\n",
            "11            Dart Appraisal  5th Fl  2023-06-01       2028-05-31       11954.0  2.91e-02  3.17e+05      26398.42     26.50     Occupied\n",
            "12                       BDO  6th Fl  2016-12-01       2027-04-30       21418.0  5.21e-02  6.64e+05      55329.83     31.00     Occupied\n",
            "13          Abraham and Rose      LL  2020-07-13       2030-07-31        5500.0  1.34e-02  1.21e+05      10083.33     22.00     Occupied\n",
            "14                       USI  1st Fl  2021-09-20       2028-12-31       13199.0  3.21e-02  3.63e+05      30247.71     27.50     Occupied\n",
            "15        Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31        3146.0  7.70e-03  8.81e+04       7340.68     21.30     Occupied\n",
            "16    Available Vacant Space     NaN         NaN              NaN           NaN       NaN       NaN           NaN       NaN       Vacant\n",
            "17                  Pavilion     NaN         NaN              NaN       14000.0  3.40e-02       NaN           NaN       NaN       Vacant\n",
            "18                 Vacant LL     NaN         NaN              NaN       24950.0  6.07e-02       NaN           NaN       NaN       Vacant\n",
            "19             Vacant 1st Fl     NaN         NaN              NaN       35552.0  8.65e-02       NaN           NaN       NaN       Vacant\n",
            "20             Vacant 2nd Fl     NaN         NaN              NaN       86358.0  2.10e-01       NaN           NaN       NaN       Vacant\n",
            "21             Vacant 4th Fl     NaN         NaN              NaN        8750.0  2.13e-02       NaN           NaN       NaN       Vacant\n",
            "22             Vacant 5th Fl     NaN         NaN              NaN       22625.0  5.50e-02       NaN           NaN       NaN       Vacant\n",
            "\n",
            "'Available Vacant Space' exists in Tenant column: True\n",
            "Index of 'Available Vacant Space' row: Index([16], dtype='int64')\n",
            "\n",
            "✓ Code Block 1 executed successfully\n",
            "\n",
            "--- Executing Code Block 2/2 ---\n",
            "\n",
            "Filtered Dataframe (After Deleting 'Available Vacant Space' Row)\n",
            "================================================================================\n",
            "   Tenant                    Floor   Lease Start Date Lease Expiration  RSF      RSF%      Annual Rent  Monthly Rent  Rent PSF Residents\n",
            "0                       AT&T      LL  2024-09-01       2029-08-31         295.0  7.00e-04  1.01e+04        842.52     34.27     Occupied\n",
            "1        Care Tech Solutions      LL  2023-06-01       2028-05-31        8472.0  2.06e-02  1.61e+05      13414.00     19.00     Occupied\n",
            "2        Care Tech Solutions      LL  2023-06-01       2028-05-31        1847.0  4.50e-03  2.31e+04       1923.96     12.50     Occupied\n",
            "3   Clear Rate Communication      LL  2017-06-27       2026-08-26        5212.0  1.27e-02  7.69e+04       6406.42     14.75     Occupied\n",
            "4                       Aras  1st Fl  2019-09-05       2028-03-31       16759.0  4.08e-02  4.65e+05      38755.19     27.75     Occupied\n",
            "5           Dickinson Wright  3rd Fl  2011-08-01       2029-07-31       67798.0  1.65e-01  1.92e+06     160285.77     28.37     Occupied\n",
            "6           Dickinson Wright  4th Fl  2016-06-01       2029-07-31       19310.0  4.70e-02  5.89e+05      49079.58     30.50     Occupied\n",
            "7           Dickinson Wright  4th Fl  2018-04-01       2029-07-31        8906.0  2.17e-02  2.72e+05      22636.08     30.50     Occupied\n",
            "8   Clear Rate Communication  4th Fl  2017-06-27       2026-08-26       10989.0  2.67e-02  3.30e+05      27472.50     30.00     Occupied\n",
            "9           Horizon Global 2  4th Fl  2017-04-01       2027-10-31        8885.0  2.16e-02  2.67e+05      22212.50     30.00     Occupied\n",
            "10            Horizon Global  5th Fl  2016-06-01       2027-10-31       15257.0  3.71e-02  4.50e+05      37506.79     29.50     Occupied\n",
            "11            Dart Appraisal  5th Fl  2023-06-01       2028-05-31       11954.0  2.91e-02  3.17e+05      26398.42     26.50     Occupied\n",
            "12                       BDO  6th Fl  2016-12-01       2027-04-30       21418.0  5.21e-02  6.64e+05      55329.83     31.00     Occupied\n",
            "13          Abraham and Rose      LL  2020-07-13       2030-07-31        5500.0  1.34e-02  1.21e+05      10083.33     22.00     Occupied\n",
            "14                       USI  1st Fl  2021-09-20       2028-12-31       13199.0  3.21e-02  3.63e+05      30247.71     27.50     Occupied\n",
            "15        Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31        3146.0  7.70e-03  8.81e+04       7340.68     21.30     Occupied\n",
            "17                  Pavilion     NaN         NaN              NaN       14000.0  3.40e-02       NaN           NaN       NaN       Vacant\n",
            "18                 Vacant LL     NaN         NaN              NaN       24950.0  6.07e-02       NaN           NaN       NaN       Vacant\n",
            "19             Vacant 1st Fl     NaN         NaN              NaN       35552.0  8.65e-02       NaN           NaN       NaN       Vacant\n",
            "20             Vacant 2nd Fl     NaN         NaN              NaN       86358.0  2.10e-01       NaN           NaN       NaN       Vacant\n",
            "21             Vacant 4th Fl     NaN         NaN              NaN        8750.0  2.13e-02       NaN           NaN       NaN       Vacant\n",
            "22             Vacant 5th Fl     NaN         NaN              NaN       22625.0  5.50e-02       NaN           NaN       NaN       Vacant\n",
            "\n",
            "Number of rows removed: 1\n",
            "✓ Saved dataframe version v_20250617_015435: Deleted row with tenant name 'Available Vacant Space'\n",
            "  - CSV: rent_roll_versions/rent_roll_v_20250617_015435.csv\n",
            "  - Excel: rent_roll_versions/rent_roll_v_20250617_015435.xlsx\n",
            "  - Shape: (22, 10)\n",
            "  - Registry updated: 6 total versions\n",
            "\n",
            "✓ Code Block 2 executed successfully\n",
            "\n",
            "==== STEP 4: VALIDATING CHANGES (CURRENT VERSION) ====\n",
            "🔍 Validating changes with GPT-4.1...\n",
            "📝 Logging validation session to: validation_logs/validation_20250617_015435_229.log\n",
            "🔍 Validation: PASS\n",
            "\n",
            "==== CODE GENERATION COMPLETE (CURRENT VERSION) ====\n",
            "🔧 Conversation size: 31827 tokens\n",
            "🚨 Emergency truncation: 31827 -> keeping last 8 messages\n",
            "Starting code generation with version support...\n",
            "\n",
            "==== STARTING CODE GENERATION WITH VERSION SUPPORT ====\n",
            "User query: \n",
            "USING SPECIFIC VERSION: v_20250617_015435\n",
            "==================================\n",
            "Version Description: Deleted row with tenant name 'Available Vacant Space'\n",
            "Version Created: 20250617_015435\n",
            "Version Shape: (22, 10)\n",
            "Current/Latest Version Shape: (30, 9)\n",
            "\n",
            "IMPORTANT: All analysis will be performed on version v_20250617_015435, NOT the current/latest version.\n",
            "\n",
            "Version Differences:\n",
            "- Row count difference: -8 rows\n",
            "- Column count difference: 1 columns\n",
            "- Version is the latest version\n",
            "\n",
            "Data from Version v_20250617_015435:\n",
            "                Tenant Floor Lease Start Date Lease Expiration     RSF    RSF%  Annual Rent  Monthly Rent  Rent PSF Residents\n",
            "0                 AT&T    LL       2024-09-01       2029-08-31   295.0  0.0007      10110.0        842.52     34.27  Occupied\n",
            "1  Care Tech Solutions    LL       2023-06-01       2028-05-31  8472.0  0.0206     160968.0      13414.00     19.00  Occupied\n",
            "2  Care Tech Solutions    LL       2023-06-01       2028-05-31  1847.0  0.0045      23088.0       1923.96     12.50  Occupied\n",
            "\n",
            "Columns removed since this version: ['Residents']\n",
            "\n",
            "Update the rent roll dataset version v_20250617_015435 by identifying all vacant tenant spaces (where the \"Residents\" column contains \"Vacant\") and systematically populating their financial fields with zero values specifically in the \"Annual Rent,\" \"Monthly Rent,\" and \"Rent PSF\" columns.\n",
            "\n",
            "NOTE: Analysis will be performed on the specific version loaded above.\n",
            "Dataframe has 22 rows and 10 columns\n",
            "Using specific version: False\n",
            "Sending FIRST 5 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH VERSION CONTEXT) ====\n",
            "Original conversation size: 8 messages\n",
            "🔧 Applying conversation history compression...\n",
            "Initial token count: 28216\n",
            "🔧 Conversation size: 28216 tokens\n",
            "🚨 Emergency truncation: 28216 -> keeping last 8 messages\n",
            "Compressed token count: 28216\n",
            "Compression ratio: 100.00% of original size\n",
            "Messages after compression: 8\n",
            "Final GPT-4.1 request token estimate: 29890\n",
            "⚠️ Still approaching token limit, applying emergency truncation...\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE (WITH VERSION CONTEXT) ====\n",
            "You are an expert Python analyst working with a rent roll dataset. The dataframe df is ALREADY LOADED and contains ALL 22 rows from the current/latest version (v_20250617_015435). You have access to a representative sample of the data structure (see below), but your code should always operate on the full dataframe as currently loaded in df.\n",
            "\n",
            "**Key instructions:**\n",
            "\n",
            "- The dataframe variable is df and is already defined.\n",
            "- Do NOT reload or re-import the data.\n",
            "- Do NOT perform any data cleaning unle...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION (CURRENT) ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE (CURRENT VERSION) ====\n",
            "# Updating Vacant Tenant Spaces with Zero Financial Values\n",
            "\n",
            "I'll identify all vacant tenant spaces in the rent roll dataset (version v_20250617_015435) and update their financial fields with zero values. Specifically, I'll set \"Annual Rent,\" \"Monthly Rent,\" and \"Rent PSF\" to 0.0 for all rows where the \"Residents\" column contains \"Vacant\".\n",
            "\n",
            "Here's my approach:\n",
            "\n",
            "1. First, I'll examine the current dataframe to identify vacant tenant spaces\n",
            "2. Then I'll update the financial fields for these vacant s...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 2 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Display the current dataframe\n",
            "print_formatted_table(df, \"Current Dataframe (Before Update)\")\n",
            "\n",
            "# Identify vacant tenant spaces\n",
            "vacant_spaces = df[df['Residents'] == 'Vacant']\n",
            "print_formatted_table(va...\n",
            "\n",
            "-- Code Block 2 --\n",
            "# Update financial fields for vacant tenant spaces\n",
            "# Set \"Annual Rent,\" \"Monthly Rent,\" and \"Rent PSF\" to 0.0\n",
            "df.loc[df['Residents'] == 'Vacant', ['Annual Rent', 'Monthly Rent', 'Rent PSF']] = 0.0\n",
            "\n",
            "# ...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE (CURRENT VERSION) ====\n",
            "\n",
            "--- Executing Code Block 1/2 ---\n",
            "\n",
            "Current Dataframe (Before Update)\n",
            "================================================================================\n",
            "   Tenant                    Floor   Lease Start Date Lease Expiration  RSF      RSF%      Annual Rent  Monthly Rent  Rent PSF Residents\n",
            "0                       AT&T      LL  2024-09-01       2029-08-31         295.0  7.00e-04  1.01e+04        842.52     34.27     Occupied\n",
            "1        Care Tech Solutions      LL  2023-06-01       2028-05-31        8472.0  2.06e-02  1.61e+05      13414.00     19.00     Occupied\n",
            "2        Care Tech Solutions      LL  2023-06-01       2028-05-31        1847.0  4.50e-03  2.31e+04       1923.96     12.50     Occupied\n",
            "3   Clear Rate Communication      LL  2017-06-27       2026-08-26        5212.0  1.27e-02  7.69e+04       6406.42     14.75     Occupied\n",
            "4                       Aras  1st Fl  2019-09-05       2028-03-31       16759.0  4.08e-02  4.65e+05      38755.19     27.75     Occupied\n",
            "5           Dickinson Wright  3rd Fl  2011-08-01       2029-07-31       67798.0  1.65e-01  1.92e+06     160285.77     28.37     Occupied\n",
            "6           Dickinson Wright  4th Fl  2016-06-01       2029-07-31       19310.0  4.70e-02  5.89e+05      49079.58     30.50     Occupied\n",
            "7           Dickinson Wright  4th Fl  2018-04-01       2029-07-31        8906.0  2.17e-02  2.72e+05      22636.08     30.50     Occupied\n",
            "8   Clear Rate Communication  4th Fl  2017-06-27       2026-08-26       10989.0  2.67e-02  3.30e+05      27472.50     30.00     Occupied\n",
            "9           Horizon Global 2  4th Fl  2017-04-01       2027-10-31        8885.0  2.16e-02  2.67e+05      22212.50     30.00     Occupied\n",
            "10            Horizon Global  5th Fl  2016-06-01       2027-10-31       15257.0  3.71e-02  4.50e+05      37506.79     29.50     Occupied\n",
            "11            Dart Appraisal  5th Fl  2023-06-01       2028-05-31       11954.0  2.91e-02  3.17e+05      26398.42     26.50     Occupied\n",
            "12                       BDO  6th Fl  2016-12-01       2027-04-30       21418.0  5.21e-02  6.64e+05      55329.83     31.00     Occupied\n",
            "13          Abraham and Rose      LL  2020-07-13       2030-07-31        5500.0  1.34e-02  1.21e+05      10083.33     22.00     Occupied\n",
            "14                       USI  1st Fl  2021-09-20       2028-12-31       13199.0  3.21e-02  3.63e+05      30247.71     27.50     Occupied\n",
            "15        Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31        3146.0  7.70e-03  8.81e+04       7340.68     21.30     Occupied\n",
            "16                  Pavilion     NaN         NaN              NaN       14000.0  3.40e-02       NaN           NaN       NaN       Vacant\n",
            "17                 Vacant LL     NaN         NaN              NaN       24950.0  6.07e-02       NaN           NaN       NaN       Vacant\n",
            "18             Vacant 1st Fl     NaN         NaN              NaN       35552.0  8.65e-02       NaN           NaN       NaN       Vacant\n",
            "19             Vacant 2nd Fl     NaN         NaN              NaN       86358.0  2.10e-01       NaN           NaN       NaN       Vacant\n",
            "20             Vacant 4th Fl     NaN         NaN              NaN        8750.0  2.13e-02       NaN           NaN       NaN       Vacant\n",
            "21             Vacant 5th Fl     NaN         NaN              NaN       22625.0  5.50e-02       NaN           NaN       NaN       Vacant\n",
            "\n",
            "Vacant Tenant Spaces\n",
            "================================================================================\n",
            "   Tenant         Floor Lease Start Date Lease Expiration  RSF      RSF%  Annual Rent  Monthly Rent  Rent PSF Residents\n",
            "16       Pavilion  NaN   NaN              NaN              14000.0  0.03 NaN          NaN           NaN        Vacant  \n",
            "17      Vacant LL  NaN   NaN              NaN              24950.0  0.06 NaN          NaN           NaN        Vacant  \n",
            "18  Vacant 1st Fl  NaN   NaN              NaN              35552.0  0.09 NaN          NaN           NaN        Vacant  \n",
            "19  Vacant 2nd Fl  NaN   NaN              NaN              86358.0  0.21 NaN          NaN           NaN        Vacant  \n",
            "20  Vacant 4th Fl  NaN   NaN              NaN               8750.0  0.02 NaN          NaN           NaN        Vacant  \n",
            "21  Vacant 5th Fl  NaN   NaN              NaN              22625.0  0.06 NaN          NaN           NaN        Vacant  \n",
            "\n",
            "Number of vacant tenant spaces: 6\n",
            "\n",
            "✓ Code Block 1 executed successfully\n",
            "\n",
            "--- Executing Code Block 2/2 ---\n",
            "\n",
            "Updated Dataframe (After Setting Financial Fields to Zero for Vacant Spaces)\n",
            "================================================================================\n",
            "   Tenant                    Floor   Lease Start Date Lease Expiration  RSF      RSF%      Annual Rent  Monthly Rent  Rent PSF Residents\n",
            "0                       AT&T      LL  2024-09-01       2029-08-31         295.0  7.00e-04  1.01e+04        842.52     34.27     Occupied\n",
            "1        Care Tech Solutions      LL  2023-06-01       2028-05-31        8472.0  2.06e-02  1.61e+05      13414.00     19.00     Occupied\n",
            "2        Care Tech Solutions      LL  2023-06-01       2028-05-31        1847.0  4.50e-03  2.31e+04       1923.96     12.50     Occupied\n",
            "3   Clear Rate Communication      LL  2017-06-27       2026-08-26        5212.0  1.27e-02  7.69e+04       6406.42     14.75     Occupied\n",
            "4                       Aras  1st Fl  2019-09-05       2028-03-31       16759.0  4.08e-02  4.65e+05      38755.19     27.75     Occupied\n",
            "5           Dickinson Wright  3rd Fl  2011-08-01       2029-07-31       67798.0  1.65e-01  1.92e+06     160285.77     28.37     Occupied\n",
            "6           Dickinson Wright  4th Fl  2016-06-01       2029-07-31       19310.0  4.70e-02  5.89e+05      49079.58     30.50     Occupied\n",
            "7           Dickinson Wright  4th Fl  2018-04-01       2029-07-31        8906.0  2.17e-02  2.72e+05      22636.08     30.50     Occupied\n",
            "8   Clear Rate Communication  4th Fl  2017-06-27       2026-08-26       10989.0  2.67e-02  3.30e+05      27472.50     30.00     Occupied\n",
            "9           Horizon Global 2  4th Fl  2017-04-01       2027-10-31        8885.0  2.16e-02  2.67e+05      22212.50     30.00     Occupied\n",
            "10            Horizon Global  5th Fl  2016-06-01       2027-10-31       15257.0  3.71e-02  4.50e+05      37506.79     29.50     Occupied\n",
            "11            Dart Appraisal  5th Fl  2023-06-01       2028-05-31       11954.0  2.91e-02  3.17e+05      26398.42     26.50     Occupied\n",
            "12                       BDO  6th Fl  2016-12-01       2027-04-30       21418.0  5.21e-02  6.64e+05      55329.83     31.00     Occupied\n",
            "13          Abraham and Rose      LL  2020-07-13       2030-07-31        5500.0  1.34e-02  1.21e+05      10083.33     22.00     Occupied\n",
            "14                       USI  1st Fl  2021-09-20       2028-12-31       13199.0  3.21e-02  3.63e+05      30247.71     27.50     Occupied\n",
            "15        Power Mortgage LLC  5th Fl  2023-06-01       2028-07-31        3146.0  7.70e-03  8.81e+04       7340.68     21.30     Occupied\n",
            "16                  Pavilion     NaN         NaN              NaN       14000.0  3.40e-02  0.00e+00          0.00      0.00       Vacant\n",
            "17                 Vacant LL     NaN         NaN              NaN       24950.0  6.07e-02  0.00e+00          0.00      0.00       Vacant\n",
            "18             Vacant 1st Fl     NaN         NaN              NaN       35552.0  8.65e-02  0.00e+00          0.00      0.00       Vacant\n",
            "19             Vacant 2nd Fl     NaN         NaN              NaN       86358.0  2.10e-01  0.00e+00          0.00      0.00       Vacant\n",
            "20             Vacant 4th Fl     NaN         NaN              NaN        8750.0  2.13e-02  0.00e+00          0.00      0.00       Vacant\n",
            "21             Vacant 5th Fl     NaN         NaN              NaN       22625.0  5.50e-02  0.00e+00          0.00      0.00       Vacant\n",
            "\n",
            "Updated Vacant Tenant Spaces\n",
            "================================================================================\n",
            "   Tenant         Floor Lease Start Date Lease Expiration  RSF      RSF%  Annual Rent  Monthly Rent  Rent PSF Residents\n",
            "16       Pavilion  NaN   NaN              NaN              14000.0  0.03  0.0          0.0           0.0       Vacant  \n",
            "17      Vacant LL  NaN   NaN              NaN              24950.0  0.06  0.0          0.0           0.0       Vacant  \n",
            "18  Vacant 1st Fl  NaN   NaN              NaN              35552.0  0.09  0.0          0.0           0.0       Vacant  \n",
            "19  Vacant 2nd Fl  NaN   NaN              NaN              86358.0  0.21  0.0          0.0           0.0       Vacant  \n",
            "20  Vacant 4th Fl  NaN   NaN              NaN               8750.0  0.02  0.0          0.0           0.0       Vacant  \n",
            "21  Vacant 5th Fl  NaN   NaN              NaN              22625.0  0.06  0.0          0.0           0.0       Vacant  \n",
            "✓ Saved dataframe version v_20250617_015750: Set financial fields to zero for vacant tenant spaces\n",
            "  - CSV: rent_roll_versions/rent_roll_v_20250617_015750.csv\n",
            "  - Excel: rent_roll_versions/rent_roll_v_20250617_015750.xlsx\n",
            "  - Shape: (22, 10)\n",
            "  - Registry updated: 7 total versions\n",
            "\n",
            "✓ Code Block 2 executed successfully\n",
            "\n",
            "==== STEP 4: VALIDATING CHANGES (CURRENT VERSION) ====\n",
            "🔍 Validating changes with GPT-4.1...\n",
            "📝 Logging validation session to: validation_logs/validation_20250617_015750_928.log\n",
            "🔍 Validation: PASS\n",
            "\n",
            "==== CODE GENERATION COMPLETE (CURRENT VERSION) ====\n",
            "✅ Session recording finalized: session_20250617_014343\n",
            "🤖 Analyzing session with GPT-4.1 to generate instructions...\n",
            "✅ Comprehensive template created: template_20250617_015838\n",
            "📁 Starting DF: rent_roll_templates/template_20250617_015838_starting_df.txt\n",
            "📁 Final DF: rent_roll_templates/template_20250617_015838_final_df.txt\n",
            "📁 Session Data: rent_roll_templates/template_20250617_015838_session.txt\n",
            "📋 Template: rent_roll_templates/template_20250617_015838.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:rent_roll_analyzer:Could not find header row with 'Current' marker. Falling back to standard loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading rent roll with specialized loader for enhanced template application...\n",
            "💾 Saved version: step_0_version_20250617_015856.csv (Shape: (32, 9))\n",
            "💾 Saved state for step 0: (32, 9)\n",
            "🔗 Loaded latest dataframe from: step_0_version_20250617_015856.csv\n",
            "🔗 Loaded latest dataframe from: step_0_version_20250617_015856.csv\n",
            "\n",
            "🚀 EXECUTING STEP 1 WITH USER INPUT\n",
            "📝 Original step: \n",
            "USING SPECIFIC VERSION: v_20250617_014250\n",
            "==================================\n",
            "Version Description: I...\n",
            "🔗 Loaded latest dataframe from: step_0_version_20250617_015856.csv\n",
            "🔗 Loaded latest dataframe from: step_0_version_20250617_015856.csv\n",
            "🔗 STEP 1 LOADING FROM VERSION FILE: step_0_version_20250617_015856.csv\n",
            "📊 Loaded DataFrame Shape: (32, 9)\n",
            "✅ Step 1 succeeded!\n",
            "💾 Saved state for step 1: (32, 9)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_1_version_20250617_020016.csv\n",
            "🔗 Loaded latest dataframe from: step_1_version_20250617_020016.csv\n",
            "\n",
            "🚀 EXECUTING STEP 2 WITH USER INPUT\n",
            "📝 Original step: LOAD FOR EDITING: User opened data editor...\n",
            "🔗 Loaded latest dataframe from: step_1_version_20250617_020016.csv\n",
            "🔗 Loaded latest dataframe from: step_1_version_20250617_020016.csv\n",
            "🔗 STEP 2 LOADING FROM VERSION FILE: step_1_version_20250617_020016.csv\n",
            "📊 Loaded DataFrame Shape: (28, 9)\n",
            "✅ Step 2 succeeded!\n",
            "💾 Saved state for step 2: (28, 9)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_2_version_20250617_020159.csv\n",
            "🔗 Loaded latest dataframe from: step_2_version_20250617_020159.csv\n",
            "\n",
            "🚀 EXECUTING STEP 3 WITH USER INPUT\n",
            "📝 Original step: LOAD SPECIFIC VERSION: v_20250617_014408...\n",
            "🔗 Loaded latest dataframe from: step_2_version_20250617_020159.csv\n",
            "🔗 Loaded latest dataframe from: step_2_version_20250617_020159.csv\n",
            "🔗 STEP 3 LOADING FROM VERSION FILE: step_2_version_20250617_020159.csv\n",
            "📊 Loaded DataFrame Shape: (28, 9)\n",
            "✅ Step 3 succeeded!\n",
            "💾 Saved state for step 3: (28, 9)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_3_version_20250617_020234.csv\n",
            "🔗 Loaded latest dataframe from: step_3_version_20250617_020234.csv\n",
            "\n",
            "🚀 EXECUTING STEP 4 WITH USER INPUT\n",
            "📝 Original step: \n",
            "USING SPECIFIC VERSION: v_20250617_014408\n",
            "==================================\n",
            "Version Description: R...\n",
            "🔗 Loaded latest dataframe from: step_3_version_20250617_020234.csv\n",
            "🔗 Loaded latest dataframe from: step_3_version_20250617_020234.csv\n",
            "🔗 STEP 4 LOADING FROM VERSION FILE: step_3_version_20250617_020234.csv\n",
            "📊 Loaded DataFrame Shape: (28, 9)\n",
            "✅ Step 4 succeeded!\n",
            "💾 Saved state for step 4: (28, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_4_version_20250617_020304.csv\n",
            "🔗 Loaded latest dataframe from: step_4_version_20250617_020304.csv\n",
            "\n",
            "🚀 EXECUTING STEP 5 WITH USER INPUT\n",
            "📝 Original step: LOAD SPECIFIC VERSION: v_20250617_014746...\n",
            "🔗 Loaded latest dataframe from: step_4_version_20250617_020304.csv\n",
            "🔗 Loaded latest dataframe from: step_4_version_20250617_020304.csv\n",
            "🔗 STEP 5 LOADING FROM VERSION FILE: step_4_version_20250617_020304.csv\n",
            "📊 Loaded DataFrame Shape: (28, 10)\n",
            "✅ Step 5 succeeded!\n",
            "💾 Saved state for step 5: (28, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_5_version_20250617_020345.csv\n",
            "🔗 Loaded latest dataframe from: step_5_version_20250617_020345.csv\n",
            "\n",
            "🚀 EXECUTING STEP 6 WITH USER INPUT\n",
            "📝 Original step: \n",
            "USING SPECIFIC VERSION: v_20250617_014746\n",
            "==================================\n",
            "Version Description: A...\n",
            "🔗 Loaded latest dataframe from: step_5_version_20250617_020345.csv\n",
            "🔗 Loaded latest dataframe from: step_5_version_20250617_020345.csv\n",
            "🔗 STEP 6 LOADING FROM VERSION FILE: step_5_version_20250617_020345.csv\n",
            "📊 Loaded DataFrame Shape: (28, 10)\n",
            "✅ Step 6 succeeded!\n",
            "💾 Saved state for step 6: (28, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_6_version_20250617_020410.csv\n",
            "🔗 Loaded latest dataframe from: step_6_version_20250617_020410.csv\n",
            "\n",
            "🚀 EXECUTING STEP 7 WITH USER INPUT\n",
            "📝 Original step: LOAD SPECIFIC VERSION: v_20250617_015006...\n",
            "🔗 Loaded latest dataframe from: step_6_version_20250617_020410.csv\n",
            "🔗 Loaded latest dataframe from: step_6_version_20250617_020410.csv\n",
            "🔗 STEP 7 LOADING FROM VERSION FILE: step_6_version_20250617_020410.csv\n",
            "📊 Loaded DataFrame Shape: (28, 10)\n",
            "✅ Step 7 succeeded!\n",
            "💾 Saved state for step 7: (28, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_7_version_20250617_020441.csv\n",
            "🔗 Loaded latest dataframe from: step_7_version_20250617_020441.csv\n",
            "\n",
            "🚀 EXECUTING STEP 8 WITH USER INPUT\n",
            "📝 Original step: \n",
            "USING SPECIFIC VERSION: v_20250617_015006\n",
            "==================================\n",
            "Version Description: C...\n",
            "🔗 Loaded latest dataframe from: step_7_version_20250617_020441.csv\n",
            "🔗 Loaded latest dataframe from: step_7_version_20250617_020441.csv\n",
            "🔗 STEP 8 LOADING FROM VERSION FILE: step_7_version_20250617_020441.csv\n",
            "📊 Loaded DataFrame Shape: (28, 10)\n",
            "✅ Step 8 succeeded!\n",
            "💾 Saved state for step 8: (25, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_8_version_20250617_020500.csv\n",
            "🔗 Loaded latest dataframe from: step_8_version_20250617_020500.csv\n",
            "\n",
            "🚀 EXECUTING STEP 9 WITH USER INPUT\n",
            "📝 Original step: LOAD SPECIFIC VERSION: v_20250617_015217...\n",
            "🔗 Loaded latest dataframe from: step_8_version_20250617_020500.csv\n",
            "🔗 Loaded latest dataframe from: step_8_version_20250617_020500.csv\n",
            "🔗 STEP 9 LOADING FROM VERSION FILE: step_8_version_20250617_020500.csv\n",
            "📊 Loaded DataFrame Shape: (25, 10)\n",
            "✅ Step 9 succeeded!\n",
            "💾 Saved state for step 9: (25, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_9_version_20250617_020622.csv\n",
            "🔗 Loaded latest dataframe from: step_9_version_20250617_020622.csv\n",
            "\n",
            "🚀 EXECUTING STEP 10 WITH USER INPUT\n",
            "📝 Original step: \n",
            "USING SPECIFIC VERSION: v_20250617_015217\n",
            "==================================\n",
            "Version Description: D...\n",
            "🔗 Loaded latest dataframe from: step_9_version_20250617_020622.csv\n",
            "🔗 Loaded latest dataframe from: step_9_version_20250617_020622.csv\n",
            "🔗 STEP 10 LOADING FROM VERSION FILE: step_9_version_20250617_020622.csv\n",
            "📊 Loaded DataFrame Shape: (25, 10)\n",
            "✅ Step 10 succeeded!\n",
            "💾 Saved state for step 10: (24, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_10_version_20250617_020640.csv\n",
            "🔗 Loaded latest dataframe from: step_10_version_20250617_020640.csv\n",
            "\n",
            "🚀 EXECUTING STEP 11 WITH USER INPUT\n",
            "📝 Original step: LOAD SPECIFIC VERSION: v_20250617_015435...\n",
            "🔗 Loaded latest dataframe from: step_10_version_20250617_020640.csv\n",
            "🔗 Loaded latest dataframe from: step_10_version_20250617_020640.csv\n",
            "🔗 STEP 11 LOADING FROM VERSION FILE: step_10_version_20250617_020640.csv\n",
            "📊 Loaded DataFrame Shape: (24, 10)\n",
            "✅ Step 11 succeeded!\n",
            "💾 Saved state for step 11: (24, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_11_version_20250617_020801.csv\n",
            "🔗 Loaded latest dataframe from: step_11_version_20250617_020801.csv\n",
            "\n",
            "🚀 EXECUTING STEP 12 WITH USER INPUT\n",
            "📝 Original step: \n",
            "USING SPECIFIC VERSION: v_20250617_015435\n",
            "==================================\n",
            "Version Description: D...\n",
            "🔗 Loaded latest dataframe from: step_11_version_20250617_020801.csv\n",
            "🔗 Loaded latest dataframe from: step_11_version_20250617_020801.csv\n",
            "🔗 STEP 12 LOADING FROM VERSION FILE: step_11_version_20250617_020801.csv\n",
            "📊 Loaded DataFrame Shape: (24, 10)\n",
            "✅ Step 12 succeeded!\n",
            "💾 Saved state for step 12: (24, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n",
            "🔗 Loaded latest dataframe from: step_12_version_20250617_020820.csv\n",
            "🔗 Loaded latest dataframe from: step_12_version_20250617_020820.csv\n",
            "\n",
            "🚀 EXECUTING STEP 13 WITH USER INPUT\n",
            "📝 Original step: LOAD SPECIFIC VERSION: v_20250617_015750...\n",
            "🔗 Loaded latest dataframe from: step_12_version_20250617_020820.csv\n",
            "🔗 Loaded latest dataframe from: step_12_version_20250617_020820.csv\n",
            "🔗 STEP 13 LOADING FROM VERSION FILE: step_12_version_20250617_020820.csv\n",
            "📊 Loaded DataFrame Shape: (24, 10)\n",
            "✅ Step 13 succeeded!\n",
            "💾 Saved state for step 13: (24, 10)\n",
            "💾 Application state saved: template_applications/app_20250617_015856_state.json\n"
          ]
        }
      ],
      "source": [
        "# Initialize the global agent state\n",
        "agent_state = None\n",
        "custom_css = \"\"\"\n",
        ".chatbot-container .message-wrap .message.bot pre {\n",
        "    white-space: pre !important;\n",
        "    overflow-x: auto !important;\n",
        "    max-width: 100% !important;\n",
        "}\n",
        ".chatbot-container .message-wrap .message.bot code {\n",
        "    white-space: pre !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def chat_with_selected_message(message, history, selected_message_content=None, selected_message_index=None):\n",
        "    \"\"\"\n",
        "    Enhanced chat function that includes a selected previous message as context\n",
        "    and loads specific dataframe versions when mentioned in the prompt\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder, conversation_manager\n",
        "\n",
        "    logger.info(f\"Received chat message with selected context: {message[:50]}...\")\n",
        "\n",
        "    if selected_message_content:\n",
        "        logger.info(f\"Using selected message as context: {selected_message_content[:100]}...\")\n",
        "\n",
        "    # Check if system is ready\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        logger.warning(\"Chat attempted before setup is complete\")\n",
        "        return history + [(message, \"Please upload a rent roll file and set up your API keys first.\")]\n",
        "\n",
        "    # NEW: Detect and load specific version if mentioned\n",
        "    version_specific_df = None\n",
        "    version_context = \"\"\n",
        "    version_loaded = False\n",
        "\n",
        "    version_pattern = r'v_\\d{8}_\\d{6}'\n",
        "    version_match = re.search(version_pattern, message)\n",
        "\n",
        "    if version_match:\n",
        "        requested_version = version_match.group(0)\n",
        "        logger.info(f\"User requested specific version: {requested_version}\")\n",
        "\n",
        "        # Load the specific version\n",
        "        versions_dir = \"rent_roll_versions\"\n",
        "        version_file = os.path.join(versions_dir, f\"rent_roll_{requested_version}.csv\")\n",
        "\n",
        "        if os.path.exists(version_file):\n",
        "            try:\n",
        "                version_specific_df = pd.read_csv(version_file)\n",
        "                version_loaded = True\n",
        "                logger.info(f\"Successfully loaded version {requested_version} with shape {version_specific_df.shape}\")\n",
        "\n",
        "                # Find version metadata\n",
        "                version_info = None\n",
        "                for v in app_state.get(\"df_versions\", []):\n",
        "                    if v.get(\"name\") == requested_version:\n",
        "                        version_info = v\n",
        "                        break\n",
        "\n",
        "                # Create detailed version context\n",
        "                version_context = f\"\"\"\n",
        "USING SPECIFIC VERSION: {requested_version}\n",
        "==================================\n",
        "Version Description: {version_info.get('description', 'No description available') if version_info else 'Version not found in registry'}\n",
        "Version Created: {version_info.get('timestamp', 'Unknown') if version_info else 'Unknown'}\n",
        "Version Shape: {version_specific_df.shape}\n",
        "Current/Latest Version Shape: {app_state[\"df\"].shape}\n",
        "\n",
        "IMPORTANT: All analysis will be performed on version {requested_version}, NOT the current/latest version.\n",
        "\n",
        "Version Differences:\n",
        "- Row count difference: {version_specific_df.shape[0] - app_state[\"df\"].shape[0]} rows\n",
        "- Column count difference: {version_specific_df.shape[1] - app_state[\"df\"].shape[1]} columns\n",
        "- Version is {'older' if version_info and app_state.get('df_versions', [])[-1]['name'] != requested_version else 'the latest'} version\n",
        "\n",
        "Data from Version {requested_version}:\n",
        "{version_specific_df.head(3).to_string()}\n",
        "\"\"\"\n",
        "\n",
        "                # Check if columns differ\n",
        "                current_cols = set(app_state[\"df\"].columns)\n",
        "                version_cols = set(version_specific_df.columns)\n",
        "\n",
        "                if current_cols != version_cols:\n",
        "                    added_cols = current_cols - version_cols\n",
        "                    removed_cols = version_cols - current_cols\n",
        "\n",
        "                    if added_cols:\n",
        "                        version_context += f\"\\nColumns added since this version: {list(added_cols)}\"\n",
        "                    if removed_cols:\n",
        "                        version_context += f\"\\nColumns removed since this version: {list(removed_cols)}\"\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to load version {requested_version}: {e}\")\n",
        "                version_specific_df = None\n",
        "                version_context = f\"\"\"\n",
        "VERSION LOADING ERROR:\n",
        "=====================\n",
        "Requested version '{requested_version}' could not be loaded.\n",
        "Error: {str(e)}\n",
        "Using current dataframe instead.\n",
        "\"\"\"\n",
        "        else:\n",
        "            logger.warning(f\"Version file not found: {version_file}\")\n",
        "            available_versions = [v['name'] for v in app_state.get('df_versions', [])][:5]\n",
        "            version_context = f\"\"\"\n",
        "VERSION NOT FOUND:\n",
        "==================\n",
        "Requested version '{requested_version}' file not found at: {version_file}\n",
        "Using current dataframe instead.\n",
        "Available versions: {available_versions}...\n",
        "\"\"\"\n",
        "\n",
        "    # Start session recording if not already started\n",
        "    if not session_recorder.current_session_file:\n",
        "        rent_roll_filename = getattr(app_state, 'original_filename', 'uploaded_rent_roll.xlsx')\n",
        "        session_id = session_recorder.start_session_recording(rent_roll_filename)\n",
        "        logger.info(f\"Started new session recording: {session_id}\")\n",
        "\n",
        "        # Record initial dataframe state\n",
        "        if app_state.get(\"df_versions\") and len(app_state[\"df_versions\"]) > 0:\n",
        "            first_version = app_state[\"df_versions\"][0]\n",
        "            session_recorder.record_dataframe_version(\n",
        "                version_name=first_version[\"name\"],\n",
        "                description=first_version[\"description\"],\n",
        "                shape=list(app_state[\"df\"].shape),\n",
        "                columns=list(app_state[\"df\"].columns)\n",
        "            )\n",
        "\n",
        "    # Get previous messages from history\n",
        "    prev_messages = []\n",
        "    if history:\n",
        "        for user_msg, assistant_msg in history:\n",
        "            prev_messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            prev_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "    # Create message list\n",
        "    all_messages = []\n",
        "    all_messages.extend(prev_messages)\n",
        "\n",
        "    # Enhanced message with selected context if provided\n",
        "    if selected_message_content and selected_message_content.strip():\n",
        "        enhanced_message = f\"\"\"CONTEXT FROM PREVIOUS ASSISTANT MESSAGE:\n",
        "{selected_message_content}\n",
        "\n",
        "USER'S FOLLOW-UP QUERY:\n",
        "{message}\n",
        "\n",
        "Please continue the analysis building on the context provided above.\"\"\"\n",
        "\n",
        "        # Record the context usage in session\n",
        "        if session_recorder.current_session_file:\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] USER SELECTED PREVIOUS MESSAGE FOR CONTEXT\\n\")\n",
        "                f.write(f\"Selected Message (first 200 chars): {selected_message_content[:200]}...\\n\")\n",
        "                f.write(f\"User's follow-up query: {message}\\n\")\n",
        "                f.write(\"-\" * 60 + \"\\n\")\n",
        "    else:\n",
        "        enhanced_message = message\n",
        "\n",
        "    # Add version context to the message if a specific version was requested\n",
        "    if version_context:\n",
        "        enhanced_message = f\"\"\"{version_context}\n",
        "\n",
        "{enhanced_message}\n",
        "\n",
        "{'NOTE: Analysis will be performed on the specific version loaded above.' if version_loaded else 'NOTE: Requested version could not be loaded, using current data.'}\"\"\"\n",
        "\n",
        "        # Record version usage in session\n",
        "        if session_recorder.current_session_file:\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] USER REQUESTED SPECIFIC VERSION\\n\")\n",
        "                f.write(f\"Requested Version: {version_match.group(0) if version_match else 'Unknown'}\\n\")\n",
        "                f.write(f\"Version Loaded Successfully: {version_loaded}\\n\")\n",
        "                f.write(f\"Version Shape: {version_specific_df.shape if version_specific_df is not None else 'N/A'}\\n\")\n",
        "                f.write(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "    # Add the current user message (enhanced with context if provided)\n",
        "    all_messages.append({\"role\": \"user\", \"content\": enhanced_message})\n",
        "\n",
        "    # *** COMPRESSION STEP ***\n",
        "    openai_client = app_state.get(\"openai_client\") or OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "    optimized_messages = conversation_manager.compress_history_if_needed(all_messages, openai_client)\n",
        "\n",
        "    # Log compression if it happened\n",
        "    original_size = conversation_manager.get_conversation_size(all_messages)\n",
        "    optimized_size = conversation_manager.get_conversation_size(optimized_messages)\n",
        "    if optimized_size < original_size:\n",
        "        logger.info(f\"Compressed conversation: {original_size} -> {optimized_size} tokens\")\n",
        "        if session_recorder.current_session_file:\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"\\n[COMPRESSION] Reduced conversation from {original_size} to {optimized_size} tokens ({len(all_messages)} -> {len(optimized_messages)} messages)\\n\")\n",
        "\n",
        "    # Create a state dictionary for the graph\n",
        "    state = {\n",
        "        \"messages\": optimized_messages,\n",
        "        \"system_message\": app_state[\"system_message\"],\n",
        "        \"df\": version_specific_df if version_specific_df is not None else app_state[\"df\"],  # USE VERSION-SPECIFIC DF\n",
        "        \"issues\": app_state[\"issues\"],\n",
        "        \"needs_clarification\": False,\n",
        "        \"generate_code\": False,\n",
        "        \"execution_plan\": None,\n",
        "        \"clarification_question\": None,\n",
        "        \"code_execution_results\": None,\n",
        "        \"final_response\": None,\n",
        "        \"anthropic_client\": app_state[\"anthropic_client\"],\n",
        "        \"openai_client\": openai_client,\n",
        "        \"selected_context\": selected_message_content,\n",
        "        \"version_context\": version_context,  # Pass version context\n",
        "        \"using_specific_version\": version_loaded,  # Flag for workflow\n",
        "        \"requested_version\": version_match.group(0) if version_match else None  # Store requested version name\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the workflow if not already created\n",
        "        if not hasattr(chat_with_selected_message, \"workflow\"):\n",
        "            chat_with_selected_message.workflow = create_agentic_rent_roll_analyzer()\n",
        "            logger.info(\"Created agentic workflow\")\n",
        "\n",
        "        # Run the workflow with the current state\n",
        "        logger.info(f\"Running agentic workflow with {'version-specific dataframe' if version_loaded else 'current dataframe'}\")\n",
        "        result = chat_with_selected_message.workflow.invoke(state)\n",
        "\n",
        "        # Get the final response from the result state\n",
        "        final_response = result.get(\"final_response\", \"I'm sorry, I couldn't process your request.\")\n",
        "        logger.info(f\"Received final response from workflow: {final_response[:50]}...\")\n",
        "\n",
        "        # Enhanced session recording\n",
        "        action_type = \"version_specific_analysis\" if version_loaded else (\"contextual_analysis\" if selected_message_content else \"analysis\")\n",
        "\n",
        "        if result.get(\"needs_clarification\"):\n",
        "            action_type = \"clarification\"\n",
        "        elif result.get(\"generate_code\"):\n",
        "            action_type = f\"version_specific_data_processing\" if version_loaded else (\"contextual_data_processing\" if selected_message_content else \"data_processing\")\n",
        "\n",
        "        # Extract executed code from response\n",
        "        code_executed = None\n",
        "        code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', final_response, re.DOTALL)\n",
        "        if code_blocks:\n",
        "            code_executed = \"\\n\\n# --- Next Code Block ---\\n\\n\".join(code_blocks)\n",
        "\n",
        "        # Check if a new dataframe version was saved\n",
        "        version_saved = None\n",
        "        if \"✓ Saved dataframe version\" in final_response:\n",
        "            version_match_response = re.search(r'version (v_\\w+)', final_response)\n",
        "            if version_match_response:\n",
        "                version_saved = version_match_response.group(1)\n",
        "\n",
        "        # Record the conversation turn with enhanced context information\n",
        "        session_description = enhanced_message if len(enhanced_message) < 500 else enhanced_message[:500] + \"...\"\n",
        "        session_recorder.record_conversation_turn(\n",
        "            user_message=session_description,\n",
        "            ai_response=final_response,\n",
        "            action_type=action_type,\n",
        "            code_executed=code_executed,\n",
        "            version_saved=version_saved\n",
        "        )\n",
        "\n",
        "        # Add version-specific note to response if a specific version was used\n",
        "        if version_loaded:\n",
        "            version_note = f\"\\n\\n📋 **Version Note**: This analysis was performed on version `{state['requested_version']}` as requested, not the current/latest dataframe.\"\n",
        "            final_response += version_note\n",
        "\n",
        "        # Use the correct format for Gradio chatbot\n",
        "        history_list = list(history) if history else []\n",
        "        # Add the original message (not the enhanced one) to maintain clean chat display\n",
        "        history_list.append((message, final_response))\n",
        "\n",
        "        logger.info(\"Chat response processing complete with version-specific handling\")\n",
        "        return history_list\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing chat with version handling: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        error_message = f\"Error getting response: {str(e)}\"\n",
        "\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=enhanced_message,\n",
        "                ai_response=error_message,\n",
        "                action_type=\"system_error_with_version\" if version_loaded else (\"system_error_with_context\" if selected_message_content else \"system_error\"),\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        history_list = list(history) if history else []\n",
        "        history_list.append((message, error_message))\n",
        "        return history_list\n",
        "\n",
        "# Helper functions for message selection\n",
        "def extract_messages_for_selection(history):\n",
        "    \"\"\"Extract assistant messages for selection dropdown\"\"\"\n",
        "    if not history:\n",
        "        return gr.update(choices=[], value=None)\n",
        "\n",
        "    choices = []\n",
        "    for i, (user_msg, assistant_msg) in enumerate(history):\n",
        "        # Create a preview of the assistant message\n",
        "        preview = assistant_msg[:100] + \"...\" if len(assistant_msg) > 100 else assistant_msg\n",
        "        # Remove newlines for cleaner display\n",
        "        preview = preview.replace('\\n', ' ').replace('\\r', '')\n",
        "        choices.append((f\"#{i+1}: {preview}\", i))\n",
        "\n",
        "    return gr.update(choices=choices, value=None)\n",
        "\n",
        "def get_selected_message_preview(history, selected_index):\n",
        "    \"\"\"Get full preview of selected message\"\"\"\n",
        "    if selected_index is None or not history or selected_index >= len(history):\n",
        "        return \"\"\n",
        "\n",
        "    assistant_msg = history[selected_index][1]\n",
        "    return f\"Selected Message #{selected_index + 1}:\\n\\n{assistant_msg}\"\n",
        "\n",
        "def clear_message_selection():\n",
        "    \"\"\"Clear the message selection\"\"\"\n",
        "    return None, \"\", gr.update(value=None)\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\"), css=custom_css) as demo:\n",
        "    gr.Markdown(\"# Agentic Commercial Real Estate Rent Roll Analyzer\")\n",
        "    gr.Markdown(\"## Hybrid AI System: GPT-4 for Decision Making & Claude for Code Generation\")\n",
        "\n",
        "    with gr.Tab(\"Setup\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                file_input = gr.File(label=\"Upload Rent Roll Excel File (.xlsx, .xls)\")\n",
        "\n",
        "                # Add separate API key inputs for OpenAI and Anthropic\n",
        "                anthropic_api_key = gr.Textbox(\n",
        "                    label=\"Anthropic API Key (Optional - for code generation)\",\n",
        "                    placeholder=\"Leave blank to use the default API key\",\n",
        "                    type=\"password\"\n",
        "                )\n",
        "\n",
        "                openai_api_key = gr.Textbox(\n",
        "                    label=\"OpenAI API Key (Optional - for decision making and text responses)\",\n",
        "                    placeholder=\"Leave blank to use the default API key\",\n",
        "                    type=\"password\"\n",
        "                )\n",
        "\n",
        "                # Updated auto-analyze checkbox\n",
        "                auto_analyze = gr.Checkbox(\n",
        "                    label=\"Automatically analyze for issues using GPT-4\",\n",
        "                    value=True,\n",
        "                    info=\"When checked, GPT-4 will automatically identify issues in your rent roll\"\n",
        "                )\n",
        "\n",
        "                upload_button = gr.Button(\"Load Rent Roll & Start Chat\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                result = gr.Textbox(label=\"Status\")\n",
        "                preview = gr.HTML(label=\"Data Preview\")\n",
        "\n",
        "    with gr.Tab(\"Chat\"):\n",
        "        # Session management buttons\n",
        "        with gr.Row():\n",
        "            view_versions_btn = gr.Button(\"View Version History\")\n",
        "            create_template_btn = gr.Button(\"🎯 Create Template from Session\", variant=\"primary\")\n",
        "            end_session_btn = gr.Button(\"🔚 End Current Session\")\n",
        "            session_status_btn = gr.Button(\"📊 Session Status\")\n",
        "\n",
        "        data_view = gr.HTML()\n",
        "        chatbot = gr.Chatbot(label=\"Agentic Rent Roll Analysis Chat\", height=500)\n",
        "\n",
        "        # NEW: Message Selection System\n",
        "        with gr.Accordion(\"💬 Reply to Specific Message\", open=False):\n",
        "            gr.Markdown(\"\"\"\n",
        "            **Select a previous assistant message to use as context for your next question.**\n",
        "            This helps maintain conversation flow and build on previous analysis.\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                message_selector = gr.Dropdown(\n",
        "                    label=\"Select Assistant Message\",\n",
        "                    choices=[],\n",
        "                    value=None,\n",
        "                    interactive=True,\n",
        "                    scale=3\n",
        "                )\n",
        "                refresh_messages_btn = gr.Button(\"🔄 Refresh\", size=\"sm\", scale=1)\n",
        "\n",
        "            selected_message_preview = gr.Textbox(\n",
        "                label=\"Selected Message Preview\",\n",
        "                lines=4,\n",
        "                interactive=False,\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                use_selection_btn = gr.Button(\"✅ Use Selected Message as Context\", variant=\"primary\", visible=False)\n",
        "                clear_selection_btn = gr.Button(\"❌ Clear Selection\", size=\"sm\", visible=False)\n",
        "\n",
        "            # Hidden state to store selected message content\n",
        "            selected_message_content = gr.State(value=None)\n",
        "            selected_message_index = gr.State(value=None)\n",
        "\n",
        "        # Context indicator\n",
        "        context_indicator = gr.HTML(value=\"\", visible=False)\n",
        "\n",
        "        # Enhanced message input area with prompt enhancement\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                msg = gr.Textbox(\n",
        "                    label=\"Your question\",\n",
        "                    placeholder=\"Ask about the rent roll... (Use ✨ Enhance for AI-improved prompts)\",\n",
        "                    lines=2\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    enhance_btn = gr.Button(\"✨ Enhance\", variant=\"secondary\", size=\"sm\")\n",
        "                    send_btn = gr.Button(\"Send\", variant=\"primary\", size=\"sm\")\n",
        "\n",
        "        # Status line for enhancement feedback\n",
        "        enhancement_status = gr.Textbox(\n",
        "            label=\"Enhancement Status\",\n",
        "            interactive=False,\n",
        "            visible=True,\n",
        "            lines=1,\n",
        "            value=\"Ready to enhance prompts with AI context\"\n",
        "        )\n",
        "        compression_status = gr.Textbox(\n",
        "            label=\"Conversation Compression Status\",\n",
        "            interactive=False,\n",
        "            visible=True,\n",
        "            lines=1,\n",
        "            value=\"Conversation size: Normal\"\n",
        "        )\n",
        "        clear_btn = gr.Button(\"Clear Chat History\")\n",
        "\n",
        "        # Template creation input\n",
        "        with gr.Accordion(\"Template Creation\", open=False):\n",
        "            template_name_input = gr.Textbox(\n",
        "                label=\"Template Name (Optional)\",\n",
        "                placeholder=\"e.g., 'Monthly Rent Roll Cleanup Process'\",\n",
        "                lines=1\n",
        "            )\n",
        "            template_status = gr.Textbox(label=\"Template Creation Status\", interactive=False, lines=5)\n",
        "\n",
        "        # Message selection handlers\n",
        "        refresh_messages_btn.click(\n",
        "            extract_messages_for_selection,\n",
        "            inputs=[chatbot],\n",
        "            outputs=[message_selector]\n",
        "        )\n",
        "\n",
        "        message_selector.change(\n",
        "            get_selected_message_preview,\n",
        "            inputs=[chatbot, message_selector],\n",
        "            outputs=[selected_message_preview]\n",
        "        ).then(\n",
        "            lambda selected: (\n",
        "                gr.update(visible=selected is not None),\n",
        "                gr.update(visible=selected is not None),\n",
        "                gr.update(visible=selected is not None)\n",
        "            ),\n",
        "            inputs=[message_selector],\n",
        "            outputs=[selected_message_preview, use_selection_btn, clear_selection_btn]\n",
        "        )\n",
        "\n",
        "        use_selection_btn.click(\n",
        "            lambda history, idx: (\n",
        "                history[idx][1] if idx is not None and idx < len(history) else None,\n",
        "                idx,\n",
        "                f\"<div style='background-color: #e3f2fd; padding: 10px; border-radius: 5px; margin: 10px 0;'><strong>🔗 Context Active:</strong> Using message #{idx + 1} as context</div>\" if idx is not None else \"\"\n",
        "            ),\n",
        "            inputs=[chatbot, message_selector],\n",
        "            outputs=[selected_message_content, selected_message_index, context_indicator]\n",
        "        ).then(\n",
        "            lambda: gr.update(visible=True),\n",
        "            outputs=[context_indicator]\n",
        "        )\n",
        "\n",
        "        clear_selection_btn.click(\n",
        "            clear_message_selection,\n",
        "            outputs=[selected_message_content, context_indicator, message_selector]\n",
        "        ).then(\n",
        "            lambda: gr.update(visible=False),\n",
        "            outputs=[context_indicator]\n",
        "        )\n",
        "\n",
        "        # Enhancement button handler\n",
        "        enhance_btn.click(\n",
        "            enhance_prompt_interface,\n",
        "            inputs=[msg, chatbot],\n",
        "            outputs=[enhancement_status, msg]\n",
        "        )\n",
        "\n",
        "        # Enhanced chat handlers with message selection\n",
        "        msg.submit(\n",
        "            chat_with_compression_status,\n",
        "            inputs=[msg, chatbot, selected_message_content, selected_message_index],\n",
        "            outputs=[chatbot, compression_status]\n",
        "        ).then(\n",
        "            lambda: (\"\", None, None, \"\"),  # Clear message, context, and indicator\n",
        "            outputs=[msg, selected_message_content, selected_message_index, context_indicator]\n",
        "        ).then(\n",
        "            lambda: gr.update(visible=False),\n",
        "            outputs=[context_indicator]\n",
        "        ).then(\n",
        "            extract_messages_for_selection,  # Refresh message selector\n",
        "            inputs=[chatbot],\n",
        "            outputs=[message_selector]\n",
        "        )\n",
        "\n",
        "        send_btn.click(\n",
        "            chat_with_compression_status,\n",
        "            inputs=[msg, chatbot, selected_message_content, selected_message_index],\n",
        "            outputs=[chatbot, compression_status]\n",
        "        ).then(\n",
        "            lambda: (\"\", None, None, \"\"),\n",
        "            outputs=[msg, selected_message_content, selected_message_index, context_indicator]\n",
        "        ).then(\n",
        "            lambda: gr.update(visible=False),\n",
        "            outputs=[context_indicator]\n",
        "        ).then(\n",
        "            extract_messages_for_selection,\n",
        "            inputs=[chatbot],\n",
        "            outputs=[message_selector]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            clear_chat,\n",
        "            outputs=[chatbot]\n",
        "        ).then(\n",
        "            lambda: gr.update(choices=[], value=None),\n",
        "            outputs=[message_selector]\n",
        "        ).then(\n",
        "            lambda: \"✅ New conversation\",  # ← ADDED - Reset compression status\n",
        "            outputs=[compression_status]\n",
        "        )\n",
        "\n",
        "        # Enhanced event handlers for session management\n",
        "        view_versions_btn.click(view_dataframe_versions, None, data_view)\n",
        "\n",
        "        create_template_btn.click(\n",
        "            create_template_from_current_session,\n",
        "            inputs=[template_name_input],\n",
        "            outputs=[template_status]\n",
        "        )\n",
        "\n",
        "        end_session_btn.click(\n",
        "            end_current_session,\n",
        "            outputs=[template_status]\n",
        "        )\n",
        "\n",
        "        session_status_btn.click(\n",
        "            get_current_session_status,\n",
        "            outputs=[template_status]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Edit Data\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 📝 Edit Rent Roll Data\n",
        "\n",
        "        You can directly edit cells in the table below, just like in Excel.\n",
        "        - Click on any cell to edit it\n",
        "        - Use Tab or arrow keys to navigate\n",
        "        - Changes are analyzed by GPT-4.1 and recorded in your session\n",
        "        - All changes are automatically saved to session history\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=3):\n",
        "                # Version selector\n",
        "                version_dropdown = gr.Dropdown(\n",
        "                    label=\"Select Version to Edit\",\n",
        "                    choices=get_version_choices(),\n",
        "                    value=None,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                refresh_versions_btn = gr.Button(\"🔄 Refresh Versions\", size=\"sm\")\n",
        "                with gr.Row():\n",
        "                    load_latest_btn = gr.Button(\"📂 Load Latest\", variant=\"secondary\", size=\"sm\")\n",
        "                    load_version_btn = gr.Button(\"📂 Load Selected\", variant=\"primary\", size=\"sm\")\n",
        "\n",
        "        # Status display\n",
        "        edit_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        # The editable dataframe\n",
        "        editable_df = gr.Dataframe(\n",
        "            label=\"Editable Data (Click any cell to edit) - Changes tracked by AI\",\n",
        "            interactive=True,\n",
        "            wrap=True,\n",
        "            max_height=500,\n",
        "            column_widths=[\"100px\"] * 20,\n",
        "        )\n",
        "\n",
        "        # Save controls\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=3):\n",
        "                save_description = gr.Textbox(\n",
        "                    label=\"Description of Changes (GPT-4.1 will analyze if left blank)\",\n",
        "                    placeholder=\"e.g., 'Updated rent for units 101-105' or leave blank for AI analysis\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                save_changes_btn = gr.Button(\"💾 Save & Analyze Changes\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        save_status = gr.Textbox(label=\"Save Status & AI Analysis\", interactive=False, lines=8)\n",
        "\n",
        "        # Quick actions section\n",
        "        with gr.Accordion(\"Quick Actions\", open=False):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### Bulk Operations\n",
        "            Use these buttons for common bulk edits:\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                # Add quick action buttons here in future\n",
        "                gr.Button(\"🧹 Clean Empty Rows\", size=\"sm\", interactive=False)\n",
        "                gr.Button(\"💵 Round All Currency\", size=\"sm\", interactive=False)\n",
        "                gr.Button(\"📅 Fix Date Formats\", size=\"sm\", interactive=False)\n",
        "                gr.Button(\"🔢 Recalculate Totals\", size=\"sm\", interactive=False)\n",
        "\n",
        "        # Enhanced session tracking notice\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 🤖 AI-Powered Change Tracking\n",
        "        - **GPT-4.1 Analysis**: Every edit is analyzed for business impact\n",
        "        - **Session Recording**: All changes saved to copiloting session\n",
        "        - **Template Ready**: Manual edits become part of reusable workflows\n",
        "        - **Quality Assurance**: AI detects data quality improvements/issues\n",
        "        \"\"\")\n",
        "\n",
        "        # Event handlers for Edit Data tab with enhanced functions\n",
        "        refresh_versions_btn.click(\n",
        "            refresh_version_dropdown,\n",
        "            outputs=[version_dropdown]\n",
        "        )\n",
        "\n",
        "        load_latest_btn.click(\n",
        "            load_latest_version_for_editing,  # ← Enhanced function\n",
        "            outputs=[editable_df, edit_status]\n",
        "        )\n",
        "\n",
        "        load_version_btn.click(\n",
        "            load_specific_version,  # ← Enhanced function\n",
        "            inputs=[version_dropdown],\n",
        "            outputs=[editable_df, edit_status]\n",
        "        )\n",
        "\n",
        "        save_changes_btn.click(\n",
        "            save_edited_dataframe,  # ← Enhanced function\n",
        "            inputs=[editable_df, save_description],\n",
        "            outputs=[save_status, editable_df]\n",
        "        ).then(\n",
        "            refresh_version_dropdown,  # Refresh the dropdown after saving\n",
        "            outputs=[version_dropdown]\n",
        "        )\n",
        "\n",
        "        # Instructions\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### 💡 How to Use Enhanced Edit Data:\n",
        "        1. **Load Data**: Click \"Load Latest\" or select a specific version\n",
        "        2. **Edit Cells**: Click on any cell and type to edit (just like Excel!)\n",
        "        3. **Navigate**: Use Tab, Enter, or arrow keys to move between cells\n",
        "        4. **Save Changes**: Enter a description (optional) and click \"Save & Analyze Changes\"\n",
        "        5. **AI Analysis**: GPT-4.1 will analyze your changes and provide insights\n",
        "\n",
        "        ### ⚠️ Enhanced Features:\n",
        "        - **Automatic Analysis**: AI understands what you changed and why\n",
        "        - **Business Impact**: Get insights on how changes affect rent calculations\n",
        "        - **Session Integration**: All edits become part of your copiloting history\n",
        "        - **Template Building**: Manual edits are included in reusable templates\n",
        "        - **Quality Checks**: AI warns if changes might impact data quality\n",
        "        \"\"\")\n",
        "\n",
        "    create_enhanced_template_application_tab()\n",
        "\n",
        "    with gr.Tab(\"Template Manager\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 📋 Template Management System\n",
        "\n",
        "        Create, view, and apply reusable rent roll processing templates.\n",
        "        Templates capture your complete workflow including conversations, code, and manual edits.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### Available Templates\")\n",
        "                template_list = gr.HTML(label=\"Template List\")\n",
        "                refresh_templates_btn = gr.Button(\"🔄 Refresh Template List\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### Template Details\")\n",
        "                template_details = gr.HTML(label=\"Template Summary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            template_id_input = gr.Textbox(\n",
        "                label=\"Template ID\",\n",
        "                placeholder=\"e.g., template_20250526_143022\"\n",
        "            )\n",
        "            with gr.Column():\n",
        "                view_template_btn = gr.Button(\"👁️ View Template\", variant=\"secondary\")\n",
        "                delete_template_btn = gr.Button(\"🗑️ Delete Template\", variant=\"stop\")\n",
        "\n",
        "        template_action_status = gr.Textbox(label=\"Status\", interactive=False, lines=3)\n",
        "\n",
        "        # Template management event handlers\n",
        "        refresh_templates_btn.click(\n",
        "            lambda: list_available_templates(),\n",
        "            outputs=[template_list]\n",
        "        )\n",
        "\n",
        "        view_template_btn.click(\n",
        "            lambda template_id: enhanced_template_manager.get_template_summary(template_id) if template_id else \"Please enter a template ID\",\n",
        "            inputs=[template_id_input],\n",
        "            outputs=[template_details]\n",
        "        )\n",
        "\n",
        "        delete_template_btn.click(\n",
        "            lambda template_id: enhanced_template_manager.delete_template(template_id) if template_id else \"Please enter a template ID\",\n",
        "            inputs=[template_id_input],\n",
        "            outputs=[template_action_status]\n",
        "        ).then(\n",
        "            lambda: list_available_templates(),  # Refresh list after deletion\n",
        "            outputs=[template_list]\n",
        "        )\n",
        "\n",
        "    # Initially hide the chat interface\n",
        "    chatbot.visible = False\n",
        "\n",
        "    # Updated upload button event with both API keys and version dropdown\n",
        "    upload_button.click(\n",
        "        upload_rent_roll,\n",
        "        inputs=[file_input, anthropic_api_key, openai_api_key, auto_analyze],\n",
        "        outputs=[result, preview, chatbot, version_dropdown]\n",
        "    )\n",
        "\n",
        "    # Updated style and help info\n",
        "    gr.Markdown(\"\"\"\n",
        "    ## How to use this Enhanced Agentic Rent Roll Analyzer:\n",
        "\n",
        "    ### ✨ **NEW: AI Prompt Enhancement**\n",
        "\n",
        "    **In the Chat tab, use the \"✨ Enhance\" button to:**\n",
        "    - Transform simple requests into professional CRE analysis prompts\n",
        "    - Add context from your actual rent roll data (column names, data types)\n",
        "    - Include relevant business terminology and best practices\n",
        "    - Make prompts more specific and actionable\n",
        "\n",
        "    **Example Enhancement:**\n",
        "    - **Before:** \"show me the data\"\n",
        "    - **After:** \"Display a comprehensive overview of the rent roll data including tenant information from [TenantName] column, rent amounts from [BaseRent] and [TotalRent] columns, lease dates from [LeaseStart] and [LeaseEnd] columns, and occupancy status. Also highlight any data quality issues such as missing values or inconsistent formatting.\"\n",
        "\n",
        "    ### 🚀 **Template Application System**\n",
        "\n",
        "    #### **Workflow Overview:**\n",
        "    1. **Create Templates** (Chat Tab): Work through your rent roll analysis normally\n",
        "    2. **Save Templates**: Click \"Create Template from Session\" to save your workflow\n",
        "    3. **Apply Templates** (Apply Template Tab): Use saved templates on new rent roll files\n",
        "    4. **Automated Processing**: GPT-4.1 + Claude 3.7 adapt and execute each step\n",
        "\n",
        "    ### 📋 **Step-by-Step Guide:**\n",
        "\n",
        "    #### **Phase 1: Create Your First Template**\n",
        "    1. **Setup Tab**: Upload your rent roll Excel file\n",
        "    2. **Chat Tab**: Interact normally - ask questions, get analysis, make changes\n",
        "    3. **Use ✨ Enhance**: Make your prompts more professional and context-aware\n",
        "    4. **Edit Data Tab**: Make any manual edits (tracked by AI)\n",
        "    5. **Create Template**: Click \"🎯 Create Template from Session\"\n",
        "\n",
        "    #### **Phase 2: Apply Template to New Files**\n",
        "    1. **Apply Template Tab**: Select your saved template ID\n",
        "    2. **Upload New File**: Choose a similar rent roll file\n",
        "    3. **Start Application**: Click \"🚀 Start Application\"\n",
        "    4. **Execute Steps**: Run \"▶️ Execute Next Step\" or \"⏭️ Execute All Steps\"\n",
        "\n",
        "    ### 🤖 **AI Workflow in Template Application:**\n",
        "\n",
        "    **For Each Template Step:**\n",
        "    1. **GPT-4.1 Analysis**:\n",
        "       - Analyzes original template step\n",
        "       - Maps columns from template to new file\n",
        "       - Adapts parameters and business rules\n",
        "       - Creates optimized prompt for Claude\n",
        "\n",
        "    2. **Claude 3.7 Execution**:\n",
        "       - Receives adapted instructions\n",
        "       - Generates appropriate Python code\n",
        "       - Executes data processing\n",
        "       - Returns results and updates dataframe\n",
        "\n",
        "    3. **Validation & Progress**:\n",
        "       - Validates step completion\n",
        "       - Records success/failure\n",
        "       - Logs detailed results\n",
        "       - Moves to next step\n",
        "\n",
        "    ### 🔧 **Key Features:**\n",
        "\n",
        "    - **✨ AI Prompt Enhancement**: Smart context-aware prompt improvement\n",
        "    - **Intelligent Adaptation**: Automatically maps different column names\n",
        "    - **Business Logic Preservation**: Maintains the intent of original analysis\n",
        "    - **Error Recovery**: Handles failures gracefully and continues\n",
        "    - **Progress Tracking**: Real-time status of template application\n",
        "    - **Complete Logging**: Detailed logs of every step and decision\n",
        "\n",
        "    ### 💡 **Use Cases:**\n",
        "\n",
        "    - **Monthly Processing**: Apply same cleanup to each month's rent roll\n",
        "    - **Property Portfolios**: Use one template across multiple properties\n",
        "    - **Team Workflows**: Share proven analysis methods\n",
        "    - **Quality Assurance**: Ensure consistent processing standards\n",
        "    - **Time Savings**: Automate repetitive analysis tasks\n",
        "\n",
        "    ### ⚡ **Quick Start:**\n",
        "\n",
        "    1. Upload rent roll → Chat about analysis (use ✨ Enhance!) → Create template\n",
        "    2. Get template ID from Template Manager\n",
        "    3. Go to Apply Template → Enter template ID → Upload new file → Execute!\n",
        "\n",
        "    The system transforms your one-time analysis into reusable, intelligent automation!\n",
        "    \"\"\")\n",
        "\n",
        "# Additional helper functions for Template Manager tab\n",
        "def list_available_templates():\n",
        "    \"\"\"Generate HTML list of available templates\"\"\"\n",
        "    try:\n",
        "        templates = enhanced_template_manager.list_templates()\n",
        "\n",
        "        if not templates:\n",
        "            return \"<p>No templates available yet. Create your first template by using the 'Create Template from Session' button in the Chat tab.</p>\"\n",
        "\n",
        "        html = \"<div style='max-height: 400px; overflow-y: auto;'>\"\n",
        "\n",
        "        for template in templates:\n",
        "            gpt_status = \"🤖 GPT-4 Analysis\" if template.get('gpt4_analysis_available') else \"📝 Basic Info\"\n",
        "\n",
        "            html += f\"\"\"\n",
        "            <div style='border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 8px; background-color: #f9f9f9;'>\n",
        "                <h4 style='margin: 0 0 10px 0; color: #333;'>{template['template_name']}</h4>\n",
        "                <p style='margin: 5px 0; color: #666;'><strong>ID:</strong> <code>{template['template_id']}</code></p>\n",
        "                <p style='margin: 5px 0; color: #666;'><strong>Created:</strong> {template['created_date'][:10]}</p>\n",
        "                <p style='margin: 5px 0; color: #666;'><strong>Source:</strong> {template['source_file']}</p>\n",
        "                <p style='margin: 5px 0; color: #666;'><strong>Steps:</strong> {template['steps_count']} workflow steps</p>\n",
        "                <p style='margin: 5px 0;'><span style='background-color: #e3f2fd; padding: 2px 6px; border-radius: 4px; font-size: 12px;'>{gpt_status}</span></p>\n",
        "            </div>template_app_engine = TemplateApplicationEngine()\n",
        "            \"\"\"\n",
        "\n",
        "        html += \"</div>\"\n",
        "        return html\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        return f\"<p>Error loading templates: {str(e)}</p>\"\n",
        "\n",
        "\n",
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Agentic Rent Roll Analyzer application with prompt enhancement\")\n",
        "    demo.launch(debug=True)\n",
        "    logger.info(\"Application shutdown\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8Y_is2j0UCT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2xWsOChRZ8Z"
      },
      "source": [
        "Summarize the key user-guided instructions and solutions from this rent roll copiloting session.\n",
        "\n",
        "For each step, briefly describe:\n",
        "\n",
        "What the user wanted to accomplish (without quoting them directly).\n",
        "\n",
        "How the request was addressed or solved by the copilot.\n",
        "\n",
        "Avoid excessive detail, don’t repeat the user’s exact instructions, and keep each summary concise and clear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGHC6Ofa2VmZ"
      },
      "outputs": [],
      "source": [
        "# Run this in your Colab cell to diagnose the version system\n",
        "\n",
        "def run_version_diagnostic():\n",
        "    \"\"\"Complete diagnostic of the version system\"\"\"\n",
        "\n",
        "    print(\"🔍 COMPREHENSIVE VERSION SYSTEM DIAGNOSTIC\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. Check global app_state\n",
        "    print(\"1. APP_STATE CHECK:\")\n",
        "    if 'app_state' in globals():\n",
        "        print(\"   ✅ app_state exists globally\")\n",
        "        if app_state is None:\n",
        "            print(\"   ❌ app_state is None\")\n",
        "        else:\n",
        "            print(\"   ✅ app_state is not None\")\n",
        "            print(f\"   📊 Keys in app_state: {list(app_state.keys())}\")\n",
        "\n",
        "            if \"df_versions\" in app_state:\n",
        "                versions = app_state[\"df_versions\"]\n",
        "                print(f\"   📋 Versions in registry: {len(versions)}\")\n",
        "                for i, v in enumerate(versions):\n",
        "                    name = v.get('name', 'UNKNOWN')\n",
        "                    desc = v.get('description', 'No description')[:50]\n",
        "                    print(f\"      {i+1}. {name} - {desc}\")\n",
        "            else:\n",
        "                print(\"   ❌ No df_versions key in app_state\")\n",
        "    else:\n",
        "        print(\"   ❌ app_state not found in globals\")\n",
        "\n",
        "    # 2. Check file system\n",
        "    print(\"\\n2. FILE SYSTEM CHECK:\")\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "    if os.path.exists(versions_dir):\n",
        "        print(f\"   ✅ Directory exists: {versions_dir}\")\n",
        "        files = os.listdir(versions_dir)\n",
        "        csv_files = [f for f in files if f.endswith('.csv')]\n",
        "        excel_files = [f for f in files if f.endswith('.xlsx')]\n",
        "\n",
        "        print(f\"   📄 Total files: {len(files)}\")\n",
        "        print(f\"   📄 CSV files: {len(csv_files)}\")\n",
        "        print(f\"   📄 Excel files: {len(excel_files)}\")\n",
        "\n",
        "        print(\"   📋 CSV Files found:\")\n",
        "        for csv_file in csv_files[:10]:  # Show first 10\n",
        "            file_path = os.path.join(versions_dir, csv_file)\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            print(f\"      • {csv_file} ({file_size:,} bytes)\")\n",
        "\n",
        "        if len(csv_files) > 10:\n",
        "            print(f\"      ... and {len(csv_files) - 10} more\")\n",
        "    else:\n",
        "        print(f\"   ❌ Directory not found: {versions_dir}\")\n",
        "\n",
        "    # 3. Test version choices function\n",
        "    print(\"\\n3. VERSION CHOICES TEST:\")\n",
        "    try:\n",
        "        choices = get_version_choices()\n",
        "        print(f\"   📋 Generated choices: {len(choices)}\")\n",
        "        for choice in choices[:5]:  # Show first 5\n",
        "            print(f\"      • {choice}\")\n",
        "        if len(choices) > 5:\n",
        "            print(f\"      ... and {len(choices) - 5} more\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error getting version choices: {e}\")\n",
        "\n",
        "    # 4. Test specific version loading\n",
        "    print(\"\\n4. VERSION LOADING TEST:\")\n",
        "    if 'choices' in locals() and choices:\n",
        "        test_version = choices[0].split(\" (\")[0].strip()  # Get clean name\n",
        "        print(f\"   🧪 Testing load of version: {test_version}\")\n",
        "        try:\n",
        "            df, status = load_specific_version(test_version)\n",
        "            if df is not None:\n",
        "                print(f\"   ✅ Successfully loaded: {df.shape}\")\n",
        "                print(f\"   📄 Status: {status[:100]}...\")\n",
        "            else:\n",
        "                print(f\"   ❌ Failed to load: {status}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error loading version: {e}\")\n",
        "    else:\n",
        "        print(\"   ⚠️  No versions available to test\")\n",
        "\n",
        "    print(\"\\n5. RECOMMENDATIONS:\")\n",
        "\n",
        "    # Check for common issues\n",
        "    if 'app_state' not in globals() or app_state is None:\n",
        "        print(\"   🔧 Run: Restart and upload a rent roll file first\")\n",
        "    elif \"df_versions\" not in app_state or not app_state[\"df_versions\"]:\n",
        "        if os.path.exists(\"rent_roll_versions\") and len([f for f in os.listdir(\"rent_roll_versions\") if f.endswith('.csv')]) > 0:\n",
        "            print(\"   🔧 Run: force_sync_versions() to sync files with registry\")\n",
        "        else:\n",
        "            print(\"   🔧 Create some versions first by using the chat or editing data\")\n",
        "    else:\n",
        "        print(\"   ✅ System looks healthy!\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Run the diagnostic\n",
        "run_version_diagnostic()\n",
        "\n",
        "# If you have sync issues, also run this:\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"RUNNING FORCE SYNC...\")\n",
        "print(\"=\"*40)\n",
        "try:\n",
        "    result = force_sync_versions()\n",
        "    print(result)\n",
        "\n",
        "    # Test again after sync\n",
        "    print(\"\\nTesting version choices after sync:\")\n",
        "    choices = get_version_choices()\n",
        "    print(f\"Available choices: {len(choices)}\")\n",
        "    for choice in choices[:3]:\n",
        "        print(f\"  • {choice}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Sync failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc-75XeTtyKI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
