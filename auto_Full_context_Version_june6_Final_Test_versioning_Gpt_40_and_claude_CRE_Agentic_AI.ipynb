{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IX7KhzaJbF8-",
        "outputId": "eba04e47-abb7-4af6-f872-96c8380d3ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.32.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.52.2-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.52.2-py3-none-any.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.3/286.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.52.2\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.63)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.4.8 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "!pip install anthropic\n",
        "!pip install langgraph langchain langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgmo4iqobHNJ"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import glob\n",
        "import os\n",
        "from anthropic import Anthropic\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import traceback\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "import numpy as np\n",
        "import logging\n",
        "import json\n",
        "from typing import List, Optional, Dict, Any, Union\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMymkJcBcBB5"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('rent_roll_analyzer')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yes16m2AcCzI"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Optional, Union, Dict, Any\n",
        "\n",
        "# Define the state as a TypedDict\n",
        "class AgentState(TypedDict, total=False):\n",
        "    messages: List[Dict[str, str]]\n",
        "    df: Optional[pd.DataFrame]\n",
        "    issues: List[str]\n",
        "    execution_plan: Optional[str]\n",
        "    needs_clarification: bool\n",
        "    clarification_question: Optional[str]\n",
        "    generate_code: bool\n",
        "    code_execution_results: Optional[str]\n",
        "    final_response: Optional[str]\n",
        "    anthropic_client: Optional[Any]  # For Claude API\n",
        "    openai_client: Optional[Any]     # For OpenAI API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlFG6YSrcEUU"
      },
      "outputs": [],
      "source": [
        "def read_rent_roll_simple(file_path):\n",
        "    \"\"\"\n",
        "    Improved function to read rent roll Excel files that handles special formatting\n",
        "    commonly found in commercial real estate rent roll sheets.\n",
        "    \"\"\"\n",
        "    # Read the raw Excel file with no header\n",
        "    df = pd.read_excel(file_path, header=None)\n",
        "\n",
        "    # Find the row containing the column headers\n",
        "    header_row = None\n",
        "    for i, row in df.iterrows():\n",
        "        if row.iloc[0] == \"Current\":\n",
        "            header_row = i + 1  # Headers are in the row after \"Current\"\n",
        "            break\n",
        "\n",
        "    if header_row is None:\n",
        "        logger.warning(\"Could not find header row with 'Current' marker. Falling back to standard loading.\")\n",
        "        return pd.read_excel(file_path)\n",
        "\n",
        "    # Get the headers\n",
        "    headers = []\n",
        "    for val in df.iloc[header_row]:\n",
        "        if pd.isna(val):\n",
        "            headers.append(\"NaN\")  # Use \"NaN\" for empty header cells\n",
        "        else:\n",
        "            headers.append(str(val))\n",
        "\n",
        "    # Create a new dataframe starting after the header row\n",
        "    data_rows = df.iloc[(header_row+1):].values\n",
        "\n",
        "    # Create a new dataframe with the extracted headers\n",
        "    result_df = pd.DataFrame(data_rows, columns=headers)\n",
        "\n",
        "    logger.info(f\"Successfully loaded rent roll with {len(result_df)} rows using specialized loader\")\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwQXfnG3cGOW"
      },
      "outputs": [],
      "source": [
        "def analyze_rent_roll_gpt(file_path, api_key):\n",
        "    \"\"\"\n",
        "    Analyzes a CRE rent roll Excel file by sending the data rows to GPT-4.\n",
        "    \"\"\"\n",
        "    # Load the rent roll\n",
        "    try:\n",
        "        df = read_rent_roll_simple(file_path)\n",
        "        logger.info(\"File loaded successfully for GPT analysis.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading file: {e}\")\n",
        "        return []\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Convert the DataFrame to CSV string format\n",
        "    csv_data = df.to_csv(index=False)\n",
        "    logger.info(f\"Converted DataFrame to CSV with {len(df)} rows and {len(df.columns)} columns\")\n",
        "\n",
        "    # Enhance the system prompt to focus on general rent roll issues\n",
        "    system_prompt = \"\"\"\n",
        "    You are a Commercial Real Estate rent roll expert specializing in identifying data quality, formatting, and consistency issues.\n",
        "\n",
        "    When analyzing any CRE rent roll, rigorously check for these common categories of issues:\n",
        "\n",
        "    1. DUPLICATE OR REDUNDANT ENTRIES: Look for any repeated charges, fees, or line items\n",
        "    2. INCONSISTENT TERMINOLOGY: Identify any unclear, non-standard, or ambiguous descriptions\n",
        "    3. DATE ANOMALIES: Flag any suspicious or illogical date patterns across move-in, lease start/end\n",
        "    4. RENT DISCREPANCIES: Identify deviations between market rent values and actual charged amounts\n",
        "    5. CALCULATION INCONSISTENCIES: Check if component charges properly sum to totals\n",
        "    6. EXCEL ARTIFACTS: Identify any visible formulas, function calls, or spreadsheet mechanics\n",
        "    7. FORMATTING IRREGULARITIES: Notice inconsistent data entry patterns or splitting of information\n",
        "    8. BALANCE ANOMALIES: Identify unusual balances, especially negative values\n",
        "    9. OCCUPANCY MISMATCHES: Look for occupied units with zero rent or vacant units with charges\n",
        "    10. UNIT IDENTIFICATION PATTERNS: Check for inconsistencies in unit numbering or identification\n",
        "\n",
        "    Be extremely thorough and specific in your analysis. Report ALL issues you find, regardless of how minor they may seem.\n",
        "    DO NOT return \"No issues detected\" unless you've comprehensively analyzed the data for each category above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use a simplified prompt focused on analyzing the raw CSV data\n",
        "    prompt = (\n",
        "        f\"Please analyze this Commercial Real Estate rent roll data in CSV format and identify ALL potential issues \"\n",
        "        f\"that could affect data quality, accuracy, or decision-making.\\n\\n{csv_data}\\n\\n\"\n",
        "\n",
        "        f\"Based on your expertise in CRE rent rolls, provide a numbered list of ALL issues you can identify, including but not limited to:\\n\\n\"\n",
        "\n",
        "        f\"- Any duplicate or redundant charges\\n\"\n",
        "        f\"- Unclear, non-standard, or inconsistent descriptions\\n\"\n",
        "        f\"- Suspicious or illogical date patterns\\n\"\n",
        "        f\"- Inconsistencies between market rent and actual rent values\\n\"\n",
        "        f\"- Calculation errors where components don't match totals\\n\"\n",
        "        f\"- Spreadsheet artifacts like visible formulas\\n\"\n",
        "        f\"- Inconsistent data entry patterns\\n\"\n",
        "        f\"- Unusual balance values\\n\"\n",
        "        f\"- Occupancy status mismatches\\n\"\n",
        "        f\"- Inconsistent unit numbering or identification\\n\\n\"\n",
        "\n",
        "        f\"IMPORTANT: For each issue found, please reference the specific unit(s) affected and explain why it's problematic. \"\n",
        "        f\"Be comprehensive - rent roll accuracy is critical for CRE investment and property management decisions.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Sending request to GPT-4 for analysis...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=2000,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        response_text = response.choices[0].message.content\n",
        "        logger.info(\"Received response from GPT-4.\")\n",
        "\n",
        "        # Simple parsing of the response - split by numbered items\n",
        "        lines = response_text.split('\\n')\n",
        "        issues = []\n",
        "        current_issue = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            # If it's a new numbered item\n",
        "            if line.strip() and line[0].isdigit() and '. ' in line[:5]:\n",
        "                # If we were building a previous issue, add it\n",
        "                if current_issue:\n",
        "                    issues.append(current_issue.strip())\n",
        "                current_issue = line.strip()\n",
        "            elif line.strip() and current_issue:\n",
        "                # Continue building the current issue\n",
        "                current_issue += \" \" + line.strip()\n",
        "\n",
        "        # Add the last issue if there is one\n",
        "        if current_issue:\n",
        "            issues.append(current_issue.strip())\n",
        "\n",
        "        if not issues:\n",
        "            issues.append(\"No issues detected by GPT-4.\")\n",
        "\n",
        "        logger.info(f\"Identified {len(issues)} issues in the rent roll\")\n",
        "        return issues\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error calling GPT-4 for analysis: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return [\"Failed to analyze rent roll due to API error.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05TftxjAcKhx"
      },
      "outputs": [],
      "source": [
        "def determine_action(state):\n",
        "    \"\"\"Decide whether to answer directly, ask for clarification, or generate code.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    user_message = messages[-1][\"content\"] if messages[-1][\"role\"] == \"user\" else \"\"\n",
        "    df = state[\"df\"]\n",
        "\n",
        "    # Create OpenAI client for this function call\n",
        "    client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "    # Get column information for context\n",
        "    if df is not None:\n",
        "        try:\n",
        "            # Safer way to get column data types\n",
        "            column_info = []\n",
        "            for col in df.columns:\n",
        "                try:\n",
        "                    dtype_str = str(df[col].dtype)  # Convert dtype to string directly\n",
        "                    column_info.append(f\"- {col}: {dtype_str}\")\n",
        "                except:\n",
        "                    column_info.append(f\"- {col}: unknown type\")\n",
        "            column_info_str = \"\\n\".join(column_info)\n",
        "            df_preview = df.head(3).to_string()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting column info: {e}\")\n",
        "            column_info_str = \"Error retrieving column information\"\n",
        "            df_preview = \"Error retrieving data preview\"\n",
        "    else:\n",
        "        column_info_str = \"No dataframe loaded\"\n",
        "        df_preview = \"No data available\"\n",
        "\n",
        "    # Use GPT-4 to analyze the query and determine the best action\n",
        "    prompt = f\"\"\"\n",
        "    User query: {user_message}\n",
        "\n",
        "    Dataframe information:\n",
        "    - Rows: {len(df) if df is not None else 'No data loaded'}\n",
        "    - Columns: {column_info_str}\n",
        "\n",
        "    Data preview:\n",
        "    {df_preview}\n",
        "\n",
        "    Analyze the user query and determine the most appropriate action:\n",
        "    1. If the query is ambiguous or lacks specificity, choose \"ask_clarification\"\n",
        "    2. If the query can be answered with a simple explanation without analysis, choose \"text_response\"\n",
        "    3. If the query requires data analysis, calculations, or visualizations, choose \"generate_code\"\n",
        "\n",
        "    Respond with a JSON object containing:\n",
        "    {{\"action\": \"ask_clarification\" | \"text_response\" | \"generate_code\", \"reason\": \"brief explanation\"}}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a decision-making agent for a rent roll analysis system. Output ONLY a JSON object with the determined action and reason.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        response_text = response.choices[0].message.content\n",
        "\n",
        "        # Extract JSON from the response\n",
        "        json_match = re.search(r'{.*}', response_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            action_data = json.loads(json_match.group(0))\n",
        "            action = action_data.get(\"action\", \"text_response\")\n",
        "        else:\n",
        "            # Default to text response if parsing fails\n",
        "            action = \"text_response\"\n",
        "\n",
        "        logger.info(f\"Determined action using GPT-4: {action}\")\n",
        "\n",
        "        # Create a new state dict with updated values\n",
        "        new_state = dict(state)  # Create a copy\n",
        "        new_state[\"needs_clarification\"] = action == \"ask_clarification\"\n",
        "        new_state[\"generate_code\"] = action == \"generate_code\"\n",
        "\n",
        "        return new_state\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in determine_action: {e}\")\n",
        "        # Default to text response on error\n",
        "        new_state = dict(state)\n",
        "        new_state[\"needs_clarification\"] = False\n",
        "        new_state[\"generate_code\"] = False\n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IDnMakrcOA4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ask_clarification(state: AgentState) -> Dict:\n",
        "    \"\"\"Generate a clarification question for the user using GPT-4.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    user_message = messages[-1][\"content\"] if messages[-1][\"role\"] == \"user\" else \"\"\n",
        "\n",
        "    # Create OpenAI client for this function call\n",
        "    client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"\"\"You are a commercial real estate rent roll analyst.\n",
        "                Generate a clear, specific clarification question to better understand\n",
        "                what the user is asking about their rent roll data.\"\"\"},\n",
        "                {\"role\": \"user\", \"content\": f\"My question is: {user_message}\"}\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        clarification_question = response.choices[0].message.content\n",
        "\n",
        "        # Create a new state dict with updated values\n",
        "        new_state = dict(state)\n",
        "        new_state[\"clarification_question\"] = clarification_question\n",
        "        new_state[\"final_response\"] = clarification_question\n",
        "\n",
        "        # Add the clarification question to the messages\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": clarification_question})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        logger.info(f\"Generated clarification question using GPT-4: {clarification_question[:50]}...\")\n",
        "        return new_state\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in ask_clarification: {e}\")\n",
        "        # Fallback to a generic clarification question\n",
        "        generic_question = \"Could you please clarify what specific aspect of the rent roll you'd like me to analyze?\"\n",
        "\n",
        "        new_state = dict(state)\n",
        "        new_state[\"clarification_question\"] = generic_question\n",
        "        new_state[\"final_response\"] = generic_question\n",
        "\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": generic_question})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-C3FXmrcQ8J"
      },
      "outputs": [],
      "source": [
        "def generate_text_response(state):\n",
        "    \"\"\"Generate a simple text response to the user query using GPT-4.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    df = state[\"df\"]\n",
        "    issues = state[\"issues\"]\n",
        "\n",
        "    # Create OpenAI client for this function call\n",
        "    client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "    # Prepare context for GPT-4\n",
        "    issues_text = \"\\n\".join([f\"- {issue}\" for issue in issues])\n",
        "\n",
        "    # Get column and data preview for context\n",
        "    if df is not None:\n",
        "        column_info = \", \".join(df.columns)\n",
        "        data_stats = []\n",
        "        for col in df.columns[:10]:  # Limit to first 10 columns to avoid token limits\n",
        "            try:\n",
        "                if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                    stat = f\"- {col}: min={df[col].min()}, max={df[col].max()}, mean={df[col].mean():.2f}, null={df[col].isna().sum()}\"\n",
        "                else:\n",
        "                    unique_vals = df[col].nunique()\n",
        "                    stat = f\"- {col}: unique values={unique_vals}, null={df[col].isna().sum()}\"\n",
        "                data_stats.append(stat)\n",
        "            except:\n",
        "                data_stats.append(f\"- {col}: [error calculating stats]\")\n",
        "        data_stats_str = \"\\n\".join(data_stats)\n",
        "        df_preview = df.head(3).to_string()\n",
        "    else:\n",
        "        column_info = \"No columns available\"\n",
        "        data_stats_str = \"No data statistics available\"\n",
        "        df_preview = \"No data preview available\"\n",
        "\n",
        "    system_prompt = f\"\"\"You are a commercial real estate rent roll analyst.\n",
        "    The rent roll data has {len(df) if df is not None else 0} rows and\n",
        "    {len(df.columns) if df is not None else 0} columns.\n",
        "\n",
        "    Column information: {column_info}\n",
        "\n",
        "    Data statistics:\n",
        "    {data_stats_str}\n",
        "\n",
        "    Data preview:\n",
        "    {df_preview}\n",
        "\n",
        "    Identified issues:\n",
        "    {issues_text}\n",
        "\n",
        "    Provide a concise, informative answer to the user's question.\n",
        "    Focus on being helpful and direct, with only 1-2 paragraphs.\n",
        "    Do not include code or detailed analysis unless absolutely necessary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract system message and filter other messages\n",
        "    filtered_messages = []\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] != \"system\":\n",
        "            filtered_messages.append(msg)\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                *filtered_messages\n",
        "            ],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        text_response = response.choices[0].message.content\n",
        "\n",
        "        # Create a new state dict with updated values\n",
        "        new_state = dict(state)\n",
        "        new_state[\"final_response\"] = text_response\n",
        "\n",
        "        # Add the response to the messages\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": text_response})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        logger.info(f\"Generated text response using GPT-4: {text_response[:50]}...\")\n",
        "        return new_state\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in generate_text_response: {e}\")\n",
        "        # Fallback to a generic response\n",
        "        fallback_response = \"I'm sorry, I'm having trouble analyzing your rent roll data right now. Could you try rephrasing your question?\"\n",
        "\n",
        "        new_state = dict(state)\n",
        "        new_state[\"final_response\"] = fallback_response\n",
        "\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": fallback_response})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiwl_X7iDPVC"
      },
      "outputs": [],
      "source": [
        "def trim_dataframe_output(output_text, max_rows=20, max_chars=None):\n",
        "    \"\"\"\n",
        "    Extremely simplified function that just returns the first 20 lines of output.\n",
        "\n",
        "    Args:\n",
        "        output_text: The text output\n",
        "        max_rows: Maximum number of rows to keep (default: 20)\n",
        "        max_chars: Not used, kept for compatibility\n",
        "\n",
        "    Returns:\n",
        "        Trimmed text showing only top rows\n",
        "    \"\"\"\n",
        "    lines = output_text.split('\\n')\n",
        "\n",
        "    if len(lines) <= max_rows:\n",
        "        return output_text\n",
        "\n",
        "    trimmed_lines = lines[:max_rows]\n",
        "    trimmed_lines.append(f\"... [output truncated, showing first {max_rows} lines only] ...\")\n",
        "\n",
        "    return '\\n'.join(trimmed_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeQetycQblZd"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datetime import datetime\n",
        "def save_dataframe_version(df, operation_description=\"\"):\n",
        "    \"\"\"Save the current state of the dataframe as both CSV and Excel files.\n",
        "\n",
        "    Args:\n",
        "        df: The dataframe to save\n",
        "        operation_description: A string describing what operation was performed\n",
        "\n",
        "    Returns:\n",
        "        version_name: The name of the version that was saved\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create versions directory if it doesn't exist\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "    os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "    # Generate version name with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    version_name = f\"v_{timestamp}\"\n",
        "\n",
        "    # Create filenames for both CSV and Excel\n",
        "    csv_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.csv\")\n",
        "    excel_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.xlsx\")\n",
        "\n",
        "    # Save as CSV\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    # Save as Excel\n",
        "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "\n",
        "    # Add version metadata to the registry\n",
        "    if 'app_state' in globals():\n",
        "        version_info = {\n",
        "            'name': version_name,\n",
        "            'description': operation_description,\n",
        "            'timestamp': timestamp,\n",
        "            'filename': csv_filename,  # Keep CSV as primary for backward compatibility\n",
        "            'excel_filename': excel_filename,  # Add Excel filename\n",
        "            'is_original': len(app_state[\"df_versions\"]) == 0  # First one is original\n",
        "        }\n",
        "        app_state[\"df_versions\"].append(version_info)\n",
        "\n",
        "    print(f\"✓ Saved dataframe version {version_name}: {operation_description}\")\n",
        "    print(f\"  - CSV: {csv_filename}\")\n",
        "    print(f\"  - Excel: {excel_filename}\")\n",
        "\n",
        "    # Return the version name for reference\n",
        "    return version_name\n",
        "\n",
        "def get_versions_info_for_prompt():\n",
        "    \"\"\"Generate version information for the Claude prompt.\"\"\"\n",
        "    if not app_state[\"df_versions\"]:\n",
        "        return \"No versions available yet.\"\n",
        "\n",
        "    # Find the original version\n",
        "    original = next((v for v in app_state[\"df_versions\"] if v.get('is_original')), app_state[\"df_versions\"][0])\n",
        "\n",
        "    # Get the latest version\n",
        "    latest = app_state[\"df_versions\"][-1]\n",
        "\n",
        "    # Format all versions\n",
        "    all_versions = []\n",
        "    for i, version in enumerate(app_state[\"df_versions\"]):\n",
        "        status = []\n",
        "        if version == original:\n",
        "            status.append(\"ORIGINAL\")\n",
        "        if version == latest:\n",
        "            status.append(\"LATEST\")\n",
        "\n",
        "        status_str = f\" ({', '.join(status)})\" if status else \"\"\n",
        "        all_versions.append(f\"{i+1}. {version['name']}{status_str}: {version['description']}\")\n",
        "\n",
        "    versions_text = \"\\n\".join(all_versions)\n",
        "\n",
        "    return f\"\"\"\n",
        "DATAFRAME VERSION HISTORY:\n",
        "{versions_text}\n",
        "\n",
        "Original version: {original['name']}\n",
        "Latest version: {latest['name']}\n",
        "Total versions: {len(app_state[\"df_versions\"])}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bMY3yPyjw6z"
      },
      "outputs": [],
      "source": [
        "def generate_code_and_execute(state: AgentState) -> Dict:\n",
        "    \"\"\"\n",
        "    Generate and execute code using a two-step AI approach:\n",
        "    1. Use GPT-4 to create an optimal prompt for Claude\n",
        "    2. Have Claude generate the code based on this optimized prompt\n",
        "    3. Execute the code and handle errors with up to 3 retries\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    df = state[\"df\"]\n",
        "\n",
        "    # Get OpenAI client from state or create new one\n",
        "    openai_client = state.get(\"openai_client\") or OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "    # Get Anthropic client from state or create new one\n",
        "    anthropic_client = state.get(\"anthropic_client\") or Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "    # Get column information for context\n",
        "    column_info = \", \".join(df.columns) if df is not None else \"No columns available\"\n",
        "\n",
        "    # Create SAMPLE dataframe content for GPT-4.1 (first 50 rows instead of full dataset)\n",
        "    if df is not None:\n",
        "        # Get first 50 rows for GPT-4.1 context\n",
        "        sample_df = df.head(50)\n",
        "\n",
        "        # Convert sample dataframe to string representation\n",
        "        df_sample_content = sample_df.to_string(index=False)\n",
        "\n",
        "        # Also get CSV format for better structure\n",
        "        df_csv_content = sample_df.to_csv(index=False)\n",
        "\n",
        "        # Prepare comprehensive data summary with sample data\n",
        "        df_summary = f\"\"\"\n",
        "SAMPLE DATAFRAME CONTENT (First 50 rows out of {len(df)} total rows):\n",
        "{df_sample_content}\n",
        "\n",
        "CSV FORMAT (First 50 rows):\n",
        "{df_csv_content}\n",
        "\n",
        "FULL DATAFRAME STATISTICS:\n",
        "- Shape: {df.shape}\n",
        "- Columns: {list(df.columns)}\n",
        "- Data types: {dict(df.dtypes)}\n",
        "- Memory usage: {df.memory_usage(deep=True).sum()} bytes\n",
        "- Null values per column: {dict(df.isnull().sum())}\n",
        "\n",
        "NOTE: This is a sample of the first 50 rows. The complete dataframe has {len(df)} rows.\n",
        "\"\"\"\n",
        "    else:\n",
        "        df_summary = \"No data available\"\n",
        "\n",
        "    # Create versions directory if it doesn't exist\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "    os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "    # Print initial state for debugging\n",
        "    print(f\"\\n==== STARTING CODE GENERATION ====\")\n",
        "    print(f\"User query: {messages[-1]['content'] if messages[-1]['role'] == 'user' else 'No user query found'}\")\n",
        "    print(f\"Dataframe has {len(df) if df is not None else 0} rows and {len(df.columns) if df is not None else 0} columns\")\n",
        "    print(f\"Sending FIRST 50 ROWS to GPT-4.1 (sample instead of full dataset)\")\n",
        "\n",
        "    try:\n",
        "        # First, use GPT-4 to create the optimal prompt for Claude\n",
        "        print(\"\\n==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH SAMPLE OF 50 ROWS) ====\")\n",
        "        versions_info = get_versions_info_for_prompt()\n",
        "\n",
        "        # System prompt for GPT-4 to create a Claude prompt\n",
        "        gpt_system_prompt = f\"\"\"You are an expert at creating prompts for Claude AI to generate code.\n",
        "        Your task is to analyze the user query history and convert it into an optimal prompt for Claude to generate Python code that analyzes a rent roll dataframe.\n",
        "\n",
        "        CRITICAL INFORMATION: The dataframe is ALREADY LOADED and available as 'df'.\n",
        "        It contains REAL DATA with {len(df)} rows and {len(df.columns)} columns.\n",
        "\n",
        "        HERE IS A SAMPLE OF THE DATAFRAME CONTENT (FIRST 50 ROWS OUT OF {len(df)} TOTAL ROWS):\n",
        "        {df_summary}\n",
        "\n",
        "        IMPORTANT: This is only a sample of the first 50 rows to give you context about the data structure and content.\n",
        "        The actual dataframe that Claude will work with contains ALL {len(df)} rows.\n",
        "\n",
        "        # IMPORTANT: DATAFRAME VERSION MANAGEMENT\n",
        "        {versions_info}\n",
        "\n",
        "        # IMPORTANT VERSION IDENTIFICATION:\n",
        "        - Versions are stored in chronological order by timestamp\n",
        "        - The original version is always the first one saved (earliest timestamp)\n",
        "        - The latest version is always the most recent one saved (latest timestamp)\n",
        "        - When a user says \"original dataframe,\" load the version with the earliest timestamp\n",
        "        - When a user says \"latest version,\" use the current df (which is already the latest)\n",
        "        - When a user specifies a version by name (e.g., \"v_20250518_112345\"), load that exact version\n",
        "\n",
        "        ALL versions are saved as CSV files in the \"rent_roll_versions\" directory.\n",
        "        For example, to load a specific version:\n",
        "\n",
        "        ```python\n",
        "        # To load a specific version (e.g., the original version)\n",
        "        import pandas as pd\n",
        "        import os\n",
        "\n",
        "        # Example: Load the original version\n",
        "        original_version_name = \"{{app_state[\"df_versions\"][0]['name'] if app_state[\"df_versions\"] else \"v_example\"}}\"\n",
        "        original_file_path = os.path.join(\"rent_roll_versions\", f\"rent_roll_{{original_version_name}}.csv\")\n",
        "        original_df = pd.read_csv(original_file_path)\n",
        "\n",
        "        print(f\"Loaded original version: {{original_version_name}}\")\n",
        "        print(f\"Shape: {{original_df.shape}}\")\n",
        "\n",
        "        # You can either work with this as a separate dataframe, or replace the current df:\n",
        "        # df = original_df  # This would replace the current df with the original\n",
        "        ```\n",
        "\n",
        "        If you make any changes to the dataframe, ALWAYS save a new version using save_dataframe_version().\n",
        "\n",
        "        Some important guidelines to include in your prompt to Claude:\n",
        "        1. The variable 'df' is ALREADY DEFINED and CONTAINS ALL {len(df)} ROWS OF DATA. Claude must not say \"I need to see the data first\"\n",
        "        2. Claude should explain its approach step by step before showing code\n",
        "        3. Code must be wrapped in ```python and ``` blocks\n",
        "        4. Code MUST display ALL rows in the output when showing tables (no limiting rows)\n",
        "        5. Claude should not attempt to clean data unless specifically requested\n",
        "        6. Code should include proper error handling\n",
        "        7. IMPORTANT: After performing any analysis or showing results, Claude should ALWAYS call the save_dataframe_version() function to maintain version history, even if no changes were made to the dataframe.\n",
        "        8. CRITICAL: Claude should NOT use try-except blocks in its code. Any errors should be allowed to propagate naturally. This ensures that our retry system can properly handle errors.\n",
        "\n",
        "        Your output will be directly sent to Claude, so format it as a complete system prompt.\n",
        "        Include any table formatting functions that might be useful.\n",
        "\n",
        "        Make sure to include these helper functions in your prompt:\n",
        "\n",
        "        ```python\n",
        "        # For tabular display with proper formatting (PREFERRED METHOD):\n",
        "        def print_formatted_table(df, title=None): #Print a dataframe with proper formatting without modifying data\n",
        "            if title:\n",
        "                print(f\"\\\\n{{title}}\")\n",
        "                print(\"=\" * 80)\n",
        "\n",
        "            # Create a display copy (doesn't change original df)\n",
        "            display_df = df.copy()\n",
        "\n",
        "            # Set pandas display options for better readability\n",
        "            # Show ALL rows - no limits\n",
        "            pd.set_option('display.max_rows', None)\n",
        "            pd.set_option('display.max_columns', None)\n",
        "            pd.set_option('display.width', 1000)\n",
        "            pd.set_option('display.colheader_justify', 'left')\n",
        "            pd.set_option('display.precision', 2)\n",
        "\n",
        "            # Display the dataframe - ALL rows will be shown\n",
        "            print(display_df)\n",
        "\n",
        "            # Reset display options to default\n",
        "            pd.reset_option('display.max_rows')\n",
        "            pd.reset_option('display.max_columns')\n",
        "            pd.reset_option('display.width')\n",
        "            pd.reset_option('display.colheader_justify')\n",
        "            pd.reset_option('display.precision')\n",
        "        ```\n",
        "\n",
        "        ```python\n",
        "        # For bordered table display with precise control:\n",
        "        def print_bordered_table(df, title=None): #Print a dataframe with borders for better readability - SHOWS ALL ROWS\n",
        "            if title:\n",
        "                print(f\"\\\\n{{title}}\")\n",
        "                print(\"=\" * 80)\n",
        "\n",
        "            if len(df) == 0:\n",
        "                print(\"No data available\")\n",
        "                return\n",
        "\n",
        "            # Create a display copy (doesn't change original data)\n",
        "            display_df = df.copy()\n",
        "\n",
        "            # Calculate column widths for display purposes only\n",
        "            col_widths = {{}}\n",
        "            for col in display_df.columns:\n",
        "                # Convert values to string only for width calculation\n",
        "                col_values = display_df[col].astype(str)\n",
        "                max_data_width = col_values.str.len().max()\n",
        "                col_widths[col] = max(len(str(col)), max_data_width) + 2  # +2 for padding\n",
        "\n",
        "            # Create header row\n",
        "            header = \"| \" + \" | \".join(str(col).ljust(col_widths[col]) for col in display_df.columns) + \" |\"\n",
        "            separator = \"+\" + \"+\".join(\"-\" * (col_widths[col] + 2) for col in display_df.columns) + \"+\"\n",
        "\n",
        "            # Print header\n",
        "            print(separator)\n",
        "            print(header)\n",
        "            print(separator)\n",
        "\n",
        "            # Print ALL rows - NO LIMIT\n",
        "            for i in range(len(display_df)):\n",
        "                row = display_df.iloc[i]\n",
        "                row_str = \"| \" + \" | \".join(str(val).ljust(col_widths[col]) for col, val in row.items()) + \" |\"\n",
        "                print(row_str)\n",
        "\n",
        "            print(separator)\n",
        "            print(f\"Total rows: {{len(display_df)}}\")\n",
        "        ```\n",
        "\n",
        "        ```python\n",
        "        # Function to save dataframe versions\n",
        "        def save_dataframe_version(df, operation_description=\"\"):\n",
        "            \\\"\\\"\\\"Save the current state of the dataframe as both CSV and Excel files.\n",
        "\n",
        "            This function should be called whenever you make changes to the dataframe,\n",
        "            or after generating analysis results, to maintain version history.\n",
        "\n",
        "            Args:\n",
        "                df: The dataframe to save\n",
        "                operation_description: A string describing what operation was performed\n",
        "\n",
        "            Returns:\n",
        "                version_name: The name of the version that was saved\n",
        "            \\\"\\\"\\\"\n",
        "            import os\n",
        "            from datetime import datetime\n",
        "\n",
        "            # Create versions directory if it doesn't exist\n",
        "            versions_dir = \"rent_roll_versions\"\n",
        "            os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "            # Generate version name with timestamp\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            version_name = f\"v_{{timestamp}}\"\n",
        "\n",
        "            # Create filenames for both CSV and Excel\n",
        "            csv_filename = os.path.join(versions_dir, f\"rent_roll_{{version_name}}.csv\")\n",
        "            excel_filename = os.path.join(versions_dir, f\"rent_roll_{{version_name}}.xlsx\")\n",
        "\n",
        "            # Save as CSV\n",
        "            df.to_csv(csv_filename, index=False)\n",
        "\n",
        "            # Save as Excel\n",
        "            df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "\n",
        "            print(f\"✓ Saved dataframe version {{version_name}}: {{operation_description}}\")\n",
        "            print(f\"  - CSV: {{csv_filename}}\")\n",
        "            print(f\"  - Excel: {{excel_filename}}\")\n",
        "\n",
        "            # Return the version name for reference\n",
        "            return version_name\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        # Filter out system messages and DON'T trim dataframe outputs in the conversation history\n",
        "        filtered_messages = []\n",
        "        for msg in messages:\n",
        "            if msg[\"role\"] != \"system\":\n",
        "                # Don't trim here anymore\n",
        "                filtered_messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
        "\n",
        "        # Convert the messages to the format expected by OpenAI\n",
        "        gpt_messages = [{\"role\": \"system\", \"content\": gpt_system_prompt}]\n",
        "        for msg in filtered_messages:\n",
        "            gpt_messages.append(msg)\n",
        "\n",
        "        # Add a final message explaining the task clearly\n",
        "        gpt_messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Based on this conversation history and the SAMPLE dataframe content (first 50 rows) provided above, create the optimal Claude prompt to generate Python code for rent roll analysis. The prompt should emphasize that the dataframe already exists and is loaded as 'df' with ALL {len(df)} rows, that ALL rows should be displayed when requested, and that versions should be saved with save_dataframe_version() function. You have access to a representative sample of the data structure.\"\n",
        "        })\n",
        "\n",
        "        # Get the optimized prompt from GPT-4\n",
        "        gpt_response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=gpt_messages,\n",
        "            max_tokens=4000,  # Increased token limit to handle larger dataframes\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        claude_system_prompt = gpt_response.choices[0].message.content\n",
        "\n",
        "        # Print the generated prompt for debugging\n",
        "        print(\"\\n==== GPT-4 GENERATED PROMPT FOR CLAUDE ====\")\n",
        "        print(claude_system_prompt[:500] + \"...\" if len(claude_system_prompt) > 500 else claude_system_prompt)\n",
        "        print(\"==== END OF PROMPT (TRUNCATED) ====\\n\")\n",
        "\n",
        "        logger.info(\"Generated optimized prompt for Claude using GPT-4 with sample dataframe (50 rows)\")\n",
        "\n",
        "        # Now use the GPT-4 generated prompt to ask Claude for code\n",
        "        print(\"\\n==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION ====\")\n",
        "        logger.info(\"Sending optimized prompt to Claude for code generation\")\n",
        "\n",
        "        # Prepare messages for Claude with the sample dataframe content\n",
        "        claude_messages = filtered_messages.copy()\n",
        "\n",
        "        # Add the sample dataframe content to help Claude understand the data exists\n",
        "        sample_data_message = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Here is a SAMPLE of the dataframe that's already loaded as 'df' (showing first 50 rows out of {len(df)} total rows):\\n\\n{df_summary}\\n\\nPlease process my request using this FULL dataset of {len(df)} rows and remember to save versions with save_dataframe_version().\"\n",
        "        }\n",
        "        claude_messages.append(sample_data_message)\n",
        "\n",
        "        # Try to get code from Claude\n",
        "        claude_response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            system=claude_system_prompt,\n",
        "            messages=claude_messages,\n",
        "            max_tokens=4000,  # Increased to handle more complex responses\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Extract the response text from Claude\n",
        "        response_text = claude_response.content[0].text\n",
        "\n",
        "        # Print Claude's response for debugging\n",
        "        print(\"\\n==== CLAUDE'S RESPONSE ====\")\n",
        "        print(response_text[:500] + \"...\" if len(response_text) > 500 else response_text)\n",
        "        print(\"==== END OF CLAUDE RESPONSE (TRUNCATED) ====\\n\")\n",
        "\n",
        "        # Extract code blocks\n",
        "        code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "\n",
        "        # Print extracted code blocks for debugging\n",
        "        print(f\"\\n==== EXTRACTED {len(code_blocks)} CODE BLOCKS ====\")\n",
        "        for i, block in enumerate(code_blocks):\n",
        "            print(f\"\\n-- Code Block {i+1} --\")\n",
        "            print(block[:200] + \"...\" if len(block) > 200 else block)\n",
        "\n",
        "        # If no code blocks are found, add emergency code\n",
        "        if len(code_blocks) == 0:\n",
        "            emergency_code = \"\"\"\n",
        "            # Emergency code to display the dataframe\n",
        "            pd.set_option('display.max_rows', None)\n",
        "            pd.set_option('display.max_columns', None)\n",
        "            pd.set_option('display.width', 1000)\n",
        "\n",
        "            print(\"\\\\n=== RENT ROLL DATA ===\\\\n\")\n",
        "            print(f\"Displaying all {len(df)} rows and {len(df.columns)} columns\\\\n\")\n",
        "\n",
        "            # Print the entire dataframe\n",
        "            print(df)\n",
        "\n",
        "            # Save a version of the dataframe\n",
        "            from datetime import datetime\n",
        "            import os\n",
        "\n",
        "            # Create versions directory if it doesn't exist\n",
        "            versions_dir = \"rent_roll_versions\"\n",
        "            os.makedirs(versions_dir, exist_ok=True)\n",
        "\n",
        "            # Generate version name with timestamp\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            version_name = f\"v_{timestamp}\"\n",
        "\n",
        "            # Create filename\n",
        "            filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.csv\")\n",
        "\n",
        "            # Save dataframe\n",
        "            df.to_csv(filename, index=False)\n",
        "\n",
        "            print(f\"✓ Saved dataframe version {version_name}: Emergency display of data\")\n",
        "            \"\"\"\n",
        "            code_blocks.append(emergency_code)\n",
        "            print(\"\\n-- Added Emergency Code Block --\")\n",
        "            print(\"Emergency code added since Claude didn't generate code\")\n",
        "\n",
        "        # Rest of the function remains the same (helper functions, execution loop, etc.)\n",
        "        # Define helper functions\n",
        "        def print_formatted_table(df, title=None):\n",
        "            if title:\n",
        "                print(f\"\\n{title}\")\n",
        "                print(\"=\" * 80)\n",
        "\n",
        "            # Create a display copy (doesn't change original df)\n",
        "            display_df = df.copy()\n",
        "\n",
        "            # Set pandas display options for better readability\n",
        "            # Show ALL rows - no limits\n",
        "            pd.set_option('display.max_rows', None)\n",
        "            pd.set_option('display.max_columns', None)\n",
        "            pd.set_option('display.width', 1000)\n",
        "            pd.set_option('display.colheader_justify', 'left')\n",
        "            pd.set_option('display.precision', 2)\n",
        "\n",
        "            # Display the dataframe - ALL rows will be shown\n",
        "            print(display_df)\n",
        "\n",
        "            # Reset display options to default\n",
        "            pd.reset_option('display.max_rows')\n",
        "            pd.reset_option('display.max_columns')\n",
        "            pd.reset_option('display.width')\n",
        "            pd.reset_option('display.colheader_justify')\n",
        "            pd.reset_option('display.precision')\n",
        "\n",
        "        def print_bordered_table(df, title=None):\n",
        "            if title:\n",
        "                print(f\"\\n{title}\")\n",
        "                print(\"=\" * 80)\n",
        "\n",
        "            if len(df) == 0:\n",
        "                print(\"No data available\")\n",
        "                return\n",
        "\n",
        "            # Create a display copy (doesn't change original data)\n",
        "            display_df = df.copy()\n",
        "\n",
        "            # Calculate column widths for display purposes only\n",
        "            col_widths = {}\n",
        "            for col in display_df.columns:\n",
        "                # Convert values to string only for width calculation\n",
        "                col_values = display_df[col].astype(str)\n",
        "                max_data_width = col_values.str.len().max()\n",
        "                col_widths[col] = max(len(str(col)), max_data_width) + 2  # +2 for padding\n",
        "\n",
        "            # Create header row\n",
        "            header = \"| \" + \" | \".join(str(col).ljust(col_widths[col]) for col in display_df.columns) + \" |\"\n",
        "            separator = \"+\" + \"+\".join(\"-\" * (col_widths[col] + 2) for col in display_df.columns) + \"+\"\n",
        "\n",
        "            # Print header\n",
        "            print(separator)\n",
        "            print(header)\n",
        "            print(separator)\n",
        "\n",
        "            # Print ALL rows - NO LIMIT\n",
        "            for i in range(len(display_df)):\n",
        "                row = display_df.iloc[i]\n",
        "                row_str = \"| \" + \" | \".join(str(val).ljust(col_widths[col]) for col, val in row.items()) + \" |\"\n",
        "                print(row_str)\n",
        "\n",
        "            print(separator)\n",
        "            print(f\"Total rows: {len(display_df)}\")\n",
        "\n",
        "        # Add to globals_dict before executing code\n",
        "        globals_dict = {\n",
        "            \"df\": df,\n",
        "            \"pd\": pd,\n",
        "            \"np\": np,\n",
        "            \"os\": os,                   # Add os for folder creation\n",
        "            \"datetime\": datetime,       # Add datetime for timestamp\n",
        "            \"versions_dir\": versions_dir,  # Pass the versions directory\n",
        "            \"print_formatted_table\": print_formatted_table,  # Add the helper function\n",
        "            \"print_bordered_table\": print_bordered_table,    # Add the helper function\n",
        "            \"save_dataframe_version\": save_dataframe_version  # Make sure this is defined too\n",
        "        }\n",
        "\n",
        "        execution_results = \"\"\n",
        "        all_executed_successfully = False\n",
        "        max_retries = 5  # Maximum number of retries\n",
        "        retry_count = 0  # Initialize retry counter\n",
        "        failed_code = \"\"  # Store the failed code for context\n",
        "        error_msg = \"\"    # Store the error message\n",
        "\n",
        "        print(\"\\n==== STEP 3: EXECUTING CODE WITH RETRIES ====\")\n",
        "\n",
        "        # Main retry loop (rest of the execution code remains the same)\n",
        "        while not all_executed_successfully and retry_count <= max_retries:\n",
        "            # If this is a retry attempt (not the first try)\n",
        "            if retry_count > 0:\n",
        "                print(f\"\\n==== RETRY ATTEMPT {retry_count}/{max_retries} ====\")\n",
        "\n",
        "                # Create a retry message with more details each time\n",
        "                retry_message = {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"The code you provided failed with this error: {error_msg}\n",
        "\n",
        "                    Here is the code that failed:\n",
        "                    ```python\n",
        "                    {failed_code}\n",
        "                    ```\n",
        "\n",
        "                    This is retry attempt {retry_count} of {max_retries}.\n",
        "\n",
        "                    {\"After multiple attempts, please try a completely different approach.\" if retry_count >= 2 else \"Please fix this specific error.\"}\n",
        "                    IMPORTANT: DO NOT use try-except blocks in your code. Allow any errors to propagate naturally so our system can detect them.\n",
        "                    Please fix this code to handle the specific error while maintaining the requirement to show ALL rows in the output and saving a version with save_dataframe_version().\n",
        "                    Return the corrected code wrapped in ```python and ``` blocks.\"\"\"\n",
        "                }\n",
        "\n",
        "                # Add this feedback to the messages\n",
        "                fix_messages = claude_messages.copy()\n",
        "                fix_messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "                fix_messages.append(retry_message)\n",
        "\n",
        "                # Get Claude's fixed code\n",
        "                retry_response = anthropic_client.messages.create(\n",
        "                    model=\"claude-3-7-sonnet-20250219\",\n",
        "                    system=claude_system_prompt,\n",
        "                    messages=fix_messages,\n",
        "                    max_tokens=3500,\n",
        "                    temperature=0.3\n",
        "                )\n",
        "\n",
        "                retry_text = retry_response.content[0].text\n",
        "                print(f\"\\n==== CLAUDE'S FIX SUGGESTION (ATTEMPT {retry_count}) ====\")\n",
        "                print(retry_text[:500] + \"...\" if len(retry_text) > 500 else retry_text)\n",
        "\n",
        "                # Extract the fixed code blocks\n",
        "                fixed_code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', retry_text, re.DOTALL)\n",
        "\n",
        "                if fixed_code_blocks:\n",
        "                    # Use the first fixed code block\n",
        "                    code_to_execute = fixed_code_blocks[0]\n",
        "\n",
        "                    # Update response text to include the fix explanation\n",
        "                    fix_explanation = f\"\\n\\n**🔧 Code Fix (Attempt {retry_count}):**\\n\"\n",
        "                    fix_explanation += f\"The code encountered an error. Here's the fix for retry attempt {retry_count}:\\n\"\n",
        "                    fix_explanation += \"\\n```python\\n\" + code_to_execute + \"\\n```\\n\"\n",
        "\n",
        "                    if retry_count == 1:\n",
        "                        # First retry - add to original response\n",
        "                        response_text = response_text + fix_explanation\n",
        "                    else:\n",
        "                        # Subsequent retries - replace previous fix explanation\n",
        "                        prev_fix_marker = f\"**🔧 Code Fix (Attempt {retry_count-1}):**\"\n",
        "                        if prev_fix_marker in response_text:\n",
        "                            # Replace previous fix with new one\n",
        "                            response_text = response_text.replace(\n",
        "                                prev_fix_marker,\n",
        "                                f\"**🔧 Code Fix (Attempt {retry_count}):**\"\n",
        "                            )\n",
        "                        else:\n",
        "                            # Just append this fix\n",
        "                            response_text = response_text + fix_explanation\n",
        "                else:\n",
        "                    # If no code blocks found in retry, try emergency code\n",
        "                    code_to_execute = f\"\"\"\n",
        "                    # Emergency code for retry {retry_count}\n",
        "                    print(f\"\\\\n=== EMERGENCY DISPLAY (RETRY {retry_count}) ===\\\\n\")\n",
        "                    print(f\"DataFrame shape: {{df.shape}}\")\n",
        "                    print(\"\\\\nColumn names:\")\n",
        "                    for col in df.columns:\n",
        "                        print(f\"- {{col}}\")\n",
        "\n",
        "                    print(\"\\\\nFirst 10 rows:\")\n",
        "                    print(df.head(10))\n",
        "\n",
        "                    save_dataframe_version(df, f\"Emergency display after retry {retry_count}\")\n",
        "                    \"\"\"\n",
        "                    print(f\"No code blocks found in retry. Using emergency code.\")\n",
        "            else:\n",
        "                # Initial execution (not a retry)\n",
        "                # Run the original code block\n",
        "                if code_blocks:\n",
        "                    code_to_execute = code_blocks[0]  # Use the first code block\n",
        "                else:\n",
        "                    # This should not happen due to the earlier check, but just in case\n",
        "                    code_to_execute = \"\"\"\n",
        "                    print(\"No code blocks found. Displaying basic dataframe info.\")\n",
        "                    print(f\"DataFrame shape: {df.shape}\")\n",
        "                    print(df.head())\n",
        "                    save_dataframe_version(df, \"Automatic save after initial execution\")\n",
        "                    \"\"\"\n",
        "\n",
        "            # Execute the current code\n",
        "            print(f\"\\n{'Executing' if retry_count == 0 else 'Retrying'} code...\")\n",
        "            output_buffer = io.StringIO()\n",
        "            try:\n",
        "                # Store the code in case it fails\n",
        "                failed_code = code_to_execute\n",
        "\n",
        "                with redirect_stdout(output_buffer):\n",
        "                    exec(code_to_execute, globals_dict)\n",
        "\n",
        "                execution_output = output_buffer.getvalue()\n",
        "                print(f\"Execution {'successful' if retry_count == 0 else 'fixed on retry ' + str(retry_count)}! Output length: {len(execution_output)} characters\")\n",
        "                print(execution_output[:200] + \"...\" if len(execution_output) > 200 else execution_output)\n",
        "\n",
        "                # ONLY trim the execution output for storing, not the entire response\n",
        "                trimmed_output = trim_dataframe_output(execution_output, max_rows=20)\n",
        "\n",
        "                # Format the results message based on retry count\n",
        "                if retry_count == 0:\n",
        "                    results_msg = \"**✅ Code Execution Results:**\"\n",
        "                else:\n",
        "                    results_msg = f\"**✅ Code Execution Results (After Fix Attempt {retry_count}):**\"\n",
        "\n",
        "                execution_results = f\"\\n\\n{results_msg}\\n```\\n{trimmed_output}\\n```\\n\"\n",
        "\n",
        "                # Check if a version was saved\n",
        "                if \"✓ Saved dataframe version\" not in execution_output:\n",
        "                    # Auto-save a version\n",
        "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                    version_name = f\"v_{timestamp}\"\n",
        "                    csv_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.csv\")\n",
        "                    excel_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.xlsx\")\n",
        "\n",
        "                    # Save both CSV and Excel\n",
        "                    df.to_csv(csv_filename, index=False)\n",
        "                    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "\n",
        "                    save_message = f\"✓ Saved dataframe version {version_name}: Automatic save after {'execution' if retry_count == 0 else 'retry ' + str(retry_count)}\"\n",
        "                    print(save_message)\n",
        "                    print(f\"  - CSV: {csv_filename}\")\n",
        "                    print(f\"  - Excel: {excel_filename}\")\n",
        "                    execution_results += f\"\\n{save_message}\\n\"\n",
        "\n",
        "                # Mark as successful and break the retry loop\n",
        "                all_executed_successfully = True\n",
        "                logger.info(f\"Successfully executed code {'' if retry_count == 0 else 'on retry ' + str(retry_count)}\")\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "                # Execution failed\n",
        "                error_msg = f\"Error: {str(e)}\"\n",
        "                print(f\"Execution failed with error: {error_msg}\")\n",
        "\n",
        "                # Log the error\n",
        "                if retry_count == 0:\n",
        "                    execution_results = f\"\\n\\n**❌ Code Execution Failed:**\\n```\\n{error_msg}\\n```\\n\"\n",
        "                else:\n",
        "                    execution_results = f\"\\n\\n**❌ Code Execution Failed (Retry {retry_count}):**\\n```\\n{error_msg}\\n```\\n\"\n",
        "\n",
        "                logger.error(f\"Code execution failed on {'initial attempt' if retry_count == 0 else 'retry ' + str(retry_count)}: {e}\")\n",
        "                logger.error(traceback.format_exc())\n",
        "\n",
        "                # Increment retry counter\n",
        "                retry_count += 1\n",
        "\n",
        "                # If we've hit max retries and still failed, try emergency display as last resort\n",
        "                if retry_count > max_retries:\n",
        "                    print(\"\\n==== MAX RETRIES REACHED, TRYING EMERGENCY DISPLAY ====\")\n",
        "\n",
        "                    # Create emergency display code\n",
        "                    emergency_code = \"\"\"\n",
        "                    try:\n",
        "                        print(\"\\\\n=== EMERGENCY FALLBACK DISPLAY ===\\\\n\")\n",
        "                        print(f\"DataFrame shape: {df.shape}\")\n",
        "                        print(\"\\\\nColumn names:\")\n",
        "                        for col in df.columns:\n",
        "                            print(f\"- {col}\")\n",
        "\n",
        "                        print(\"\\\\nFirst 10 rows:\")\n",
        "                        print(df.head(10))\n",
        "\n",
        "                        # Try to show some basic stats about numeric columns\n",
        "                        try:\n",
        "                            numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "                            if len(numeric_cols) > 0:\n",
        "                                print(\"\\\\nBasic statistics for numeric columns:\")\n",
        "                                print(df[numeric_cols].describe())\n",
        "                        except Exception as stats_err:\n",
        "                            print(f\"Could not generate statistics: {stats_err}\")\n",
        "\n",
        "                        # Save version - both CSV and Excel\n",
        "                        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                        version_name = f\"v_{timestamp}_emergency\"\n",
        "                        csv_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.csv\")\n",
        "                        excel_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.xlsx\")\n",
        "\n",
        "                        df.to_csv(csv_filename, index=False)\n",
        "                        df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "\n",
        "                        print(f\"✓ Saved dataframe version {version_name}: Emergency display after all retries failed\")\n",
        "                        print(f\"  - CSV: {csv_filename}\")\n",
        "                        print(f\"  - Excel: {excel_filename}\")\n",
        "                    except Exception as e_inner:\n",
        "                        print(f\"Even emergency display failed: {e_inner}\")\n",
        "                    \"\"\"\n",
        "\n",
        "                    output_buffer = io.StringIO()\n",
        "                    try:\n",
        "                        with redirect_stdout(output_buffer):\n",
        "                            exec(emergency_code, globals_dict)\n",
        "\n",
        "                        emergency_output = output_buffer.getvalue()\n",
        "                        # Only trim the emergency output, not the whole response\n",
        "                        execution_results += f\"\\n\\n**⚠️ Emergency Data Display (After {max_retries} Failed Retries):**\\n```\\n{trim_dataframe_output(emergency_output, max_rows=20)}\\n```\\n\"\n",
        "                    except Exception as e_final:\n",
        "                        print(f\"Emergency fallback also failed: {e_final}\")\n",
        "                        execution_results += f\"\\n\\n**❌ All Recovery Attempts Failed**\\n\"\n",
        "\n",
        "        # Add a note about the hybrid approach and retry attempts\n",
        "        if retry_count > 0 and all_executed_successfully:\n",
        "            hybrid_note = f\"\\n\\n**📝 Note:** This analysis was performed using a hybrid approach with GPT-4 and Claude. The code was successfully fixed after {retry_count} retry attempts. GPT-4 received the complete dataframe for optimal context.\"\n",
        "        elif retry_count > max_retries:\n",
        "            hybrid_note = f\"\\n\\n**📝 Note:** This analysis was attempted using a hybrid approach with GPT-4 and Claude, but all {max_retries} retry attempts failed. Some basic information was displayed as a fallback.\"\n",
        "        else:\n",
        "            hybrid_note = \"\\n\\n**📝 Note:** This analysis was performed using a hybrid approach: GPT-4 optimized the prompt with full dataframe context, and Claude generated and executed the code for detailed rent roll analysis.\"\n",
        "\n",
        "        # Combine the response and execution results\n",
        "        full_response = response_text + execution_results + hybrid_note\n",
        "\n",
        "        print(\"\\n==== FINAL RESPONSE GENERATED ====\")\n",
        "        print(f\"Original response length: {len(full_response)} characters\")\n",
        "        print(f\"Retry attempts: {retry_count}\")\n",
        "        print(f\"Execution successful: {all_executed_successfully}\")\n",
        "\n",
        "        # Create a new state dict with updated values\n",
        "        new_state = dict(state)\n",
        "        new_state[\"code_execution_results\"] = execution_results\n",
        "        new_state[\"final_response\"] = full_response  # Don't trim the full response\n",
        "\n",
        "        # Add the response to the messages - don't trim it here either\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        logger.info(\"Code generation and execution complete using hybrid GPT-4/Claude approach with full dataframe\")\n",
        "        print(\"\\n==== CODE GENERATION COMPLETE ====\")\n",
        "\n",
        "        return new_state\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in hybrid code generation: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        print(f\"\\n==== ERROR IN CODE GENERATION ====\\n{e}\\n{traceback.format_exc()}\")\n",
        "\n",
        "        # Try to save a version even on error\n",
        "        try:\n",
        "            # Generate version name with timestamp\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            version_name = f\"v_{timestamp}_system_error\"\n",
        "\n",
        "            # Create filenames\n",
        "            csv_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.csv\")\n",
        "            excel_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.xlsx\")\n",
        "\n",
        "            # Save both formats\n",
        "            df.to_csv(csv_filename, index=False)\n",
        "            df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
        "\n",
        "            save_message = f\"✓ Saved dataframe version {version_name}: System error - {str(e)[:100]}\"\n",
        "            print(save_message)\n",
        "            print(f\"  - CSV: {csv_filename}\")\n",
        "            print(f\"  - Excel: {excel_filename}\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"Failed to save error version: {save_error}\")\n",
        "\n",
        "        # Fallback to a generic response\n",
        "        fallback_response = f\"\"\"\n",
        "        I'm sorry, I encountered an issue while generating and executing code for your request.\n",
        "\n",
        "        **Technical Details:** {str(e)}\n",
        "\n",
        "        Could you try asking your question in a different way? For complex analyses, it sometimes helps to break down your request into smaller, more specific questions.\n",
        "        \"\"\"\n",
        "\n",
        "        new_state = dict(state)\n",
        "        new_state[\"final_response\"] = fallback_response\n",
        "\n",
        "        new_messages = state[\"messages\"].copy()\n",
        "        new_messages.append({\"role\": \"assistant\", \"content\": fallback_response})\n",
        "        new_state[\"messages\"] = new_messages\n",
        "\n",
        "        return new_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q7RJJWhjgO7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFZKl40KcZaV"
      },
      "outputs": [],
      "source": [
        "# Build the LangGraph workflow\n",
        "def create_agentic_rent_roll_analyzer():\n",
        "    \"\"\"Create and return the agentic rent roll analyzer workflow.\"\"\"\n",
        "\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(AgentState)\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    workflow.add_node(\"determine_action\", determine_action)\n",
        "    workflow.add_node(\"ask_clarification\", ask_clarification)\n",
        "    workflow.add_node(\"generate_text_response\", generate_text_response)\n",
        "    workflow.add_node(\"generate_code_and_execute\", generate_code_and_execute)\n",
        "\n",
        "    # Set the entry point\n",
        "    workflow.set_entry_point(\"determine_action\")\n",
        "\n",
        "    # Define conditional edges based on dictionary state values\n",
        "    workflow.add_conditional_edges(\n",
        "        \"determine_action\",\n",
        "        lambda state: \"ask_clarification\" if state.get(\"needs_clarification\") else\n",
        "                      \"generate_code_and_execute\" if state.get(\"generate_code\") else\n",
        "                      \"generate_text_response\"\n",
        "    )\n",
        "\n",
        "    # Add edges to END\n",
        "    workflow.add_edge(\"ask_clarification\", END)\n",
        "    workflow.add_edge(\"generate_text_response\", END)\n",
        "    workflow.add_edge(\"generate_code_and_execute\", END)\n",
        "\n",
        "    # Compile the graph\n",
        "    agentic_analyzer = workflow.compile()\n",
        "\n",
        "    return agentic_analyzer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mKF1WukcceN"
      },
      "outputs": [],
      "source": [
        "def upload_rent_roll(file, anthropic_api_key, openai_api_key, auto_analyze):\n",
        "    \"\"\"Process the uploaded rent roll file and initialize the chat.\"\"\"\n",
        "    global app_state\n",
        "\n",
        "    logger.info(\"Starting rent roll upload and processing\")\n",
        "\n",
        "    # Use the default API keys if none are provided\n",
        "    anthropic_key = anthropic_api_key if anthropic_api_key else DEFAULT_ANTHROPIC_API_KEY\n",
        "    openai_key = openai_api_key if openai_api_key else DEFAULT_OPENAI_API_KEY\n",
        "    logger.info(\"API keys configured\")\n",
        "\n",
        "    # Validate inputs\n",
        "    if not file:\n",
        "        logger.warning(\"No file uploaded\")\n",
        "        return \"Please upload a rent roll Excel file.\", None, gr.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        # Save the uploaded file to a temporary location\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx')\n",
        "        temp_file.close()\n",
        "        file_path = temp_file.name\n",
        "        logger.info(f\"Created temporary file: {file_path}\")\n",
        "\n",
        "        # Copy the uploaded file to our temporary location\n",
        "        with open(file.name, 'rb') as src_file, open(file_path, 'wb') as dst_file:\n",
        "            dst_file.write(src_file.read())\n",
        "        logger.info(\"File copied to temporary location\")\n",
        "\n",
        "        # Use our improved rent roll loader\n",
        "        try:\n",
        "            logger.info(\"Loading rent roll with specialized loader...\")\n",
        "            rent_roll_df = read_rent_roll_simple(file_path)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error with specialized loader: {e}. Falling back to standard loading.\")\n",
        "            # Fallback to basic loading\n",
        "            rent_roll_df = pd.read_excel(file_path)\n",
        "            logger.info(\"Fallback: Loaded rent roll with default pandas settings\")\n",
        "\n",
        "        logger.info(f\"Loaded rent roll data: {len(rent_roll_df)} rows, {len(rent_roll_df.columns)} columns\")\n",
        "\n",
        "        # Auto-analyze with GPT if selected\n",
        "        if auto_analyze:\n",
        "            logger.info(\"Auto-analyze option selected. Calling GPT for analysis...\")\n",
        "            issues_list = analyze_rent_roll_gpt(file_path, openai_key)  # Use OpenAI key for this\n",
        "            logger.info(f\"GPT analysis complete. Found {len(issues_list)} issues.\")\n",
        "        else:\n",
        "            # Create empty issues list if not auto-analyzing\n",
        "            issues_list = []\n",
        "            logger.info(\"No auto-analysis performed.\")\n",
        "\n",
        "        # Initialize the global app state with version tracking\n",
        "        app_state = {\n",
        "            \"df\": rent_roll_df,\n",
        "            \"issues\": issues_list,\n",
        "            \"anthropic_client\": Anthropic(api_key=anthropic_key),\n",
        "            \"openai_client\": OpenAI(api_key=openai_key),\n",
        "            \"system_message\": \"\",  # Will be populated below\n",
        "            \"df_versions\": []  # Initialize empty version registry\n",
        "        }\n",
        "\n",
        "        # Save the initial version\n",
        "        initial_version = save_dataframe_version(rent_roll_df, \"Initial upload - original dataset\")\n",
        "        logger.info(f\"Created initial dataframe version: {initial_version}\")\n",
        "\n",
        "        # Create system message with data understanding\n",
        "        column_info = []\n",
        "        for col in rent_roll_df.columns:\n",
        "            try:\n",
        "                dtype_str = str(rent_roll_df[col].dtype)\n",
        "                column_info.append(f\"- {col}: {dtype_str}\")\n",
        "            except Exception as e:\n",
        "                column_info.append(f\"- {col}: [Error determining type: {str(e)}]\")\n",
        "        column_info_str = \"\\n\".join(column_info)\n",
        "        # Calculate basic stats about the data\n",
        "        data_stats = []\n",
        "        for col in rent_roll_df.columns:\n",
        "            try:\n",
        "                if pd.api.types.is_numeric_dtype(rent_roll_df[col]):\n",
        "                    stat = f\"- {col}: min={rent_roll_df[col].min()}, max={rent_roll_df[col].max()}, mean={rent_roll_df[col].mean():.2f}, null={rent_roll_df[col].isna().sum()}\"\n",
        "                else:\n",
        "                    unique_vals = rent_roll_df[col].nunique()\n",
        "                    stat = f\"- {col}: unique values={unique_vals}, null={rent_roll_df[col].isna().sum()}\"\n",
        "                data_stats.append(stat)\n",
        "            except:\n",
        "                data_stats.append(f\"- {col}: [error calculating stats]\")\n",
        "        data_stats_str = \"\\n\".join(data_stats)\n",
        "\n",
        "        # Format issues for display\n",
        "        issues_text = \"\\n\".join([f\"- {issue}\" for issue in issues_list])\n",
        "\n",
        "        system_message = f\"\"\"\n",
        "        You are a Commercial Real Estate rent roll assistant that has analyzed a rent roll and found the following issues:\n",
        "\n",
        "        {issues_text}\n",
        "\n",
        "        The rent roll data has {len(rent_roll_df)} rows and {len(rent_roll_df.columns)} columns.\n",
        "\n",
        "        Column information:\n",
        "        {column_info_str}\n",
        "\n",
        "        Data statistics:\n",
        "        {data_stats_str}\n",
        "\n",
        "        When helping the user, follow these critical guidelines:\n",
        "        1. DO NOT generate placeholder code with fake column names. Work ONLY with the actual columns from the dataframe.\n",
        "        2. NEVER assume column names that don't exist in the actual data.\n",
        "        3. Always start by examining the first few rows to understand the meaning of each column.\n",
        "        4. If you can't identify which columns contain certain information, clearly state this limitation.\n",
        "        5. DO NOT proceed with analysis using made-up column names that don't exist in the data.\n",
        "\n",
        "        The entire dataframe is available as 'df' in the execution environment.\n",
        "\n",
        "        Important instructions for code and calculations:\n",
        "        1. ALWAYS share your chain of thought reasoning in your responses. For each analysis:\n",
        "          - Begin with \"**Thinking through this step by step:**\" in bold\n",
        "          - Clearly explain your understanding of the request\n",
        "          - Describe your approach to solving the problem\n",
        "          - Outline the data exploration steps you'll take\n",
        "          - Explain why you're choosing specific columns and methods\n",
        "          - Discuss any challenges you anticipate with the data structure\n",
        "          This chain of thought should be visible to the user in your chat responses.\n",
        "        \"\"\"\n",
        "\n",
        "        # Save the system message to the app state\n",
        "        app_state[\"system_message\"] = system_message\n",
        "\n",
        "        # Clean up the temporary file\n",
        "        os.unlink(file_path)\n",
        "        logger.info(\"Temporary file removed\")\n",
        "\n",
        "        # Generate a preview of the data and issues\n",
        "        preview_html = f\"\"\"\n",
        "        <h3>Rent Roll Preview</h3>\n",
        "        <p>Successfully loaded rent roll with {len(rent_roll_df)} rows and {len(rent_roll_df.columns)} columns.</p>\n",
        "        {rent_roll_df.head(5).fillna('').to_html(index=False)}\n",
        "\n",
        "        <h3>Identified Issues</h3>\n",
        "        <ol>\n",
        "        \"\"\"\n",
        "\n",
        "        # Format each issue for the HTML preview\n",
        "        for issue in issues_list:\n",
        "            # If issue starts with a number (like \"1. Issue\"), strip the number\n",
        "            if issue and issue[0].isdigit() and \". \" in issue[:5]:\n",
        "                issue = issue[issue.find(\". \")+2:]\n",
        "            preview_html += f\"<li>{issue}</li>\"\n",
        "\n",
        "        preview_html += \"\"\"\n",
        "        </ol>\n",
        "        <p>You can now start asking questions in the chat below!</p>\n",
        "        <p><strong>Note:</strong> This application uses GPT-4 for decision making and text responses,\n",
        "        and Claude AI specifically for code generation and execution.</p>\n",
        "        \"\"\"\n",
        "        version_choices = get_version_choices()\n",
        "        # Make the chat interface visible\n",
        "        logger.info(\"Setup complete. Ready for chat interaction.\")\n",
        "        return (\n",
        "            \"Rent roll loaded successfully! You can now start chatting.\",\n",
        "            preview_html,\n",
        "            gr.update(visible=True),  # chatbot visibility\n",
        "            gr.update(choices=version_choices, value=version_choices[-1] if version_choices else None)  # version dropdown\n",
        "        )\n",
        "\n",
        "\n",
        "    # Also update the error return:\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during rent roll processing: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        if 'file_path' in locals() and os.path.exists(file_path):\n",
        "            os.unlink(file_path)\n",
        "            logger.info(\"Cleaned up temporary file after error\")\n",
        "        return f\"Error: {str(e)}\", None, gr.update(visible=False), gr.update(choices=[], value=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g86vl7iVq7lV"
      },
      "outputs": [],
      "source": [
        "def load_latest_version_for_editing():\n",
        "    \"\"\"Load the most recent version of the dataframe for editing\"\"\"\n",
        "    global app_state\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        return None, \"No data loaded. Please upload a rent roll first.\"\n",
        "\n",
        "    try:\n",
        "        # Use the current dataframe (which is the latest)\n",
        "        df = app_state[\"df\"].copy()\n",
        "        df = df.fillna('')\n",
        "        # Get version info\n",
        "        if app_state[\"df_versions\"]:\n",
        "            latest_version = app_state[\"df_versions\"][-1]\n",
        "            version_info = f\"Loaded version: {latest_version['name']} - {latest_version['description']}\"\n",
        "        else:\n",
        "            version_info = \"Loaded current data (no versions saved yet)\"\n",
        "\n",
        "        logger.info(f\"Loaded dataframe for editing: {df.shape}\")\n",
        "        return df, version_info\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data for editing: {e}\")\n",
        "        return None, f\"Error loading data: {str(e)}\"\n",
        "\n",
        "def save_edited_dataframe(edited_df, description):\n",
        "    \"\"\"Save the edited dataframe as a new version\"\"\"\n",
        "    global app_state\n",
        "\n",
        "    if edited_df is None or edited_df.empty:\n",
        "        return \"No data to save\", gr.update()\n",
        "\n",
        "    try:\n",
        "        # Convert the edited dataframe to proper pandas DataFrame if needed\n",
        "        if not isinstance(edited_df, pd.DataFrame):\n",
        "            edited_df = pd.DataFrame(edited_df)\n",
        "\n",
        "        # Generate a meaningful description\n",
        "        if not description:\n",
        "            description = \"Manual edits via data editor\"\n",
        "\n",
        "        # Save as new version\n",
        "        version_name = save_dataframe_version(edited_df, description)\n",
        "\n",
        "        # Update the app state with the edited dataframe\n",
        "        app_state[\"df\"] = edited_df\n",
        "\n",
        "        # Log the changes\n",
        "        logger.info(f\"Saved edited dataframe as version {version_name}\")\n",
        "\n",
        "        # Return success message and update the view\n",
        "        return f\"✅ Successfully saved as version {version_name}\", gr.update(value=edited_df)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving edited dataframe: {e}\")\n",
        "        return f\"❌ Error saving: {str(e)}\", gr.update()\n",
        "\n",
        "def load_specific_version(version_name):\n",
        "    \"\"\"Load a specific version for editing\"\"\"\n",
        "    global app_state\n",
        "\n",
        "    if not version_name:\n",
        "        return None, \"Please select a version to load\"\n",
        "\n",
        "    try:\n",
        "        # Find the version file\n",
        "        versions_dir = \"rent_roll_versions\"\n",
        "        csv_filename = os.path.join(versions_dir, f\"rent_roll_{version_name}.csv\")\n",
        "\n",
        "        if os.path.exists(csv_filename):\n",
        "            df = pd.read_csv(csv_filename)\n",
        "            df = df.fillna('')\n",
        "            logger.info(f\"Loaded version {version_name} for editing\")\n",
        "            return df, f\"Loaded version: {version_name}\"\n",
        "        else:\n",
        "            return None, f\"Version file not found: {version_name}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading version {version_name}: {e}\")\n",
        "        return None, f\"Error loading version: {str(e)}\"\n",
        "\n",
        "def get_version_choices():\n",
        "    \"\"\"Get list of available versions for dropdown\"\"\"\n",
        "    global app_state\n",
        "\n",
        "    if app_state and \"df_versions\" in app_state and app_state[\"df_versions\"]:\n",
        "        choices = []\n",
        "        for i, version in enumerate(app_state[\"df_versions\"]):\n",
        "            status = \"\"\n",
        "            if i == 0:\n",
        "                status = \" (ORIGINAL)\"\n",
        "            elif i == len(app_state[\"df_versions\"]) - 1:\n",
        "                status = \" (LATEST)\"\n",
        "\n",
        "            choices.append(f\"{version['name']}{status}\")\n",
        "        return choices\n",
        "    return []\n",
        "\n",
        "def refresh_version_dropdown():\n",
        "    \"\"\"Refresh the version dropdown choices\"\"\"\n",
        "    choices = get_version_choices()\n",
        "    if choices:\n",
        "        return gr.update(choices=choices, value=choices[-1])  # Default to latest\n",
        "    return gr.update(choices=[], value=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH7oMejeHAfW"
      },
      "outputs": [],
      "source": [
        "class SessionRecorder:\n",
        "    def __init__(self):\n",
        "        self.sessions_dir = \"copiloting_sessions\"\n",
        "        os.makedirs(self.sessions_dir, exist_ok=True)\n",
        "        self.current_session_file = None\n",
        "        self.current_session_data = {}\n",
        "\n",
        "    def start_session_recording(self, rent_roll_filename):\n",
        "        \"\"\"Start recording the entire copiloting session\"\"\"\n",
        "        session_id = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        self.current_session_file = os.path.join(self.sessions_dir, f\"{session_id}.txt\")\n",
        "\n",
        "        # Initialize session data\n",
        "        self.current_session_data = {\n",
        "            \"session_id\": session_id,\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"rent_roll_file\": rent_roll_filename,\n",
        "            \"conversation_history\": [],\n",
        "            \"code_executions\": [],\n",
        "            \"dataframe_versions\": [],\n",
        "            \"issues_found\": [],\n",
        "            \"user_goals\": []\n",
        "        }\n",
        "\n",
        "        # Write session header to text file\n",
        "        with open(self.current_session_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"=== RENT ROLL COPILOTING SESSION ===\\n\")\n",
        "            f.write(f\"Session ID: {session_id}\\n\")\n",
        "            f.write(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Rent Roll File: {rent_roll_filename}\\n\")\n",
        "            f.write(f\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        print(f\"📝 Started session recording: {session_id}\")\n",
        "        return session_id\n",
        "\n",
        "    def record_conversation_turn(self, user_message, ai_response, action_type, code_executed=None, version_saved=None):\n",
        "        \"\"\"Record each conversation turn in real-time\"\"\"\n",
        "        if not self.current_session_file:\n",
        "            return\n",
        "\n",
        "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
        "        turn_data = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"user_message\": user_message,\n",
        "            \"ai_response\": ai_response,\n",
        "            \"action_type\": action_type,\n",
        "            \"code_executed\": code_executed,\n",
        "            \"version_saved\": version_saved\n",
        "        }\n",
        "\n",
        "        # Add to session data\n",
        "        self.current_session_data[\"conversation_history\"].append(turn_data)\n",
        "\n",
        "        # Append to text file immediately\n",
        "        with open(self.current_session_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"[{timestamp}] USER: {user_message}\\n\")\n",
        "            f.write(f\"Action Type: {action_type}\\n\")\n",
        "\n",
        "            if code_executed:\n",
        "                f.write(f\"CODE EXECUTED:\\n```python\\n{code_executed}\\n```\\n\")\n",
        "\n",
        "            f.write(f\"AI RESPONSE: {ai_response}\\n\")\n",
        "\n",
        "            if version_saved:\n",
        "                f.write(f\"VERSION SAVED: {version_saved}\\n\")\n",
        "\n",
        "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
        "\n",
        "        # Track code executions separately\n",
        "        if code_executed:\n",
        "            self.current_session_data[\"code_executions\"].append({\n",
        "                \"timestamp\": timestamp,\n",
        "                \"code\": code_executed,\n",
        "                \"purpose\": user_message,\n",
        "                \"result\": ai_response[:200] + \"...\" if len(ai_response) > 200 else ai_response\n",
        "            })\n",
        "\n",
        "    def record_dataframe_version(self, version_name, description, shape, columns):\n",
        "        \"\"\"Record dataframe version changes\"\"\"\n",
        "        version_info = {\n",
        "            \"timestamp\": datetime.now().strftime('%H:%M:%S'),\n",
        "            \"version_name\": version_name,\n",
        "            \"description\": description,\n",
        "            \"shape\": shape,\n",
        "            \"columns\": columns\n",
        "        }\n",
        "\n",
        "        self.current_session_data[\"dataframe_versions\"].append(version_info)\n",
        "\n",
        "        # Append to text file\n",
        "        if self.current_session_file:\n",
        "            with open(self.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"VERSION SAVED: {version_name}\\n\")\n",
        "                f.write(f\"Description: {description}\\n\")\n",
        "                f.write(f\"Shape: {shape}\\n\")\n",
        "                f.write(f\"Columns: {columns}\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\\n\")\n",
        "\n",
        "    def record_issue_found(self, issue_description, severity=\"medium\"):\n",
        "        \"\"\"Record issues found during analysis\"\"\"\n",
        "        issue_info = {\n",
        "            \"timestamp\": datetime.now().strftime('%H:%M:%S'),\n",
        "            \"description\": issue_description,\n",
        "            \"severity\": severity\n",
        "        }\n",
        "\n",
        "        self.current_session_data[\"issues_found\"].append(issue_info)\n",
        "\n",
        "        if self.current_session_file:\n",
        "            with open(self.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"ISSUE FOUND [{severity.upper()}]: {issue_description}\\n\")\n",
        "                f.write(\"-\" * 40 + \"\\n\\n\")\n",
        "\n",
        "    def finalize_session(self):\n",
        "        \"\"\"End session recording and return session data\"\"\"\n",
        "        if not self.current_session_file:\n",
        "            return None\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - datetime.fromisoformat(self.current_session_data[\"start_time\"])\n",
        "\n",
        "        # Write session summary\n",
        "        with open(self.current_session_file, 'a', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "            f.write(\"SESSION SUMMARY\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\")\n",
        "            f.write(f\"Session Duration: {duration.total_seconds()/60:.1f} minutes\\n\")\n",
        "            f.write(f\"Total Conversations: {len(self.current_session_data['conversation_history'])}\\n\")\n",
        "            f.write(f\"Code Executions: {len(self.current_session_data['code_executions'])}\\n\")\n",
        "            f.write(f\"Versions Created: {len(self.current_session_data['dataframe_versions'])}\\n\")\n",
        "            f.write(f\"Issues Found: {len(self.current_session_data['issues_found'])}\\n\")\n",
        "            f.write(f\"Ended: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        # Update session data\n",
        "        self.current_session_data[\"end_time\"] = end_time.isoformat()\n",
        "        self.current_session_data[\"duration_minutes\"] = duration.total_seconds() / 60\n",
        "\n",
        "        session_data = self.current_session_data.copy()\n",
        "\n",
        "        # Reset for next session\n",
        "        self.current_session_file = None\n",
        "        self.current_session_data = {}\n",
        "\n",
        "        print(f\"✅ Session recording finalized: {session_data['session_id']}\")\n",
        "        return session_data\n",
        "\n",
        "# Global session recorder\n",
        "session_recorder = SessionRecorder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ06LnpjG3UW"
      },
      "outputs": [],
      "source": [
        "class EnhancedTemplateManager:\n",
        "    def __init__(self):\n",
        "        self.templates_dir = \"rent_roll_templates\"\n",
        "        os.makedirs(self.templates_dir, exist_ok=True)\n",
        "        self.current_session = None\n",
        "\n",
        "    def create_template_from_session(self, session_data, starting_df, final_df, template_name):\n",
        "        \"\"\"Generate comprehensive template from complete session data using GPT-4\"\"\"\n",
        "\n",
        "        print(\"🤖 Analyzing session with GPT-4.1 to generate instructions...\")\n",
        "\n",
        "        # Prepare comprehensive session context for GPT-4\n",
        "        session_context = self._prepare_session_context(session_data)\n",
        "\n",
        "        # Use GPT-4.1 to analyze and generate instructions\n",
        "        client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "        analysis_prompt = f\"\"\"\n",
        "        You are an expert at analyzing data analysis workflows and creating reusable instruction templates.\n",
        "\n",
        "        I will provide you with a complete copiloting session where a user worked on a rent roll analysis.\n",
        "        Your task is to:\n",
        "        1. Analyze the entire workflow\n",
        "        2. Identify the key transformation patterns\n",
        "        3. Create step-by-step instructions that can be applied to similar rent roll files\n",
        "        4. Generate reusable code templates with placeholders\n",
        "        5. Document the business logic and decision points\n",
        "\n",
        "        SESSION DATA:\n",
        "        {session_context}\n",
        "\n",
        "        STARTING DATAFRAME INFO:\n",
        "        - Shape: {starting_df.shape}\n",
        "        - Columns: {list(starting_df.columns)}\n",
        "        - Sample data: {starting_df.head(2).to_string()}\n",
        "\n",
        "        FINAL DATAFRAME INFO:\n",
        "        - Shape: {final_df.shape}\n",
        "        - Columns: {list(final_df.columns)}\n",
        "        - Sample data: {final_df.head(2).to_string()}\n",
        "\n",
        "        Please generate a comprehensive analysis in the following JSON format:\n",
        "        {{\n",
        "            \"workflow_summary\": \"Brief description of what was accomplished\",\n",
        "            \"key_transformations\": [\n",
        "                {{\n",
        "                    \"step_name\": \"Clean Tenant Names\",\n",
        "                    \"description\": \"Standardize tenant name formatting\",\n",
        "                    \"business_rule\": \"All tenant names should be Title Case with no extra whitespace\",\n",
        "                    \"code_template\": \"df['{{column_name}}'] = df['{{original_column}}'].str.strip().str.title()\",\n",
        "                    \"parameters\": [\"column_name\", \"original_column\"],\n",
        "                    \"conditions\": \"Apply when tenant names have inconsistent formatting\"\n",
        "                }}\n",
        "            ],\n",
        "            \"data_quality_improvements\": [\n",
        "                \"List of data quality issues that were resolved\"\n",
        "            ],\n",
        "            \"reusable_patterns\": [\n",
        "                \"Pattern 1: Column standardization\",\n",
        "                \"Pattern 2: Missing value handling\"\n",
        "            ],\n",
        "            \"business_insights\": [\n",
        "                \"Key insights discovered during analysis\"\n",
        "            ],\n",
        "            \"prerequisites\": [\n",
        "                \"What conditions must be met for this template to work\"\n",
        "            ],\n",
        "            \"instructions_for_reuse\": [\n",
        "                \"Step 1: Upload new rent roll file\",\n",
        "                \"Step 2: Map columns (if different names)\",\n",
        "                \"Step 3: Apply transformations in order\"\n",
        "            ]\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4.1\",  # Using latest GPT-4\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert data analyst who creates reusable workflow templates from analysis sessions. Provide detailed, actionable instructions.\"},\n",
        "                    {\"role\": \"user\", \"content\": analysis_prompt}\n",
        "                ],\n",
        "                max_tokens=4000,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            # Extract the analysis\n",
        "            gpt_analysis = response.choices[0].message.content\n",
        "\n",
        "            # Try to extract JSON from the response\n",
        "            json_match = re.search(r'{.*}', gpt_analysis, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    workflow_analysis = json.loads(json_match.group(0))\n",
        "                except:\n",
        "                    # Fallback if JSON parsing fails\n",
        "                    workflow_analysis = {\"analysis\": gpt_analysis}\n",
        "            else:\n",
        "                workflow_analysis = {\"analysis\": gpt_analysis}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error with GPT-4 analysis: {e}\")\n",
        "            workflow_analysis = {\"error\": str(e), \"fallback_analysis\": \"Manual analysis required\"}\n",
        "\n",
        "        # Create template with all data\n",
        "        template_id = f\"template_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        # Save dataframes as text files\n",
        "        starting_df_file = f\"{template_id}_starting_df.txt\"\n",
        "        final_df_file = f\"{template_id}_final_df.txt\"\n",
        "        session_file = f\"{template_id}_session.txt\"\n",
        "\n",
        "        starting_df_path = os.path.join(self.templates_dir, starting_df_file)\n",
        "        final_df_path = os.path.join(self.templates_dir, final_df_file)\n",
        "        session_path = os.path.join(self.templates_dir, session_file)\n",
        "\n",
        "        # Save dataframes\n",
        "        starting_df.to_csv(starting_df_path, index=False)\n",
        "        final_df.to_csv(final_df_path, index=False)\n",
        "\n",
        "        # Save raw session data\n",
        "        with open(session_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(session_context)\n",
        "\n",
        "        # Create comprehensive template\n",
        "        template_data = {\n",
        "            \"template_id\": template_id,\n",
        "            \"template_name\": template_name,\n",
        "            \"created_date\": datetime.now().isoformat(),\n",
        "            \"source_session_id\": session_data.get(\"session_id\", \"unknown\"),\n",
        "            \"source_file_name\": session_data.get(\"rent_roll_file\", \"unknown\"),\n",
        "\n",
        "            \"files\": {\n",
        "                \"starting_dataframe\": starting_df_file,\n",
        "                \"final_dataframe\": final_df_file,\n",
        "                \"raw_session\": session_file\n",
        "            },\n",
        "\n",
        "            \"session_summary\": {\n",
        "                \"duration_minutes\": session_data.get(\"duration_minutes\", 0),\n",
        "                \"total_conversations\": len(session_data.get(\"conversation_history\", [])),\n",
        "                \"code_executions\": len(session_data.get(\"code_executions\", [])),\n",
        "                \"versions_created\": len(session_data.get(\"dataframe_versions\", [])),\n",
        "                \"issues_found\": len(session_data.get(\"issues_found\", []))\n",
        "            },\n",
        "\n",
        "            \"gpt4_analysis\": workflow_analysis,\n",
        "\n",
        "            \"raw_workflow_steps\": session_data.get(\"conversation_history\", []),\n",
        "            \"code_executions\": session_data.get(\"code_executions\", []),\n",
        "            \"dataframe_changes\": session_data.get(\"dataframe_versions\", []),\n",
        "            \"issues_identified\": session_data.get(\"issues_found\", [])\n",
        "        }\n",
        "\n",
        "        # Save template metadata\n",
        "        template_json_path = os.path.join(self.templates_dir, f\"{template_id}.json\")\n",
        "        with open(template_json_path, 'w') as f:\n",
        "            json.dump(template_data, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"✅ Comprehensive template created: {template_id}\")\n",
        "        print(f\"📁 Starting DF: {starting_df_path}\")\n",
        "        print(f\"📁 Final DF: {final_df_path}\")\n",
        "        print(f\"📁 Session Data: {session_path}\")\n",
        "        print(f\"📋 Template: {template_json_path}\")\n",
        "\n",
        "        return template_data\n",
        "\n",
        "    def _prepare_session_context(self, session_data):\n",
        "        \"\"\"Prepare session data for GPT-4 analysis\"\"\"\n",
        "        context = f\"\"\"\n",
        "COPILOTING SESSION ANALYSIS\n",
        "============================\n",
        "\n",
        "Session ID: {session_data.get('session_id', 'N/A')}\n",
        "Duration: {session_data.get('duration_minutes', 0):.1f} minutes\n",
        "Rent Roll File: {session_data.get('rent_roll_file', 'N/A')}\n",
        "\n",
        "CONVERSATION HISTORY:\n",
        "\"\"\"\n",
        "\n",
        "        for i, conv in enumerate(session_data.get('conversation_history', []), 1):\n",
        "            context += f\"\"\"\n",
        "--- Conversation {i} [{conv.get('timestamp', 'N/A')}] ---\n",
        "USER QUERY: {conv.get('user_message', 'N/A')}\n",
        "ACTION TYPE: {conv.get('action_type', 'N/A')}\n",
        "\"\"\"\n",
        "            if conv.get('code_executed'):\n",
        "                context += f\"CODE EXECUTED:\\n{conv['code_executed']}\\n\"\n",
        "\n",
        "            context += f\"AI RESPONSE: {conv.get('ai_response', 'N/A')[:300]}...\\n\"\n",
        "\n",
        "            if conv.get('version_saved'):\n",
        "                context += f\"VERSION SAVED: {conv['version_saved']}\\n\"\n",
        "\n",
        "            context += \"\\n\"\n",
        "\n",
        "        context += \"\\nCODE EXECUTIONS SUMMARY:\\n\"\n",
        "        for code_exec in session_data.get('code_executions', []):\n",
        "            context += f\"- [{code_exec.get('timestamp')}] {code_exec.get('purpose', 'N/A')}\\n\"\n",
        "            context += f\"  Code: {code_exec.get('code', 'N/A')[:100]}...\\n\"\n",
        "\n",
        "        context += \"\\nISSUES IDENTIFIED:\\n\"\n",
        "        for issue in session_data.get('issues_found', []):\n",
        "            context += f\"- [{issue.get('timestamp')}] {issue.get('description', 'N/A')}\\n\"\n",
        "\n",
        "        context += \"\\nDATAFRAME VERSIONS:\\n\"\n",
        "        for version in session_data.get('dataframe_versions', []):\n",
        "            context += f\"- {version.get('version_name', 'N/A')}: {version.get('description', 'N/A')}\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    def load_template_dataframes(self, template_id):\n",
        "        \"\"\"Load both starting and final dataframes from a template\"\"\"\n",
        "        try:\n",
        "            # Load template metadata\n",
        "            template_json_path = os.path.join(self.templates_dir, f\"{template_id}.json\")\n",
        "            with open(template_json_path, 'r') as f:\n",
        "                template_data = json.load(f)\n",
        "\n",
        "            # Load starting dataframe\n",
        "            starting_df_path = os.path.join(self.templates_dir, template_data[\"files\"][\"starting_dataframe\"])\n",
        "            starting_df = pd.read_csv(starting_df_path)\n",
        "\n",
        "            # Load final dataframe\n",
        "            final_df_path = os.path.join(self.templates_dir, template_data[\"files\"][\"final_dataframe\"])\n",
        "            final_df = pd.read_csv(final_df_path)\n",
        "\n",
        "            return template_data, starting_df, final_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading template: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def list_templates(self):\n",
        "        \"\"\"List all available templates\"\"\"\n",
        "        try:\n",
        "            json_files = [f for f in os.listdir(self.templates_dir) if f.endswith('.json')]\n",
        "            templates = []\n",
        "\n",
        "            for json_file in json_files:\n",
        "                template_path = os.path.join(self.templates_dir, json_file)\n",
        "                with open(template_path, 'r') as f:\n",
        "                    template_data = json.load(f)\n",
        "\n",
        "                templates.append({\n",
        "                    \"template_id\": template_data[\"template_id\"],\n",
        "                    \"template_name\": template_data[\"template_name\"],\n",
        "                    \"created_date\": template_data[\"created_date\"],\n",
        "                    \"source_file\": template_data[\"source_file_name\"],\n",
        "                    \"steps_count\": len(template_data.get(\"raw_workflow_steps\", [])),\n",
        "                    \"gpt4_analysis_available\": \"gpt4_analysis\" in template_data\n",
        "                })\n",
        "\n",
        "            return sorted(templates, key=lambda x: x[\"created_date\"], reverse=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error listing templates: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_template_summary(self, template_id):\n",
        "        \"\"\"Get a human-readable summary of a template\"\"\"\n",
        "        try:\n",
        "            template_json_path = os.path.join(self.templates_dir, f\"{template_id}.json\")\n",
        "            with open(template_json_path, 'r') as f:\n",
        "                template_data = json.load(f)\n",
        "\n",
        "            summary = f\"\"\"\n",
        "📋 Template: {template_data.get('template_name', 'Unknown')}\n",
        "🆔 ID: {template_data.get('template_id', 'Unknown')}\n",
        "📅 Created: {template_data.get('created_date', 'Unknown')}\n",
        "📁 Source File: {template_data.get('source_file_name', 'Unknown')}\n",
        "\n",
        "📊 Session Summary:\n",
        "• Duration: {template_data.get('session_summary', {}).get('duration_minutes', 0):.1f} minutes\n",
        "• Conversations: {template_data.get('session_summary', {}).get('total_conversations', 0)}\n",
        "• Code Executions: {template_data.get('session_summary', {}).get('code_executions', 0)}\n",
        "• Versions Created: {template_data.get('session_summary', {}).get('versions_created', 0)}\n",
        "• Issues Found: {template_data.get('session_summary', {}).get('issues_found', 0)}\n",
        "\n",
        "🤖 GPT-4 Analysis: {'✅ Available' if 'gpt4_analysis' in template_data else '❌ Not Available'}\n",
        "\"\"\"\n",
        "\n",
        "            # Add GPT-4 analysis summary if available\n",
        "            if 'gpt4_analysis' in template_data and isinstance(template_data['gpt4_analysis'], dict):\n",
        "                gpt_analysis = template_data['gpt4_analysis']\n",
        "\n",
        "                if 'workflow_summary' in gpt_analysis:\n",
        "                    summary += f\"\\n🔍 Workflow Summary:\\n{gpt_analysis['workflow_summary']}\\n\"\n",
        "\n",
        "                if 'key_transformations' in gpt_analysis:\n",
        "                    summary += f\"\\n🔧 Key Transformations ({len(gpt_analysis['key_transformations'])}):\\n\"\n",
        "                    for i, transform in enumerate(gpt_analysis['key_transformations'][:3], 1):  # Show first 3\n",
        "                        summary += f\"{i}. {transform.get('step_name', 'Unknown')}: {transform.get('description', 'No description')}\\n\"\n",
        "                    if len(gpt_analysis['key_transformations']) > 3:\n",
        "                        summary += f\"... and {len(gpt_analysis['key_transformations']) - 3} more\\n\"\n",
        "\n",
        "                if 'prerequisites' in gpt_analysis:\n",
        "                    summary += f\"\\n📋 Prerequisites:\\n\"\n",
        "                    for prereq in gpt_analysis['prerequisites'][:3]:  # Show first 3\n",
        "                        summary += f\"• {prereq}\\n\"\n",
        "\n",
        "            return summary\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error getting template summary: {str(e)}\"\n",
        "\n",
        "    def delete_template(self, template_id):\n",
        "        \"\"\"Delete a template and all its associated files\"\"\"\n",
        "        try:\n",
        "            template_json_path = os.path.join(self.templates_dir, f\"{template_id}.json\")\n",
        "\n",
        "            if not os.path.exists(template_json_path):\n",
        "                return f\"❌ Template {template_id} not found\"\n",
        "\n",
        "            # Load template to get file list\n",
        "            with open(template_json_path, 'r') as f:\n",
        "                template_data = json.load(f)\n",
        "\n",
        "            files_to_delete = []\n",
        "            files_to_delete.append(template_json_path)  # The main template file\n",
        "\n",
        "            # Add dataframe and session files\n",
        "            if 'files' in template_data:\n",
        "                for file_key, filename in template_data['files'].items():\n",
        "                    file_path = os.path.join(self.templates_dir, filename)\n",
        "                    if os.path.exists(file_path):\n",
        "                        files_to_delete.append(file_path)\n",
        "\n",
        "            # Delete all files\n",
        "            deleted_count = 0\n",
        "            for file_path in files_to_delete:\n",
        "                try:\n",
        "                    os.remove(file_path)\n",
        "                    deleted_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not delete {file_path}: {e}\")\n",
        "\n",
        "            return f\"✅ Template {template_id} deleted successfully. Removed {deleted_count} files.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error deleting template: {str(e)}\"\n",
        "\n",
        "# Global enhanced template manager\n",
        "enhanced_template_manager = EnhancedTemplateManager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi89MYRuGyvR"
      },
      "outputs": [],
      "source": [
        "def create_template_from_current_session():\n",
        "    \"\"\"Create template from current copiloting session\"\"\"\n",
        "    global app_state, session_recorder, enhanced_template_manager\n",
        "\n",
        "    if not session_recorder.current_session_file:\n",
        "        return \"❌ No active session to create template from\"\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        return \"❌ No dataframe loaded\"\n",
        "\n",
        "    try:\n",
        "        # Get starting dataframe (first version)\n",
        "        if app_state.get(\"df_versions\") and len(app_state[\"df_versions\"]) > 0:\n",
        "            first_version = app_state[\"df_versions\"][0]\n",
        "            starting_df_path = first_version.get(\"filename\") or first_version.get(\"csv_filename\")\n",
        "            starting_df = pd.read_csv(starting_df_path)\n",
        "        else:\n",
        "            starting_df = app_state[\"df\"]  # Fallback if no versions\n",
        "\n",
        "        # Current dataframe is the final version\n",
        "        final_df = app_state[\"df\"]\n",
        "\n",
        "        # Finalize current session\n",
        "        session_data = session_recorder.finalize_session()\n",
        "\n",
        "        if session_data is None:\n",
        "            return \"❌ Error finalizing session\"\n",
        "\n",
        "        # Generate template name suggestion\n",
        "        template_name = f\"Rent Roll Process {datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n",
        "\n",
        "        # Create comprehensive template\n",
        "        template_data = enhanced_template_manager.create_template_from_session(\n",
        "            session_data=session_data,\n",
        "            starting_df=starting_df,\n",
        "            final_df=final_df,\n",
        "            template_name=template_name\n",
        "        )\n",
        "\n",
        "        return f\"✅ Template created successfully!\\nTemplate ID: {template_data['template_id']}\\nSteps captured: {len(session_data.get('conversation_history', []))}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error creating template: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4Xebkjsdnm_"
      },
      "outputs": [],
      "source": [
        "# Global state for the application (Not part of graph state)\n",
        "app_state = {\n",
        "    \"df\": None,\n",
        "    \"anthropic_client\": None,\n",
        "    \"openai_client\": None,  # Added for GPT-4\n",
        "    \"issues\": [],\n",
        "    \"system_message\": \"\"\n",
        "}\n",
        "# Enhanced Chat Function with Complete Session Recording and Template Generation\n",
        "\n",
        "def chat(message, history):\n",
        "    \"\"\"\n",
        "    Enhanced chat function with comprehensive session recording and template generation.\n",
        "    Records every interaction, code execution, and dataframe change for template creation.\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder, enhanced_template_manager\n",
        "\n",
        "    logger.info(f\"Received chat message: {message[:50]}...\")\n",
        "\n",
        "    # Check if system is ready\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        logger.warning(\"Chat attempted before setup is complete\")\n",
        "        return history + [(message, \"Please upload a rent roll file and set up your API keys first.\")]\n",
        "\n",
        "    # Start session recording if not already started\n",
        "    if not session_recorder.current_session_file:\n",
        "        rent_roll_filename = getattr(app_state, 'original_filename', 'uploaded_rent_roll.xlsx')\n",
        "        session_id = session_recorder.start_session_recording(rent_roll_filename)\n",
        "        logger.info(f\"Started new session recording: {session_id}\")\n",
        "\n",
        "        # Record initial dataframe state\n",
        "        if app_state.get(\"df_versions\") and len(app_state[\"df_versions\"]) > 0:\n",
        "            first_version = app_state[\"df_versions\"][0]\n",
        "            session_recorder.record_dataframe_version(\n",
        "                version_name=first_version[\"name\"],\n",
        "                description=first_version[\"description\"],\n",
        "                shape=list(app_state[\"df\"].shape),\n",
        "                columns=list(app_state[\"df\"].columns)\n",
        "            )\n",
        "\n",
        "    # Get previous messages from history\n",
        "    prev_messages = []\n",
        "    if history:\n",
        "        for user_msg, assistant_msg in history:\n",
        "            prev_messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "            prev_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "    # Create message list without system message\n",
        "    all_messages = []\n",
        "    all_messages.extend(prev_messages)\n",
        "\n",
        "    # Add the current user message\n",
        "    all_messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Create a state dictionary for the graph\n",
        "    state = {\n",
        "        \"messages\": all_messages,\n",
        "        \"system_message\": app_state[\"system_message\"],\n",
        "        \"df\": app_state[\"df\"],\n",
        "        \"issues\": app_state[\"issues\"],\n",
        "        \"needs_clarification\": False,\n",
        "        \"generate_code\": False,\n",
        "        \"execution_plan\": None,\n",
        "        \"clarification_question\": None,\n",
        "        \"code_execution_results\": None,\n",
        "        \"final_response\": None,\n",
        "        \"anthropic_client\": app_state[\"anthropic_client\"],\n",
        "        \"openai_client\": app_state[\"openai_client\"]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create the workflow if not already created\n",
        "        if not hasattr(chat, \"workflow\"):\n",
        "            chat.workflow = create_agentic_rent_roll_analyzer()\n",
        "            logger.info(\"Created agentic workflow\")\n",
        "\n",
        "        # Run the workflow with the current state\n",
        "        logger.info(\"Running agentic workflow\")\n",
        "        result = chat.workflow.invoke(state)\n",
        "\n",
        "        # Get the final response from the result state\n",
        "        final_response = result.get(\"final_response\", \"I'm sorry, I couldn't process your request.\")\n",
        "        logger.info(f\"Received final response from workflow: {final_response[:50]}...\")\n",
        "\n",
        "        # === ENHANCED SESSION RECORDING ===\n",
        "\n",
        "        # 1. Determine action type based on response content and workflow state\n",
        "        action_type = \"analysis\"  # default\n",
        "\n",
        "        if result.get(\"needs_clarification\"):\n",
        "            action_type = \"clarification\"\n",
        "        elif result.get(\"generate_code\"):\n",
        "            action_type = \"data_processing\"\n",
        "        elif \"error\" in final_response.lower() or \"sorry\" in final_response.lower():\n",
        "            action_type = \"error_handling\"\n",
        "        elif \"```python\" in final_response:\n",
        "            action_type = \"code_execution\"\n",
        "        elif any(keyword in message.lower() for keyword in [\"clean\", \"fix\", \"correct\", \"standardize\"]):\n",
        "            action_type = \"data_cleaning\"\n",
        "        elif any(keyword in message.lower() for keyword in [\"calculate\", \"compute\", \"sum\", \"average\"]):\n",
        "            action_type = \"calculation\"\n",
        "        elif any(keyword in message.lower() for keyword in [\"find\", \"show\", \"display\", \"list\"]):\n",
        "            action_type = \"data_exploration\"\n",
        "        elif any(keyword in message.lower() for keyword in [\"chart\", \"graph\", \"plot\", \"visualize\"]):\n",
        "            action_type = \"visualization\"\n",
        "\n",
        "        # 2. Extract executed code from response\n",
        "        code_executed = None\n",
        "        code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', final_response, re.DOTALL)\n",
        "        if code_blocks:\n",
        "            # Combine all code blocks if multiple\n",
        "            code_executed = \"\\n\\n# --- Next Code Block ---\\n\\n\".join(code_blocks)\n",
        "\n",
        "        # 3. Check if a new dataframe version was saved\n",
        "        version_saved = None\n",
        "        if \"✓ Saved dataframe version\" in final_response:\n",
        "            version_match = re.search(r'version (v_\\w+)', final_response)\n",
        "            if version_match:\n",
        "                version_saved = version_match.group(1)\n",
        "                logger.info(f\"Detected new version saved: {version_saved}\")\n",
        "\n",
        "        # 4. Detect if issues were found or resolved\n",
        "        if any(keyword in final_response.lower() for keyword in [\"issue\", \"problem\", \"error\", \"missing\", \"duplicate\"]):\n",
        "            issue_description = message + \" - \" + final_response[:100] + \"...\"\n",
        "            severity = \"high\" if any(word in final_response.lower() for word in [\"critical\", \"error\", \"failed\"]) else \"medium\"\n",
        "            session_recorder.record_issue_found(issue_description, severity)\n",
        "\n",
        "        # 5. Extract any business insights or patterns\n",
        "        insights = []\n",
        "        if \"found\" in final_response.lower() and any(word in final_response.lower() for word in [\"units\", \"rent\", \"tenant\"]):\n",
        "            insights.append(f\"Business insight from query: {message}\")\n",
        "\n",
        "        # 6. Record the complete conversation turn with enhanced metadata\n",
        "        session_recorder.record_conversation_turn(\n",
        "            user_message=message,\n",
        "            ai_response=final_response,\n",
        "            action_type=action_type,\n",
        "            code_executed=code_executed,\n",
        "            version_saved=version_saved\n",
        "        )\n",
        "\n",
        "        # 7. Record dataframe version details if saved\n",
        "        if version_saved:\n",
        "            # Find the latest version info\n",
        "            latest_version = None\n",
        "            if app_state.get(\"df_versions\"):\n",
        "                for version in app_state[\"df_versions\"]:\n",
        "                    if version[\"name\"] == version_saved:\n",
        "                        latest_version = version\n",
        "                        break\n",
        "\n",
        "            if latest_version:\n",
        "                session_recorder.record_dataframe_version(\n",
        "                    version_name=version_saved,\n",
        "                    description=latest_version.get(\"description\", \"Auto-saved during copiloting\"),\n",
        "                    shape=list(app_state[\"df\"].shape),\n",
        "                    columns=list(app_state[\"df\"].columns)\n",
        "                )\n",
        "            else:\n",
        "                # Fallback if version not found in registry\n",
        "                session_recorder.record_dataframe_version(\n",
        "                    version_name=version_saved,\n",
        "                    description=\"Auto-saved during copiloting session\",\n",
        "                    shape=list(app_state[\"df\"].shape),\n",
        "                    columns=list(app_state[\"df\"].columns)\n",
        "                )\n",
        "\n",
        "        # 8. Track user goals and patterns\n",
        "        user_goals = []\n",
        "        if any(keyword in message.lower() for keyword in [\"clean\", \"standardize\", \"fix\"]):\n",
        "            user_goals.append(\"Data cleaning and standardization\")\n",
        "        if any(keyword in message.lower() for keyword in [\"analyze\", \"find\", \"calculate\"]):\n",
        "            user_goals.append(\"Data analysis and insights\")\n",
        "        if any(keyword in message.lower() for keyword in [\"chart\", \"graph\", \"visualize\"]):\n",
        "            user_goals.append(\"Data visualization\")\n",
        "\n",
        "        if user_goals:\n",
        "            session_recorder.current_session_data.setdefault(\"user_goals\", []).extend(user_goals)\n",
        "\n",
        "        # 9. Log session statistics\n",
        "        if session_recorder.current_session_data:\n",
        "            total_turns = len(session_recorder.current_session_data.get(\"conversation_history\", []))\n",
        "            total_code = len(session_recorder.current_session_data.get(\"code_executions\", []))\n",
        "            logger.info(f\"Session stats - Turns: {total_turns}, Code executions: {total_code}\")\n",
        "\n",
        "        # Use the correct format for Gradio chatbot\n",
        "        history_list = list(history) if history else []\n",
        "        history_list.append((message, final_response))\n",
        "\n",
        "        logger.info(\"Chat response processing complete with session recording\")\n",
        "        return history_list\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing chat: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        # Record the error in session\n",
        "        error_message = f\"Error getting response: {str(e)}\"\n",
        "\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=message,\n",
        "                ai_response=error_message,\n",
        "                action_type=\"system_error\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "            # Record as a system issue\n",
        "            session_recorder.record_issue_found(\n",
        "                f\"System error during processing: {str(e)}\",\n",
        "                severity=\"high\"\n",
        "            )\n",
        "\n",
        "        # Handle errors properly in the chat history format\n",
        "        history_list = list(history) if history else []\n",
        "        history_list.append((message, error_message))\n",
        "        return history_list\n",
        "\n",
        "\n",
        "def create_template_from_current_session(template_name_input=\"\"):\n",
        "    \"\"\"\n",
        "    Create a comprehensive template from the current copiloting session.\n",
        "    This includes GPT-4.1 analysis of the entire workflow.\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder, enhanced_template_manager\n",
        "\n",
        "    if not session_recorder.current_session_file:\n",
        "        return \"❌ No active copiloting session found. Please start chatting with the system first.\"\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        return \"❌ No dataframe loaded. Cannot create template.\"\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Starting template creation from current session...\")\n",
        "\n",
        "        # 1. Get starting dataframe (first version saved)\n",
        "        starting_df = None\n",
        "        if app_state.get(\"df_versions\") and len(app_state[\"df_versions\"]) > 0:\n",
        "            # Load the original/first version\n",
        "            first_version = app_state[\"df_versions\"][0]\n",
        "            starting_df_path = first_version.get(\"filename\") or first_version.get(\"csv_filename\")\n",
        "            if starting_df_path and os.path.exists(starting_df_path):\n",
        "                starting_df = pd.read_csv(starting_df_path)\n",
        "                logger.info(f\"Loaded starting dataframe from: {starting_df_path}\")\n",
        "            else:\n",
        "                # Try to construct the path\n",
        "                versions_dir = \"rent_roll_versions\"\n",
        "                csv_filename = os.path.join(versions_dir, f\"rent_roll_{first_version['name']}.csv\")\n",
        "                if os.path.exists(csv_filename):\n",
        "                    starting_df = pd.read_csv(csv_filename)\n",
        "                    logger.info(f\"Loaded starting dataframe from: {csv_filename}\")\n",
        "\n",
        "        # Fallback: use current dataframe if no versions found\n",
        "        if starting_df is None:\n",
        "            starting_df = app_state[\"df\"].copy()\n",
        "            logger.warning(\"Using current dataframe as starting point (no version history found)\")\n",
        "\n",
        "        # 2. Current dataframe is the final version\n",
        "        final_df = app_state[\"df\"].copy()\n",
        "\n",
        "        # 3. Finalize current session to get complete session data\n",
        "        logger.info(\"Finalizing current session...\")\n",
        "        session_data = session_recorder.finalize_session()\n",
        "\n",
        "        if session_data is None:\n",
        "            return \"❌ Error finalizing session data.\"\n",
        "\n",
        "        # 4. Generate template name if not provided\n",
        "        if not template_name_input.strip():\n",
        "            rent_roll_file = session_data.get('rent_roll_file', 'Unknown')\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d')\n",
        "            template_name = f\"Rent Roll Process - {rent_roll_file} - {timestamp}\"\n",
        "        else:\n",
        "            template_name = template_name_input.strip()\n",
        "\n",
        "        # 5. Create comprehensive template using GPT-4.1 analysis\n",
        "        logger.info(\"Creating template with GPT-4.1 analysis...\")\n",
        "        template_data = enhanced_template_manager.create_template_from_session(\n",
        "            session_data=session_data,\n",
        "            starting_df=starting_df,\n",
        "            final_df=final_df,\n",
        "            template_name=template_name\n",
        "        )\n",
        "\n",
        "        # 6. Prepare success message with details\n",
        "        session_stats = session_data.get('session_summary', {})\n",
        "        success_message = f\"\"\"✅ Template Created Successfully!\n",
        "\n",
        "📋 Template Details:\n",
        "• Template ID: {template_data['template_id']}\n",
        "• Template Name: {template_name}\n",
        "• Source File: {session_data.get('rent_roll_file', 'Unknown')}\n",
        "\n",
        "📊 Session Summary:\n",
        "• Duration: {session_stats.get('duration_minutes', 0):.1f} minutes\n",
        "• Conversations: {session_stats.get('total_conversations', 0)}\n",
        "• Code Executions: {session_stats.get('code_executions', 0)}\n",
        "• Versions Created: {session_stats.get('versions_created', 0)}\n",
        "• Issues Found: {session_stats.get('issues_found', 0)}\n",
        "\n",
        "📁 Files Created:\n",
        "• Starting Dataframe: {template_data['files']['starting_dataframe']}\n",
        "• Final Dataframe: {template_data['files']['final_dataframe']}\n",
        "• Session Recording: {template_data['files']['raw_session']}\n",
        "• Template Metadata: {template_data['template_id']}.json\n",
        "\n",
        "🤖 GPT-4.1 Analysis: {'✅ Completed' if 'gpt4_analysis' in template_data else '❌ Failed'}\n",
        "\n",
        "This template can now be applied to similar rent roll files using the Template Manager.\"\"\"\n",
        "\n",
        "        logger.info(f\"Template creation completed: {template_data['template_id']}\")\n",
        "        return success_message\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error creating template: {str(e)}\"\n",
        "        logger.error(f\"Template creation failed: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return error_msg\n",
        "\n",
        "\n",
        "def end_current_session():\n",
        "    \"\"\"\n",
        "    Manually end the current copiloting session without creating a template.\n",
        "    Useful for starting fresh or when session gets too long.\n",
        "    \"\"\"\n",
        "    global session_recorder\n",
        "\n",
        "    if not session_recorder.current_session_file:\n",
        "        return \"ℹ️ No active session to end.\"\n",
        "\n",
        "    try:\n",
        "        session_data = session_recorder.finalize_session()\n",
        "\n",
        "        if session_data:\n",
        "            session_stats = {\n",
        "                'duration': session_data.get('duration_minutes', 0),\n",
        "                'conversations': len(session_data.get('conversation_history', [])),\n",
        "                'code_executions': len(session_data.get('code_executions', [])),\n",
        "                'versions': len(session_data.get('dataframe_versions', []))\n",
        "            }\n",
        "\n",
        "            return f\"\"\"✅ Session Ended Successfully\n",
        "\n",
        "            📊 Final Session Statistics:\n",
        "            • Session ID: {session_data.get('session_id', 'Unknown')}\n",
        "            • Duration: {session_stats['duration']:.1f} minutes\n",
        "            • Total Conversations: {session_stats['conversations']}\n",
        "            • Code Executions: {session_stats['code_executions']}\n",
        "            • Dataframe Versions: {session_stats['versions']}\n",
        "\n",
        "            💾 Session data saved to: {session_data.get('session_id', 'unknown')}.txt\n",
        "\n",
        "            You can now start a new session or create a template from this completed session.\"\"\"\n",
        "        else:\n",
        "            return \"⚠️ Session ended but no data was saved.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error ending session: {str(e)}\"\n",
        "\n",
        "\n",
        "# Additional helper function to get session status\n",
        "def get_current_session_status():\n",
        "    \"\"\"Get the current session recording status and statistics.\"\"\"\n",
        "    global session_recorder\n",
        "\n",
        "    if not session_recorder.current_session_file:\n",
        "        return \"📴 No active session recording\"\n",
        "\n",
        "    try:\n",
        "        if session_recorder.current_session_data:\n",
        "            data = session_recorder.current_session_data\n",
        "            start_time = datetime.fromisoformat(data.get('start_time', datetime.now().isoformat()))\n",
        "            duration = (datetime.now() - start_time).total_seconds() / 60\n",
        "\n",
        "            status = f\"\"\"📹 Session Recording Active\n",
        "\n",
        "            📊 Current Statistics:\n",
        "            • Session ID: {data.get('session_id', 'Unknown')}\n",
        "            • Duration: {duration:.1f} minutes\n",
        "            • Conversations: {len(data.get('conversation_history', []))}\n",
        "            • Code Executions: {len(data.get('code_executions', []))}\n",
        "            • Versions Created: {len(data.get('dataframe_versions', []))}\n",
        "            • Issues Found: {len(data.get('issues_found', []))}\n",
        "\n",
        "            📁 Recording File: {session_recorder.current_session_file}\n",
        "\n",
        "            All interactions are being automatically recorded for template creation.\"\"\"\n",
        "\n",
        "            return status\n",
        "        else:\n",
        "            return \"📹 Session recording active but no data collected yet\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error getting session status: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jZK7cESeNDd"
      },
      "outputs": [],
      "source": [
        "def view_data():\n",
        "    \"\"\"Return a preview of the rent roll data.\"\"\"\n",
        "    global app_state  # Use app_state instead of agent_state\n",
        "\n",
        "    logger.info(\"View data requested\")\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:  # Note the dictionary access with [\"df\"]\n",
        "        logger.warning(\"View data requested but no data is loaded\")\n",
        "        return \"No rent roll data loaded yet.\"\n",
        "\n",
        "    # Generate HTML representation of the dataframe\n",
        "    logger.info(f\"Generating HTML preview of data with {len(app_state['df'])} rows\")\n",
        "    html = f\"\"\"\n",
        "    <h3>Rent Roll Data</h3>\n",
        "    <p>{len(app_state['df'])} rows × {len(app_state['df'].columns)} columns</p>\n",
        "    {app_state['df'].head(10).fillna('').to_html(index=False)}\n",
        "    \"\"\"\n",
        "\n",
        "    return html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jho5cKJ9ehpb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clear_chat():\n",
        "    \"\"\"Reset the chat history.\"\"\"\n",
        "    logger.info(\"Clearing chat history\")\n",
        "    return []  # Return empty list for Gradio chat history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ofDGmDz8n-"
      },
      "outputs": [],
      "source": [
        "def view_dataframe_versions():\n",
        "    \"\"\"Return HTML showing all versions of the rent roll dataframe.\"\"\"\n",
        "    global app_state\n",
        "    logger.info(\"View dataframe versions requested\")\n",
        "\n",
        "    versions_dir = \"rent_roll_versions\"\n",
        "\n",
        "    if not os.path.exists(versions_dir):\n",
        "        logger.warning(\"No versions directory found\")\n",
        "        return \"No version history found. Please save a version first.\"\n",
        "\n",
        "    # Get all files in the versions directory\n",
        "    try:\n",
        "        all_files = os.listdir(versions_dir)\n",
        "        # Match any CSV file containing rent_roll in the name\n",
        "        version_files = [f for f in all_files if f.endswith('.csv') and 'rent_roll' in f]\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading versions directory: {e}\")\n",
        "        return f\"Error listing versions: {str(e)}\"\n",
        "\n",
        "    if not version_files:\n",
        "        logger.warning(\"No version files found in directory\")\n",
        "        return f\"No version files found in the versions directory ({versions_dir}).\"\n",
        "\n",
        "    # Extract version information\n",
        "    versions = []\n",
        "    for file in version_files:\n",
        "        # Extract the version name from the filename\n",
        "        if file.startswith('rent_roll_v_'):\n",
        "            version_name = file.replace('rent_roll_', '').replace('.csv', '')\n",
        "        else:\n",
        "            version_name = os.path.splitext(file)[0].replace('rent_roll_', '')\n",
        "\n",
        "        # Get file stats\n",
        "        try:\n",
        "            file_path = os.path.join(versions_dir, file)\n",
        "            file_stats = os.stat(file_path)\n",
        "            file_size = file_stats.st_size\n",
        "            modified_time = datetime.fromtimestamp(file_stats.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "            # Try to get row and column counts\n",
        "            df_info = \"\"\n",
        "            try:\n",
        "                temp_df = pd.read_csv(file_path)\n",
        "                df_info = f\"{len(temp_df)} rows × {len(temp_df.columns)} columns\"\n",
        "            except:\n",
        "                df_info = \"Unable to read file\"\n",
        "\n",
        "            # If we have version info in app_state\n",
        "            description = \"\"\n",
        "            is_original = False\n",
        "\n",
        "            for v in app_state.get(\"df_versions\", []):\n",
        "                if v.get(\"name\") == version_name:\n",
        "                    description = v.get(\"description\", \"\")\n",
        "                    is_original = v.get(\"is_original\", False)\n",
        "                    break\n",
        "\n",
        "            # If not found in app_state, use fallback description\n",
        "            if not description and os.path.exists(file_path):\n",
        "                description = \"Found in directory\"\n",
        "\n",
        "            versions.append({\n",
        "                'version_name': version_name,\n",
        "                'file_size': file_size,\n",
        "                'modified_time': modified_time,\n",
        "                'df_info': df_info,\n",
        "                'description': description,\n",
        "                'is_original': is_original,\n",
        "                'file_path': file_path\n",
        "            })\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing version file {file}: {e}\")\n",
        "            versions.append({\n",
        "                'version_name': version_name,\n",
        "                'file_size': 0,\n",
        "                'modified_time': 'Error',\n",
        "                'df_info': f\"Error: {str(e)}\",\n",
        "                'description': '',\n",
        "                'is_original': False,\n",
        "                'file_path': os.path.join(versions_dir, file)\n",
        "            })\n",
        "\n",
        "    # Sort versions by modification time\n",
        "    versions.sort(key=lambda x: x['modified_time'])\n",
        "\n",
        "    # Create basic HTML table without zebra striping\n",
        "    html = \"\"\"\n",
        "    <h3 style=\"color: white;\">Rent Roll Dataframe Version History</h3>\n",
        "    \"\"\"\n",
        "\n",
        "    html += f\"\"\"\n",
        "    <p style=\"color: white;\">Found {len(versions)} version(s) in {versions_dir}</p>\n",
        "    <table border=\"1\" cellpadding=\"5\" cellspacing=\"0\" style=\"width: 100%; border-collapse: collapse; color: white;\">\n",
        "        <thead style=\"background-color: #009879;\">\n",
        "            <tr>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Version Name</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Status</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Created</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Size</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Data</th>\n",
        "                <th style=\"text-align: left; padding: 10px;\">Description</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "    \"\"\"\n",
        "\n",
        "    for i, v in enumerate(versions):\n",
        "        # No alternating rows - all cells have the same background and text color\n",
        "        # Always use dark background with white text for all rows\n",
        "\n",
        "        # Determine status badge\n",
        "        if i == 0 or v.get('is_original'):\n",
        "            status_html = '<span style=\"background-color: #3949ab; color: white; padding: 3px 6px; border-radius: 3px; display: inline-block;\">ORIGINAL</span>'\n",
        "        elif i == len(versions) - 1:\n",
        "            status_html = '<span style=\"background-color: #43a047; color: white; padding: 3px 6px; border-radius: 3px; display: inline-block;\">LATEST</span>'\n",
        "        else:\n",
        "            # Middle version with orange badge\n",
        "            status_html = f'<span style=\"background-color: #f57c00; color: white; padding: 3px 6px; border-radius: 3px; display: inline-block;\">v{i+1}</span>'\n",
        "\n",
        "        # All rows have dark background and white text\n",
        "        html += f\"\"\"\n",
        "        <tr style=\"background-color: #25292e; color: white; border-bottom: 1px solid #333;\">\n",
        "            <td style=\"padding: 10px;\"><code style=\"font-family: monospace; font-weight: bold;\">{v['version_name']}</code></td>\n",
        "            <td style=\"padding: 10px;\">{status_html}</td>\n",
        "            <td style=\"padding: 10px;\">{v['modified_time']}</td>\n",
        "            <td style=\"padding: 10px;\">{round(v['file_size']/1024, 2)} KB</td>\n",
        "            <td style=\"padding: 10px;\">{v['df_info']}</td>\n",
        "            <td style=\"padding: 10px;\">{v['description']}</td>\n",
        "        </tr>\n",
        "        \"\"\"\n",
        "\n",
        "    html += \"\"\"\n",
        "        </tbody>\n",
        "    </table>\n",
        "    \"\"\"\n",
        "\n",
        "    logger.info(f\"Generated version history display with {len(versions)} versions\")\n",
        "    return html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoP4lkp0EWzW"
      },
      "outputs": [],
      "source": [
        "# def analyze_dataframe_changes_with_gpt4(original_df, modified_df, user_description=\"\"):\n",
        "#     \"\"\"\n",
        "#     Use GPT-4.1 to analyze differences between original and modified dataframes,\n",
        "#     generate a detailed description of changes made, AND test Claude prompts.\n",
        "#     NOW USES COMPLETE DATAFRAMES FOR ANALYSIS AND TESTS REPLICATION.\n",
        "#     AUTOMATICALLY SAVES ALL LOGS TO TEXT FILES WHENEVER EXECUTED.\n",
        "#     ENHANCED WITH GPT-4.1 INTELLIGENT FEEDBACK LOOP FOR REPLICATION TESTING.\n",
        "#     \"\"\"\n",
        "#     import os\n",
        "#     import json\n",
        "#     import traceback\n",
        "#     from datetime import datetime\n",
        "\n",
        "#     # Create detailed logging directory\n",
        "#     logs_dir = \"manual_edit_analysis_logs\"\n",
        "#     os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "#     # Generate unique log session ID\n",
        "#     log_session_id = f\"enhanced_edit_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]}\"  # Include milliseconds for uniqueness\n",
        "#     log_file_path = os.path.join(logs_dir, f\"{log_session_id}_detailed_analysis.txt\")\n",
        "\n",
        "#     # Enhanced logging function that ALWAYS saves to file\n",
        "#     def log_to_file(content, section_title=\"\"):\n",
        "#         try:\n",
        "#             with open(log_file_path, 'a', encoding='utf-8') as f:\n",
        "#                 if section_title:\n",
        "#                     f.write(f\"\\n{'='*80}\\n\")\n",
        "#                     f.write(f\"{section_title}\\n\")\n",
        "#                     f.write(f\"{'='*80}\\n\")\n",
        "#                 f.write(f\"{content}\\n\")\n",
        "#                 f.flush()  # Ensure immediate write to disk\n",
        "#         except Exception as e:\n",
        "#             print(f\"ERROR writing to log file: {e}\")\n",
        "\n",
        "#     # ALWAYS initialize comprehensive log - even if function fails later\n",
        "#     try:\n",
        "#         log_to_file(f\"\"\"ENHANCED MANUAL EDIT ANALYSIS LOG WITH GPT-4.1 FEEDBACK LOOP\n",
        "# Session ID: {log_session_id}\n",
        "# Enhancement: Intelligent AI feedback architecture\n",
        "# Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "# User Description: \"{user_description}\"\n",
        "# Log File Path: {log_file_path}\n",
        "\n",
        "# ANALYSIS OVERVIEW:\n",
        "# This log contains the complete workflow of analyzing manual dataframe edits:\n",
        "# 1. Original vs Modified dataframe comparison\n",
        "# 2. GPT-4.1 prompt generation for Claude\n",
        "# 3. Enhanced Claude API calls with intelligent feedback loop\n",
        "# 4. GPT-4.1 gap analysis and prompt improvement\n",
        "# 5. Progressive replication attempts with learning\n",
        "# 6. Final analysis with working prompts embedded\n",
        "\n",
        "# FUNCTION EXECUTION STATUS: STARTING ENHANCED VERSION...\n",
        "# \"\"\", \"ENHANCED SESSION INITIALIZATION\")\n",
        "\n",
        "#         print(f\"📝 Enhanced manual edit analysis log initialized: {log_file_path}\")\n",
        "\n",
        "#     except Exception as init_error:\n",
        "#         print(f\"CRITICAL: Could not initialize log file: {init_error}\")\n",
        "#         # Continue execution even if logging fails\n",
        "\n",
        "#     try:\n",
        "#         # ALWAYS log dataframe information - even if analysis fails later\n",
        "#         log_to_file(f\"\"\"ORIGINAL DATAFRAME COMPLETE ANALYSIS:\n",
        "# Shape: {original_df.shape}\n",
        "# Columns: {list(original_df.columns)}\n",
        "# Data Types: {dict(original_df.dtypes.astype(str))}\n",
        "# Memory Usage: {original_df.memory_usage(deep=True).sum()} bytes\n",
        "# Null Counts: {dict(original_df.isnull().sum())}\n",
        "\n",
        "# FIRST 10 ROWS PREVIEW:\n",
        "# {original_df.head(10).to_string()}\n",
        "\n",
        "# COMPLETE ORIGINAL DATAFRAME (ALL ROWS):\n",
        "# {original_df.to_string(max_rows=None, max_cols=None)}\n",
        "\n",
        "# ORIGINAL DATAFRAME AS CSV:\n",
        "# {original_df.to_csv(index=False)}\n",
        "# \"\"\", \"ORIGINAL DATAFRAME ANALYSIS\")\n",
        "\n",
        "#         log_to_file(f\"\"\"MODIFIED DATAFRAME COMPLETE ANALYSIS:\n",
        "# Shape: {modified_df.shape}\n",
        "# Columns: {list(modified_df.columns)}\n",
        "# Data Types: {dict(modified_df.dtypes.astype(str))}\n",
        "# Memory Usage: {modified_df.memory_usage(deep=True).sum()} bytes\n",
        "# Null Counts: {dict(modified_df.isnull().sum())}\n",
        "\n",
        "# FIRST 10 ROWS PREVIEW:\n",
        "# {modified_df.head(10).to_string()}\n",
        "\n",
        "# COMPLETE MODIFIED DATAFRAME (ALL ROWS):\n",
        "# {modified_df.to_string(max_rows=None, max_cols=None)}\n",
        "\n",
        "# MODIFIED DATAFRAME AS CSV:\n",
        "# {modified_df.to_csv(index=False)}\n",
        "# \"\"\", \"MODIFIED DATAFRAME ANALYSIS\")\n",
        "\n",
        "#         # Initialize OpenAI client\n",
        "#         try:\n",
        "#             client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "#             log_to_file(\"✅ OpenAI client initialized successfully\", \"API CLIENT SETUP\")\n",
        "#         except Exception as client_error:\n",
        "#             log_to_file(f\"❌ Failed to initialize OpenAI client: {client_error}\", \"API CLIENT SETUP ERROR\")\n",
        "#             raise client_error\n",
        "\n",
        "#         # Prepare comparison data for GPT-4 - NOW WITH COMPLETE DATAFRAMES\n",
        "#         original_info = {\n",
        "#             \"shape\": original_df.shape,\n",
        "#             \"columns\": list(original_df.columns),\n",
        "#             \"dtypes\": dict(original_df.dtypes.astype(str)),\n",
        "#             \"full_data\": original_df.to_string(max_rows=None, max_cols=None),  # COMPLETE dataframe\n",
        "#             \"full_csv\": original_df.to_csv(index=False),  # Alternative format\n",
        "#             \"null_counts\": dict(original_df.isnull().sum()),\n",
        "#             \"memory_usage\": original_df.memory_usage(deep=True).sum(),\n",
        "#             \"summary_stats\": original_df.describe(include='all').to_string() if len(original_df) > 0 else \"No data\"\n",
        "#         }\n",
        "\n",
        "#         modified_info = {\n",
        "#             \"shape\": modified_df.shape,\n",
        "#             \"columns\": list(modified_df.columns),\n",
        "#             \"dtypes\": dict(modified_df.dtypes.astype(str)),\n",
        "#             \"full_data\": modified_df.to_string(max_rows=None, max_cols=None),  # COMPLETE dataframe\n",
        "#             \"full_csv\": modified_df.to_csv(index=False),  # Alternative format\n",
        "#             \"null_counts\": dict(modified_df.isnull().sum()),\n",
        "#             \"memory_usage\": modified_df.memory_usage(deep=True).sum(),\n",
        "#             \"summary_stats\": modified_df.describe(include='all').to_string() if len(modified_df) > 0 else \"No data\"\n",
        "#         }\n",
        "\n",
        "#         # Detect specific changes\n",
        "#         shape_changed = original_df.shape != modified_df.shape\n",
        "#         columns_changed = set(original_df.columns) != set(modified_df.columns)\n",
        "\n",
        "#         log_to_file(f\"\"\"STRUCTURAL CHANGES DETECTED:\n",
        "# Shape Changed: {shape_changed}\n",
        "# - Original Shape: {original_df.shape}\n",
        "# - Modified Shape: {modified_df.shape}\n",
        "# - Rows Added: {max(0, modified_df.shape[0] - original_df.shape[0])}\n",
        "# - Rows Removed: {max(0, original_df.shape[0] - modified_df.shape[0])}\n",
        "# - Columns Added: {max(0, modified_df.shape[1] - original_df.shape[1])}\n",
        "# - Columns Removed: {max(0, original_df.shape[1] - modified_df.shape[1])}\n",
        "\n",
        "# Columns Changed: {columns_changed}\n",
        "# - Original Columns: {list(original_df.columns)}\n",
        "# - Modified Columns: {list(modified_df.columns)}\n",
        "# - Added Columns: {list(set(modified_df.columns) - set(original_df.columns))}\n",
        "# - Removed Columns: {list(set(original_df.columns) - set(modified_df.columns))}\n",
        "# \"\"\", \"STRUCTURAL CHANGE ANALYSIS\")\n",
        "\n",
        "#         # COMPLETE cell-by-cell comparison - ANALYZE EVERY SINGLE CELL\n",
        "#         data_changes_detected = False\n",
        "#         changed_cells = []\n",
        "#         total_changes = 0\n",
        "#         changed_rows = set()\n",
        "#         changed_columns = set()\n",
        "\n",
        "#         if original_df.shape == modified_df.shape and list(original_df.columns) == list(modified_df.columns):\n",
        "#             print(f\"🔍 Comparing ALL {len(original_df)} rows and {len(original_df.columns)} columns...\")\n",
        "#             log_to_file(f\"\"\"STARTING COMPLETE CELL-BY-CELL COMPARISON:\n",
        "# Total cells to compare: {original_df.shape[0] * original_df.shape[1]}\n",
        "# Comparing {len(original_df)} rows × {len(original_df.columns)} columns\n",
        "# This may take time for large dataframes...\n",
        "# \"\"\", \"CELL-BY-CELL COMPARISON START\")\n",
        "\n",
        "#             # Compare EVERY cell in the entire dataframe\n",
        "#             for i in range(len(original_df)):\n",
        "#                 row_has_changes = False\n",
        "#                 row_changes = []\n",
        "\n",
        "#                 for col in original_df.columns:\n",
        "#                     try:\n",
        "#                         orig_val = original_df.iloc[i][col]\n",
        "#                         mod_val = modified_df.iloc[i][col]\n",
        "\n",
        "#                         # Handle NaN comparisons\n",
        "#                         if pd.isna(orig_val) and pd.isna(mod_val):\n",
        "#                             continue\n",
        "#                         elif pd.isna(orig_val) or pd.isna(mod_val):\n",
        "#                             data_changes_detected = True\n",
        "#                             total_changes += 1\n",
        "#                             row_has_changes = True\n",
        "#                             changed_columns.add(col)\n",
        "#                             change_detail = {\n",
        "#                                 \"row\": i,\n",
        "#                                 \"column\": col,\n",
        "#                                 \"original\": str(orig_val),\n",
        "#                                 \"modified\": str(mod_val),\n",
        "#                                 \"change_type\": \"nan_change\"\n",
        "#                             }\n",
        "#                             row_changes.append(change_detail)\n",
        "#                             # Store ALL changes, not just first 50\n",
        "#                             changed_cells.append(change_detail)\n",
        "#                         elif str(orig_val).strip() != str(mod_val).strip():\n",
        "#                             data_changes_detected = True\n",
        "#                             total_changes += 1\n",
        "#                             row_has_changes = True\n",
        "#                             changed_columns.add(col)\n",
        "#                             change_detail = {\n",
        "#                                 \"row\": i,\n",
        "#                                 \"column\": col,\n",
        "#                                 \"original\": str(orig_val),\n",
        "#                                 \"modified\": str(mod_val),\n",
        "#                                 \"change_type\": \"value_change\"\n",
        "#                             }\n",
        "#                             row_changes.append(change_detail)\n",
        "#                             # Store ALL changes, not just first 50\n",
        "#                             changed_cells.append(change_detail)\n",
        "#                     except Exception as e:\n",
        "#                         log_to_file(f\"ERROR comparing cell at row {i}, column '{col}': {str(e)}\")\n",
        "#                         continue\n",
        "\n",
        "#                 if row_has_changes:\n",
        "#                     changed_rows.add(i)\n",
        "#                     # Log each changed row immediately\n",
        "#                     log_to_file(f\"\"\"ROW {i} CHANGES ({len(row_changes)} changes):\n",
        "# {json.dumps(row_changes, indent=2)}\n",
        "# \"\"\")\n",
        "\n",
        "#                 # Log progress every 100 rows for large dataframes\n",
        "#                 if (i + 1) % 100 == 0:\n",
        "#                     log_to_file(f\"Progress: Processed {i + 1}/{len(original_df)} rows, found {total_changes} changes so far\")\n",
        "\n",
        "#             print(f\"✅ Complete comparison finished: {total_changes} total changes detected across {len(changed_rows)} rows\")\n",
        "\n",
        "#             # Log comprehensive change analysis\n",
        "#             log_to_file(f\"\"\"COMPLETE CELL-BY-CELL COMPARISON RESULTS:\n",
        "# =====================================\n",
        "# Total Changes Detected: {total_changes}\n",
        "# Affected Rows: {len(changed_rows)} out of {original_df.shape[0]} ({len(changed_rows)/original_df.shape[0]*100:.1f}%)\n",
        "# Affected Columns: {len(changed_columns)} out of {len(original_df.columns)} ({len(changed_columns)/len(original_df.columns)*100:.1f}%)\n",
        "\n",
        "# AFFECTED COLUMNS LIST:\n",
        "# {list(changed_columns)}\n",
        "\n",
        "# AFFECTED ROWS LIST:\n",
        "# {sorted(list(changed_rows))}\n",
        "\n",
        "# ALL DETECTED CHANGES ({len(changed_cells)} total):\n",
        "# \"\"\", \"COMPREHENSIVE CHANGE DETECTION RESULTS\")\n",
        "\n",
        "#             # Log ALL changes, not just a sample\n",
        "#             for i, change in enumerate(changed_cells):\n",
        "#                 log_to_file(f\"Change {i+1}: Row {change['row']}, Column '{change['column']}' ({change['change_type']}): '{change['original']}' → '{change['modified']}'\")\n",
        "\n",
        "#         else:\n",
        "#             log_to_file(f\"\"\"CANNOT PERFORM CELL-BY-CELL COMPARISON:\n",
        "# Reason: Shape or column structure differs\n",
        "# Original shape: {original_df.shape}\n",
        "# Modified shape: {modified_df.shape}\n",
        "# Original columns: {list(original_df.columns)}\n",
        "# Modified columns: {list(modified_df.columns)}\n",
        "# \"\"\", \"CELL-BY-CELL COMPARISON SKIPPED\")\n",
        "\n",
        "#         # Calculate change statistics\n",
        "#         total_cells = original_df.shape[0] * original_df.shape[1] if original_df.size > 0 else 1\n",
        "#         change_density = total_changes / total_cells\n",
        "\n",
        "#         log_to_file(f\"\"\"COMPREHENSIVE CHANGE STATISTICS:\n",
        "# ===============================\n",
        "# Total Cells in Original: {total_cells}\n",
        "# Total Cells Changed: {total_changes}\n",
        "# Change Density: {change_density*100:.4f}%\n",
        "# Percentage of Rows Affected: {len(changed_rows)/original_df.shape[0]*100:.2f}% ({len(changed_rows)}/{original_df.shape[0]})\n",
        "# Percentage of Columns Affected: {len(changed_columns)/len(original_df.columns)*100:.2f}% ({len(changed_columns)}/{len(original_df.columns)})\n",
        "\n",
        "# CHANGE PATTERN ANALYSIS:\n",
        "# - NaN Changes: {len([c for c in changed_cells if c.get('change_type') == 'nan_change'])}\n",
        "# - Value Changes: {len([c for c in changed_cells if c.get('change_type') == 'value_change'])}\n",
        "# - Most Affected Columns: {sorted(changed_columns)[:10]}\n",
        "# - Row Change Distribution: Every {original_df.shape[0]//max(1,len(changed_rows)):.0f} rows on average\n",
        "# \"\"\", \"COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
        "\n",
        "#         # STEP 1: Use GPT-4.1 to analyze and generate Claude prompt\n",
        "#         print(\"🧠 GPT-4.1: Analyzing changes and generating Claude prompt...\")\n",
        "#         log_to_file(\"🧠 STARTING GPT-4.1 ANALYSIS TO GENERATE CLAUDE PROMPT...\", \"GPT-4.1 PROMPT GENERATION START\")\n",
        "\n",
        "#         claude_prompt = _generate_claude_prompt_with_gpt4_logged(\n",
        "#             client, original_info, modified_info, user_description,\n",
        "#             total_changes, changed_rows, changed_columns, changed_cells,\n",
        "#             log_to_file\n",
        "#         )\n",
        "\n",
        "#         # STEP 2: Test the Claude prompt with ENHANCED GPT-4.1 FEEDBACK LOOP\n",
        "#         print(\"🧠 Testing Claude prompt with GPT-4.1 intelligent feedback loop...\")\n",
        "#         log_to_file(\"🧠 STARTING ENHANCED CLAUDE REPLICATION WITH AI FEEDBACK LOOP...\", \"ENHANCED CLAUDE REPLICATION START\")\n",
        "\n",
        "#         replication_results = _test_claude_prompt_replication_logged(\n",
        "#             original_df, modified_df, claude_prompt, max_attempts=3, log_to_file=log_to_file\n",
        "#         )\n",
        "\n",
        "#         # STEP 3: Get final analysis with working prompts embedded\n",
        "#         log_to_file(\"📊 GENERATING FINAL ANALYSIS WITH GPT-4.1...\", \"FINAL ANALYSIS GENERATION START\")\n",
        "\n",
        "#         final_analysis = _get_final_analysis_with_prompts_logged(\n",
        "#             client, original_info, modified_info, user_description,\n",
        "#             total_changes, changed_rows, changed_columns, changed_cells,\n",
        "#             claude_prompt, replication_results, log_to_file\n",
        "#         )\n",
        "\n",
        "#         # Add technical metadata and log file reference\n",
        "#         final_analysis[\"raw_gpt_response\"] = final_analysis.get(\"raw_gpt_response\", \"\")\n",
        "#         final_analysis[\"complete_comparison_performed\"] = True\n",
        "#         final_analysis[\"log_file_path\"] = log_file_path  # ALWAYS include log file path\n",
        "#         final_analysis[\"enhancement_method\"] = \"gpt4_feedback_loop\"\n",
        "#         final_analysis[\"full_change_statistics\"] = {\n",
        "#             \"total_cells\": total_cells,\n",
        "#             \"total_changes\": total_changes,\n",
        "#             \"change_density\": change_density,\n",
        "#             \"affected_rows\": list(changed_rows),\n",
        "#             \"affected_columns\": list(changed_columns),\n",
        "#             \"replication_tested\": True,\n",
        "#             \"replication_success\": replication_results[\"final_success\"],\n",
        "#             \"replication_attempts\": len(replication_results[\"attempts\"]),\n",
        "#             \"all_changes\": changed_cells,  # Include ALL detected changes\n",
        "#             \"progressive_improvement\": len([a for a in replication_results[\"attempts\"] if a.get(\"enhancement_type\") == \"gpt4_enhanced\"]) > 0\n",
        "#         }\n",
        "\n",
        "#         # Log final comprehensive results\n",
        "#         log_to_file(f\"\"\"ENHANCED ANALYSIS EXECUTION COMPLETED SUCCESSFULLY\n",
        "# =====================================================\n",
        "# Enhancement Method: GPT-4.1 Intelligent Feedback Loop\n",
        "# Total Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "# Log File Saved: {log_file_path}\n",
        "# Log File Size: {os.path.getsize(log_file_path)} bytes\n",
        "\n",
        "# ENHANCED FINAL RESULTS SUMMARY:\n",
        "# - Replication Success: {replication_results[\"final_success\"]}\n",
        "# - Best Match Score: {replication_results.get(\"best_attempt\", {}).get(\"match_score\", 0):.2%}\n",
        "# - Total Replication Attempts: {len(replication_results[\"attempts\"])}\n",
        "# - Progressive Improvement: {'Yes' if final_analysis[\"full_change_statistics\"][\"progressive_improvement\"] else 'No'}\n",
        "# - Changes Detected: {total_changes}\n",
        "# - Change Density: {change_density*100:.4f}%\n",
        "\n",
        "# ENHANCEMENT STATISTICS:\n",
        "# - GPT-4.1 Enhanced Attempts: {len([a for a in replication_results[\"attempts\"] if a.get(\"enhancement_type\") == \"gpt4_enhanced\"])}\n",
        "# - Gap Analysis Performed: {len([a for a in replication_results[\"attempts\"] if a.get(\"gap_analysis\")])}\n",
        "# - Prompt Improvements: {len([a for a in replication_results[\"attempts\"] if a.get(\"enhancement_type\") == \"gpt4_enhanced\"])}\n",
        "\n",
        "# COMPLETE FINAL ANALYSIS OBJECT:\n",
        "# {json.dumps(final_analysis, indent=2, default=str)}\n",
        "\n",
        "# ✅ ENHANCED ANALYSIS COMPLETE - ALL LOGS SAVED TO: {log_file_path}\n",
        "# \"\"\", \"ENHANCED FINAL EXECUTION RESULTS\")\n",
        "\n",
        "#         print(f\"📝 Enhanced complete analysis with all logs saved to: {log_file_path}\")\n",
        "#         return final_analysis\n",
        "\n",
        "#     except Exception as e:\n",
        "#         # ALWAYS log errors, even if everything else fails\n",
        "#         error_details = f\"\"\"CRITICAL ERROR DURING ENHANCED ANALYSIS EXECUTION\n",
        "# ========================================================\n",
        "# Error Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "# Error Type: {type(e).__name__}\n",
        "# Error Message: {str(e)}\n",
        "\n",
        "# FULL STACK TRACE:\n",
        "# {traceback.format_exc()}\n",
        "\n",
        "# EXECUTION CONTEXT:\n",
        "# - User Description: \"{user_description}\"\n",
        "# - Original DF Shape: {original_df.shape if 'original_df' in locals() else 'Unknown'}\n",
        "# - Modified DF Shape: {modified_df.shape if 'modified_df' in locals() else 'Unknown'}\n",
        "# - Enhancement Method: GPT-4.1 Feedback Loop\n",
        "# - Log File: {log_file_path}\n",
        "\n",
        "# ❌ ENHANCED ANALYSIS FAILED - ERROR LOGGED TO: {log_file_path}\n",
        "# \"\"\"\n",
        "\n",
        "#         log_to_file(error_details, \"CRITICAL ENHANCED ERROR\")\n",
        "\n",
        "#         error_msg = f\"Error in enhanced GPT-4 dataframe analysis: {e}\"\n",
        "#         logger.error(error_msg)\n",
        "#         print(f\"❌ Enhanced analysis failed but logs saved to: {log_file_path}\")\n",
        "\n",
        "#         return {\n",
        "#             \"change_summary\": f\"Enhanced dataframe analysis failed: {user_description}\",\n",
        "#             \"change_type\": \"data_edit\",\n",
        "#             \"session_description\": f\"User made changes to entire dataframe. Description: {user_description}. Error in enhanced analysis: {str(e)}\",\n",
        "#             \"error\": str(e),\n",
        "#             \"complete_comparison_performed\": False,\n",
        "#             \"enhancement_method\": \"gpt4_feedback_loop\",\n",
        "#             \"log_file_path\": log_file_path,  # ALWAYS include log file path, even on error\n",
        "#             \"error_logged\": True\n",
        "#         }\n",
        "\n",
        "\n",
        "# def _generate_claude_prompt_with_gpt4_logged(client, original_info, modified_info, user_description,\n",
        "#                                            total_changes, changed_rows, changed_columns, changed_cells,\n",
        "#                                            log_to_file):\n",
        "#     \"\"\"\n",
        "#     Enhanced version with comprehensive logging of GPT-4.1 prompt generation\n",
        "#     \"\"\"\n",
        "#     prompt_generation_request = f\"\"\"\n",
        "#     You are an expert at analyzing dataframe changes and generating precise prompts for Claude 3.7 to replicate manual edits.\n",
        "\n",
        "#     ORIGINAL DATAFRAME:\n",
        "#     {original_info['full_data']}\n",
        "\n",
        "#     MODIFIED DATAFRAME:\n",
        "#     {modified_info['full_data']}\n",
        "\n",
        "#     CHANGE ANALYSIS:\n",
        "#     - Total changes: {total_changes}\n",
        "#     - Changed rows: {list(changed_rows)[:20] if changed_rows else []}\n",
        "#     - Changed columns: {list(changed_columns)}\n",
        "#     - Sample changes: {changed_cells[:10]}\n",
        "#     - User description: \"{user_description}\"\n",
        "\n",
        "#     Generate a PRECISE prompt for Claude 3.7 that would replicate these exact changes.\n",
        "#     Focus on:\n",
        "#     1. Specific column names and filtering criteria\n",
        "#     2. Exact transformation logic\n",
        "#     3. Clear, executable instructions\n",
        "#     4. Business context for rent roll data\n",
        "\n",
        "#     Return ONLY the Claude prompt text, nothing else.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Log the complete GPT-4.1 prompt\n",
        "#     log_to_file(f\"\"\"GPT-4.1 PROMPT TO GENERATE CLAUDE INSTRUCTIONS:\n",
        "# Model: gpt-4.1\n",
        "# Temperature: 0.1\n",
        "# Max Tokens: 3000\n",
        "\n",
        "# COMPLETE PROMPT SENT TO GPT-4.1:\n",
        "# {'-'*60}\n",
        "# {prompt_generation_request}\n",
        "# {'-'*60}\n",
        "# \"\"\", \"GPT-4.1 REQUEST\")\n",
        "\n",
        "#     try:\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4.1\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": \"You are an expert at generating precise data transformation prompts. Return only the Claude prompt text.\"},\n",
        "#                 {\"role\": \"user\", \"content\": prompt_generation_request}\n",
        "#             ],\n",
        "#             max_tokens=3000,\n",
        "#             temperature=0.3\n",
        "#         )\n",
        "\n",
        "#         gpt_response = response.choices[0].message.content.strip()\n",
        "\n",
        "#         # Log GPT-4.1 response\n",
        "#         log_to_file(f\"\"\"GPT-4.1 RESPONSE (CLAUDE PROMPT):\n",
        "# Response Length: {len(gpt_response)} characters\n",
        "# Tokens Used: Approximately {len(gpt_response.split())} words\n",
        "\n",
        "# GENERATED CLAUDE PROMPT:\n",
        "# {'-'*60}\n",
        "# {gpt_response}\n",
        "# {'-'*60}\n",
        "# \"\"\", \"GPT-4.1 RESPONSE\")\n",
        "\n",
        "#         return gpt_response\n",
        "\n",
        "#     except Exception as e:\n",
        "#         error_msg = f\"GPT-4.1 API Error: {str(e)}\"\n",
        "#         log_to_file(f\"\"\"GPT-4.1 API CALL FAILED:\n",
        "# Error: {error_msg}\n",
        "# Fallback: Using basic prompt\n",
        "# \"\"\")\n",
        "#         return f\"Replicate the manual edits described as: {user_description}\"\n",
        "\n",
        "\n",
        "# def _test_claude_prompt_replication_logged(original_df, target_df, claude_prompt, max_attempts=3, log_to_file=None):\n",
        "#     \"\"\"\n",
        "#     ENHANCED version with GPT-4.1 intelligent feedback loop for Claude replication testing.\n",
        "\n",
        "#     Architecture:\n",
        "#     GPT-4.1 (Prompt Generator) → Claude 3.7 (Code Executor) → GPT-4.1 (Gap Analyzer) → GPT-4.1 (Prompt Improver)\n",
        "#     \"\"\"\n",
        "#     attempts = []\n",
        "#     current_prompt = claude_prompt\n",
        "\n",
        "#     log_to_file(f\"\"\"ENHANCED AI FEEDBACK LOOP REPLICATION TESTING:\n",
        "# Architecture: GPT-4.1 → Claude 3.7 → GPT-4.1 (Analyzer) → GPT-4.1 (Improver)\n",
        "# Maximum Attempts: {max_attempts}\n",
        "# Target DataFrame Shape: {target_df.shape}\n",
        "# Original DataFrame Shape: {original_df.shape}\n",
        "\n",
        "# INITIAL CLAUDE PROMPT:\n",
        "# {'-'*60}\n",
        "# {claude_prompt}\n",
        "# {'-'*60}\n",
        "# \"\"\", \"ENHANCED REPLICATION TESTING\")\n",
        "\n",
        "#     for attempt in range(max_attempts):\n",
        "#         print(f\"🔄 Enhanced Attempt {attempt + 1}/{max_attempts}\")\n",
        "#         log_to_file(f\"Starting enhanced attempt {attempt + 1}/{max_attempts}...\", f\"ENHANCED ATTEMPT {attempt + 1}\")\n",
        "\n",
        "#         try:\n",
        "#             # STEP 1: Claude executes the current prompt\n",
        "#             claude_response = _call_actual_claude_logged(current_prompt, original_df, log_to_file, attempt + 1)\n",
        "#             code_blocks = _extract_code_blocks(claude_response)\n",
        "\n",
        "#             if not code_blocks:\n",
        "#                 attempts.append({\n",
        "#                     \"attempt\": attempt + 1,\n",
        "#                     \"success\": False,\n",
        "#                     \"error\": \"No code found in Claude response\",\n",
        "#                     \"prompt_used\": current_prompt,\n",
        "#                     \"enhancement_type\": \"no_code_error\"\n",
        "#                 })\n",
        "#                 log_to_file(\"No code blocks found in Claude response\")\n",
        "#                 continue\n",
        "\n",
        "#             # Execute Claude's code\n",
        "#             test_df = original_df.copy()\n",
        "#             exec_globals = {\"df\": test_df, \"pd\": pd, \"np\": np, \"os\": os, \"datetime\": datetime}\n",
        "\n",
        "#             execution_output = \"\"\n",
        "#             for i, code in enumerate(code_blocks):\n",
        "#                 try:\n",
        "#                     exec(code, exec_globals)\n",
        "#                     execution_output += f\"Code block {i+1} executed successfully\\n\"\n",
        "#                 except Exception as exec_error:\n",
        "#                     execution_output += f\"Code block {i+1} failed: {str(exec_error)}\\n\"\n",
        "\n",
        "#             result_df = exec_globals[\"df\"]\n",
        "#             match_score = _calculate_match_score(target_df, result_df)\n",
        "\n",
        "#             log_to_file(f\"\"\"CLAUDE EXECUTION RESULTS - ATTEMPT {attempt + 1}:\n",
        "# Match Score: {match_score:.2%}\n",
        "# Result Shape: {result_df.shape}\n",
        "# Target Shape: {target_df.shape}\n",
        "# Execution Output: {execution_output}\n",
        "# \"\"\")\n",
        "\n",
        "#             # STEP 2: Check if successful or needs GPT-4.1 enhancement\n",
        "#             if match_score >= 0.95:\n",
        "#                 attempts.append({\n",
        "#                     \"attempt\": attempt + 1,\n",
        "#                     \"success\": True,\n",
        "#                     \"match_score\": match_score,\n",
        "#                     \"generated_code\": code_blocks,\n",
        "#                     \"prompt_used\": current_prompt,\n",
        "#                     \"result_shape\": result_df.shape,\n",
        "#                     \"target_shape\": target_df.shape,\n",
        "#                     \"execution_output\": execution_output,\n",
        "#                     \"enhancement_type\": \"success\"\n",
        "#                 })\n",
        "#                 log_to_file(\"🎉 REPLICATION SUCCESSFUL! Match score >= 95%\")\n",
        "#                 break\n",
        "#             else:\n",
        "#                 # STEP 3: GPT-4.1 analyzes why replication failed\n",
        "#                 gap_analysis = _analyze_replication_gaps_with_gpt4(\n",
        "#                     target_df, result_df, current_prompt, match_score, log_to_file, attempt + 1\n",
        "#                 )\n",
        "\n",
        "#                 # STEP 4: GPT-4.1 improves the prompt based on gap analysis\n",
        "#                 if attempt < max_attempts - 1:  # Don't improve on last attempt\n",
        "#                     improved_prompt = _improve_claude_prompt_with_gpt4_analysis(\n",
        "#                         current_prompt, gap_analysis, attempt + 1, log_to_file\n",
        "#                     )\n",
        "#                     current_prompt = improved_prompt\n",
        "\n",
        "#                 attempts.append({\n",
        "#                     \"attempt\": attempt + 1,\n",
        "#                     \"success\": False,\n",
        "#                     \"match_score\": match_score,\n",
        "#                     \"generated_code\": code_blocks,\n",
        "#                     \"prompt_used\": current_prompt,\n",
        "#                     \"result_shape\": result_df.shape,\n",
        "#                     \"target_shape\": target_df.shape,\n",
        "#                     \"execution_output\": execution_output,\n",
        "#                     \"gap_analysis\": gap_analysis,\n",
        "#                     \"enhancement_type\": \"gpt4_enhanced\"\n",
        "#                 })\n",
        "\n",
        "#         except Exception as e:\n",
        "#             attempts.append({\n",
        "#                 \"attempt\": attempt + 1,\n",
        "#                 \"success\": False,\n",
        "#                 \"error\": str(e),\n",
        "#                 \"prompt_used\": current_prompt,\n",
        "#                 \"enhancement_type\": \"execution_error\"\n",
        "#             })\n",
        "#             log_to_file(f\"ATTEMPT {attempt + 1} FAILED with error: {str(e)}\")\n",
        "\n",
        "#     # Final results\n",
        "#     final_success = any(attempt[\"success\"] for attempt in attempts)\n",
        "#     best_attempt = max(attempts, key=lambda x: x.get(\"match_score\", 0)) if attempts else None\n",
        "\n",
        "#     log_to_file(f\"\"\"ENHANCED REPLICATION TESTING COMPLETE:\n",
        "# Final Success: {final_success}\n",
        "# Best Match Score: {best_attempt.get('match_score', 0):.2% if best_attempt else 'N/A'}\n",
        "# Enhancement Method: GPT-4.1 Intelligent Feedback Loop\n",
        "# Progressive Improvement: {len([a for a in attempts if a.get('enhancement_type') == 'gpt4_enhanced'])} enhanced attempts\n",
        "# \"\"\", \"ENHANCED REPLICATION RESULTS\")\n",
        "\n",
        "#     return {\n",
        "#         \"attempts\": attempts,\n",
        "#         \"final_success\": final_success,\n",
        "#         \"best_attempt\": best_attempt,\n",
        "#         \"final_prompt\": current_prompt,\n",
        "#         \"enhancement_method\": \"gpt4_feedback_loop\"\n",
        "#     }\n",
        "\n",
        "\n",
        "# def _analyze_replication_gaps_with_gpt4(target_df, result_df, original_prompt, match_score, log_to_file, attempt_number):\n",
        "#     \"\"\"\n",
        "#     GPT-4.1 analyzes exactly why the replication failed and identifies specific gaps.\n",
        "#     \"\"\"\n",
        "#     log_to_file(f\"\"\"STARTING GPT-4.1 GAP ANALYSIS - ATTEMPT {attempt_number}:\n",
        "# Analyzing why match score is {match_score:.2%} instead of 95%+\n",
        "# \"\"\", f\"GPT-4.1 GAP ANALYSIS {attempt_number}\")\n",
        "\n",
        "#     try:\n",
        "#         # Calculate specific differences\n",
        "#         differences = _identify_specific_differences(target_df, result_df)\n",
        "\n",
        "#         gap_analysis_prompt = f\"\"\"You are a data analysis expert. Analyze why this dataframe replication failed.\n",
        "\n",
        "# TARGET DATAFRAME (what user wanted):\n",
        "# Shape: {target_df.shape}\n",
        "# Columns: {list(target_df.columns)}\n",
        "# First 10 rows:\n",
        "# {target_df.head(10).to_string()}\n",
        "\n",
        "# Full target data:\n",
        "# {target_df.to_string() if len(target_df) <= 50 else target_df.to_string()[:3000] + \"... [truncated]\"}\n",
        "\n",
        "# ACTUAL RESULT (what Claude produced):\n",
        "# Shape: {result_df.shape}\n",
        "# Columns: {list(result_df.columns)}\n",
        "# First 10 rows:\n",
        "# {result_df.head(10).to_string()}\n",
        "\n",
        "# Full result data:\n",
        "# {result_df.to_string() if len(result_df) <= 50 else result_df.to_string()[:3000] + \"... [truncated]\"}\n",
        "\n",
        "# CLAUDE PROMPT THAT FAILED:\n",
        "# {original_prompt}\n",
        "\n",
        "# MATCH SCORE: {match_score:.2%} (Target: 95%+)\n",
        "\n",
        "# SPECIFIC DIFFERENCES DETECTED:\n",
        "# {json.dumps(differences, indent=2)}\n",
        "\n",
        "# ANALYSIS REQUIRED:\n",
        "# Identify the TOP 3 root causes for this replication failure. For each cause:\n",
        "# 1. What specific operation failed?\n",
        "# 2. Why did it fail?\n",
        "# 3. What should have happened instead?\n",
        "# 4. What prompt instruction was unclear or missing?\n",
        "\n",
        "# Focus on actionable insights for improving the Claude prompt.\n",
        "\n",
        "# Return analysis in this JSON format:\n",
        "# {{\n",
        "#     \"top_3_issues\": [\n",
        "#         {{\n",
        "#             \"issue\": \"Brief description\",\n",
        "#             \"details\": \"Detailed explanation\",\n",
        "#             \"prompt_problem\": \"What was wrong with the prompt\",\n",
        "#             \"fix_needed\": \"Specific fix required\"\n",
        "#         }}\n",
        "#     ],\n",
        "#     \"overall_diagnosis\": \"Summary of main problem\",\n",
        "#     \"confidence\": \"high|medium|low\"\n",
        "# }}\n",
        "# \"\"\"\n",
        "\n",
        "#         log_to_file(f\"\"\"GAP ANALYSIS PROMPT TO GPT-4.1:\n",
        "# {'-'*60}\n",
        "# {gap_analysis_prompt}\n",
        "# {'-'*60}\n",
        "# \"\"\")\n",
        "\n",
        "#         # Call GPT-4.1 for gap analysis\n",
        "#         client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4.1\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": \"You are an expert at analyzing dataframe replication failures. Return only valid JSON.\"},\n",
        "#                 {\"role\": \"user\", \"content\": gap_analysis_prompt}\n",
        "#             ],\n",
        "#             max_tokens=2000,\n",
        "#             temperature=0.1\n",
        "#         )\n",
        "\n",
        "#         gap_analysis_response = response.choices[0].message.content\n",
        "\n",
        "#         log_to_file(f\"\"\"GPT-4.1 GAP ANALYSIS RESPONSE:\n",
        "# {'-'*60}\n",
        "# {gap_analysis_response}\n",
        "# {'-'*60}\n",
        "# \"\"\")\n",
        "\n",
        "#         # Parse JSON response\n",
        "#         try:\n",
        "#             import re\n",
        "#             json_match = re.search(r'{.*}', gap_analysis_response, re.DOTALL)\n",
        "#             if json_match:\n",
        "#                 gap_analysis = json.loads(json_match.group(0))\n",
        "#                 log_to_file(\"✅ Successfully parsed gap analysis JSON\")\n",
        "#                 return gap_analysis\n",
        "#             else:\n",
        "#                 log_to_file(\"❌ No JSON found in gap analysis response\")\n",
        "#         except Exception as json_error:\n",
        "#             log_to_file(f\"❌ JSON parsing failed: {str(json_error)}\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         log_to_file(f\"❌ Gap analysis failed: {str(e)}\")\n",
        "\n",
        "#     # Fallback gap analysis\n",
        "#     fallback_analysis = {\n",
        "#         \"top_3_issues\": [\n",
        "#             {\n",
        "#                 \"issue\": f\"Match score only {match_score:.2%}\",\n",
        "#                 \"details\": \"Automated analysis failed, using fallback\",\n",
        "#                 \"prompt_problem\": \"Unknown specific issue\",\n",
        "#                 \"fix_needed\": \"Manual prompt review required\"\n",
        "#             }\n",
        "#         ],\n",
        "#         \"overall_diagnosis\": \"Gap analysis failed, using basic feedback\",\n",
        "#         \"confidence\": \"low\"\n",
        "#     }\n",
        "\n",
        "#     log_to_file(f\"Using fallback gap analysis: {json.dumps(fallback_analysis, indent=2)}\")\n",
        "#     return fallback_analysis\n",
        "\n",
        "\n",
        "# def _improve_claude_prompt_with_gpt4_analysis(current_prompt, gap_analysis, attempt_number, log_to_file):\n",
        "#     \"\"\"\n",
        "#     GPT-4.1 improves the Claude prompt based on intelligent gap analysis.\n",
        "#     \"\"\"\n",
        "#     log_to_file(f\"\"\"STARTING GPT-4.1 PROMPT IMPROVEMENT - ATTEMPT {attempt_number}:\n",
        "# Using intelligent gap analysis to surgically improve the prompt\n",
        "# \"\"\", f\"GPT-4.1 PROMPT IMPROVEMENT {attempt_number}\")\n",
        "\n",
        "#     try:\n",
        "#         improvement_prompt = f\"\"\"You are a prompt engineering expert. Improve this Claude prompt based on specific failure analysis.\n",
        "\n",
        "# CURRENT CLAUDE PROMPT (that failed):\n",
        "# {current_prompt}\n",
        "\n",
        "# GAP ANALYSIS FROM PREVIOUS ATTEMPT:\n",
        "# {json.dumps(gap_analysis, indent=2)}\n",
        "\n",
        "# ATTEMPT NUMBER: {attempt_number}/3\n",
        "\n",
        "# TASK: Generate an improved Claude prompt that specifically addresses the identified issues.\n",
        "\n",
        "# REQUIREMENTS:\n",
        "# 1. Fix the TOP 3 issues identified in the gap analysis\n",
        "# 2. Make surgical improvements (don't rewrite everything)\n",
        "# 3. Add specific instructions for the problem areas\n",
        "# 4. Maintain the original intent while fixing the failures\n",
        "# 5. Be more explicit about edge cases and data handling\n",
        "\n",
        "# Focus on the most impactful improvements that will raise the match score above 95%.\n",
        "\n",
        "# Return ONLY the improved Claude prompt text, nothing else.\n",
        "# \"\"\"\n",
        "\n",
        "#         log_to_file(f\"\"\"PROMPT IMPROVEMENT REQUEST TO GPT-4.1:\n",
        "# {'-'*60}\n",
        "# {improvement_prompt}\n",
        "# {'-'*60}\n",
        "# \"\"\")\n",
        "\n",
        "#         # Call GPT-4.1 for prompt improvement\n",
        "#         client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4.1\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": \"You are an expert prompt engineer. Return only the improved Claude prompt.\"},\n",
        "#                 {\"role\": \"user\", \"content\": improvement_prompt}\n",
        "#             ],\n",
        "#             max_tokens=2000,\n",
        "#             temperature=0.3\n",
        "#         )\n",
        "\n",
        "#         improved_prompt = response.choices[0].message.content.strip()\n",
        "\n",
        "#         log_to_file(f\"\"\"GPT-4.1 IMPROVED PROMPT:\n",
        "# {'-'*60}\n",
        "# {improved_prompt}\n",
        "# {'-'*60}\n",
        "# \"\"\")\n",
        "\n",
        "#         return improved_prompt\n",
        "\n",
        "#     except Exception as e:\n",
        "#         log_to_file(f\"❌ Prompt improvement failed: {str(e)}\")\n",
        "\n",
        "#         # Fallback improvement (basic)\n",
        "#         fallback_improvement = f\"\"\"{current_prompt}\n",
        "\n",
        "# ENHANCED INSTRUCTIONS BASED ON PREVIOUS ATTEMPT:\n",
        "# - Double-check all filtering conditions\n",
        "# - Ensure exact data type matching\n",
        "# - Handle edge cases more carefully\n",
        "# - Verify all column operations\n",
        "\n",
        "# Previous attempt had {gap_analysis.get('overall_diagnosis', 'unknown issues')}.\"\"\"\n",
        "\n",
        "#         log_to_file(f\"Using fallback prompt improvement\")\n",
        "#         return fallback_improvement\n",
        "\n",
        "\n",
        "# def _identify_specific_differences(target_df, result_df):\n",
        "#     \"\"\"\n",
        "#     Identify specific differences between target and result dataframes.\n",
        "#     \"\"\"\n",
        "#     differences = {\n",
        "#         \"shape_differences\": {\n",
        "#             \"target_shape\": target_df.shape,\n",
        "#             \"result_shape\": result_df.shape,\n",
        "#             \"rows_diff\": result_df.shape[0] - target_df.shape[0],\n",
        "#             \"cols_diff\": result_df.shape[1] - target_df.shape[1]\n",
        "#         },\n",
        "#         \"column_differences\": {\n",
        "#             \"target_columns\": list(target_df.columns),\n",
        "#             \"result_columns\": list(result_df.columns),\n",
        "#             \"missing_columns\": list(set(target_df.columns) - set(result_df.columns)),\n",
        "#             \"extra_columns\": list(set(result_df.columns) - set(target_df.columns))\n",
        "#         },\n",
        "#         \"data_type_differences\": {},\n",
        "#         \"sample_data_differences\": [],\n",
        "#         \"statistical_differences\": {}\n",
        "#     }\n",
        "\n",
        "#     # Data type comparison\n",
        "#     if list(target_df.columns) == list(result_df.columns):\n",
        "#         for col in target_df.columns:\n",
        "#             if str(target_df[col].dtype) != str(result_df[col].dtype):\n",
        "#                 differences[\"data_type_differences\"][col] = {\n",
        "#                     \"target_dtype\": str(target_df[col].dtype),\n",
        "#                     \"result_dtype\": str(result_df[col].dtype)\n",
        "#                 }\n",
        "\n",
        "#     # Sample data differences (first few mismatches)\n",
        "#     if target_df.shape == result_df.shape and list(target_df.columns) == list(result_df.columns):\n",
        "#         mismatch_count = 0\n",
        "#         for i in range(min(len(target_df), 10)):  # Check first 10 rows\n",
        "#             for col in target_df.columns:\n",
        "#                 if mismatch_count >= 5:  # Limit to 5 sample differences\n",
        "#                     break\n",
        "#                 try:\n",
        "#                     target_val = target_df.iloc[i][col]\n",
        "#                     result_val = result_df.iloc[i][col]\n",
        "\n",
        "#                     if pd.isna(target_val) != pd.isna(result_val) or (not pd.isna(target_val) and str(target_val).strip() != str(result_val).strip()):\n",
        "#                         differences[\"sample_data_differences\"].append({\n",
        "#                             \"row\": i,\n",
        "#                             \"column\": col,\n",
        "#                             \"target_value\": str(target_val),\n",
        "#                             \"result_value\": str(result_val)\n",
        "#                         })\n",
        "#                         mismatch_count += 1\n",
        "#                 except:\n",
        "#                     continue\n",
        "\n",
        "#     # Basic statistical comparison for numeric columns\n",
        "#     try:\n",
        "#         for col in target_df.select_dtypes(include=[np.number]).columns:\n",
        "#             if col in result_df.columns:\n",
        "#                 differences[\"statistical_differences\"][col] = {\n",
        "#                     \"target_mean\": float(target_df[col].mean()) if not target_df[col].empty else None,\n",
        "#                     \"result_mean\": float(result_df[col].mean()) if not result_df[col].empty else None,\n",
        "#                     \"target_count\": int(target_df[col].count()),\n",
        "#                     \"result_count\": int(result_df[col].count())\n",
        "#                 }\n",
        "#     except:\n",
        "#         pass\n",
        "\n",
        "#     return differences\n",
        "\n",
        "\n",
        "# def _call_actual_claude_logged(prompt, original_df, log_to_file, attempt_number):\n",
        "#     \"\"\"\n",
        "#     Enhanced version with comprehensive logging of Claude API calls\n",
        "#     \"\"\"\n",
        "#     log_to_file(f\"\"\"CALLING CLAUDE API - ATTEMPT {attempt_number}:\n",
        "# Model: claude-sonnet-4-20250514\n",
        "# Temperature: 0.2\n",
        "# Max Tokens: 2000\n",
        "\n",
        "# PROMPT BEING SENT TO CLAUDE:\n",
        "# {'-'*60}\n",
        "# {prompt}\n",
        "# {'-'*60}\n",
        "\n",
        "# DATAFRAME CONTEXT BEING SENT:\n",
        "# Shape: {original_df.shape}\n",
        "# Columns: {list(original_df.columns)}\n",
        "# First 10 rows:\n",
        "# {original_df.head(10).to_string()}\n",
        "# \"\"\", f\"CLAUDE API CALL {attempt_number}\")\n",
        "\n",
        "#     try:\n",
        "#         # Get Anthropic client\n",
        "#         anthropic_client = Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "#         # Prepare dataframe context for Claude\n",
        "#         df_summary = f\"\"\"\n",
        "#         DATAFRAME CONTENT:\n",
        "#         {original_df.to_string(max_rows=50)}\n",
        "\n",
        "#         DATAFRAME STATISTICS:\n",
        "#         - Shape: {original_df.shape}\n",
        "#         - Columns: {list(original_df.columns)}\n",
        "#         - Data types: {dict(original_df.dtypes)}\n",
        "#         - Null values per column: {dict(original_df.isnull().sum())}\n",
        "#         \"\"\"\n",
        "\n",
        "#         # System prompt for Claude\n",
        "#         claude_system_prompt = \"\"\"You are an expert Python data analyst.\n",
        "#         You will receive a dataframe that is already loaded as 'df' and a specific task to perform.\n",
        "#         Generate Python pandas code to accomplish the task.\n",
        "\n",
        "#         IMPORTANT RULES:\n",
        "#         1. The dataframe 'df' is already loaded - do not load or import it\n",
        "#         2. Wrap all code in ```python and ``` blocks\n",
        "#         3. Do not use try-except blocks - let errors propagate naturally\n",
        "#         4. Be precise and specific in your transformations\n",
        "#         5. Provide working, executable code\n",
        "#         6. Focus on the exact transformation requested\"\"\"\n",
        "\n",
        "#         # Prepare messages for Claude\n",
        "#         claude_messages = [\n",
        "#             {\n",
        "#                 \"role\": \"user\",\n",
        "#                 \"content\": f\"Here is the dataframe that's already loaded as 'df':\\n\\n{df_summary}\\n\\nTASK: {prompt}\\n\\nGenerate Python code to accomplish this task.\"\n",
        "#             }\n",
        "#         ]\n",
        "\n",
        "#         # Call Claude\n",
        "#         claude_response = anthropic_client.messages.create(\n",
        "#             # model=\"claude-sonnet-4-20250514\",  # Use the latest Claude model\n",
        "#             model=\"claude-3-7-sonnet-20250219\",  # Use the latest Claude model\n",
        "#             system=claude_system_prompt,\n",
        "#             messages=claude_messages,\n",
        "#             max_tokens=2000,\n",
        "#             temperature=0.2\n",
        "#         )\n",
        "\n",
        "#         # Extract response text\n",
        "#         response_text = claude_response.content[0].text\n",
        "\n",
        "#         log_to_file(f\"\"\"CLAUDE API RESPONSE - ATTEMPT {attempt_number}:\n",
        "# Response Length: {len(response_text)} characters\n",
        "# Response Received Successfully: ✅\n",
        "\n",
        "# FULL CLAUDE RESPONSE:\n",
        "# {'-'*60}\n",
        "# {response_text}\n",
        "# {'-'*60}\n",
        "# \"\"\")\n",
        "\n",
        "#         return response_text\n",
        "\n",
        "#     except Exception as e:\n",
        "#         error_msg = f\"Claude API Error: {str(e)}\"\n",
        "#         log_to_file(f\"\"\"CLAUDE API CALL FAILED - ATTEMPT {attempt_number}:\n",
        "# Error: {error_msg}\n",
        "# Using fallback response\n",
        "# \"\"\")\n",
        "\n",
        "#         # Fallback response if Claude API fails\n",
        "#         fallback_response = f\"\"\"\n",
        "#         I'll help you with this task. Here's the code:\n",
        "\n",
        "#         ```python\n",
        "#         # Error calling Claude API: {str(e)}\n",
        "#         # Fallback code\n",
        "#         print(\"Claude API error, using fallback\")\n",
        "#         print(f\"Dataframe shape: {{df.shape}}\")\n",
        "#         print(df.head())\n",
        "#         ```\n",
        "#         \"\"\"\n",
        "\n",
        "#         log_to_file(f\"Fallback response generated:\\n{fallback_response}\")\n",
        "#         return fallback_response\n",
        "\n",
        "\n",
        "# def _get_final_analysis_with_prompts_logged(client, original_info, modified_info, user_description,\n",
        "#                                           total_changes, changed_rows, changed_columns, changed_cells,\n",
        "#                                           claude_prompt, replication_results, log_to_file):\n",
        "#     \"\"\"\n",
        "#     Enhanced version with comprehensive logging of final analysis generation\n",
        "#     \"\"\"\n",
        "#     best_attempt = replication_results.get(\"best_attempt\", {})\n",
        "#     final_claude_prompt = replication_results.get(\"final_prompt\", claude_prompt)\n",
        "#     success_status = \"SUCCESS\" if replication_results[\"final_success\"] else \"PARTIAL\"\n",
        "\n",
        "#     # Calculate statistics\n",
        "#     total_cells = original_info[\"shape\"][0] * original_info[\"shape\"][1] if original_info[\"shape\"][0] > 0 else 1\n",
        "#     change_density = total_changes / total_cells\n",
        "\n",
        "#     analysis_prompt = f\"\"\"\n",
        "#     Analyze this dataframe change and provide a comprehensive summary.\n",
        "\n",
        "#     ORIGINAL: {original_info['shape']} with data:\n",
        "#     {original_info['full_data'][:2000]}...\n",
        "\n",
        "#     MODIFIED: {modified_info['shape']} with data:\n",
        "#     {modified_info['full_data'][:2000]}...\n",
        "\n",
        "#     CHANGES: {total_changes} cells changed ({change_density*100:.1f}%)\n",
        "#     USER DESCRIPTION: {user_description}\n",
        "\n",
        "#     TESTED REPLICATION: {success_status}\n",
        "#     - Attempts: {len(replication_results['attempts'])}\n",
        "#     - Best match: {best_attempt.get('match_score', 0):.1%}\n",
        "#     - Enhancement method: {replication_results.get('enhancement_method', 'standard')}\n",
        "\n",
        "#     Provide analysis in this exact JSON format:\n",
        "#     {{\n",
        "#         \"change_summary\": \"Detailed technical summary including WORKING_CLAUDE_PROMPT: {final_claude_prompt} and GPT4_ANALYSIS_PROMPT: [the prompt used to generate the Claude prompt] - describe what changed and how replication performed with enhanced feedback loop\",\n",
        "#         \"change_type\": \"data_edit|structure_change|mixed\",\n",
        "#         \"structural_changes\": {{\n",
        "#             \"rows_added\": {max(0, modified_info['shape'][0] - original_info['shape'][0])},\n",
        "#             \"rows_removed\": {max(0, original_info['shape'][0] - modified_info['shape'][0])},\n",
        "#             \"columns_added\": [],\n",
        "#             \"columns_removed\": []\n",
        "#         }},\n",
        "#         \"data_modifications\": {{\n",
        "#             \"cells_changed\": {total_changes},\n",
        "#             \"total_cells\": {total_cells},\n",
        "#             \"change_percentage\": {change_density*100:.2f},\n",
        "#             \"rows_affected\": {len(changed_rows)},\n",
        "#             \"columns_affected\": {list(changed_columns)},\n",
        "#             \"common_patterns\": [\"Patterns identified\"],\n",
        "#             \"data_quality_impact\": \"improved|degraded|neutral\"\n",
        "#         }},\n",
        "#         \"business_impact\": {{\n",
        "#             \"rent_calculations_affected\": \"Analysis of rent impact\",\n",
        "#             \"tenant_information_updated\": \"Analysis of tenant data changes\",\n",
        "#             \"occupancy_status_changed\": \"Analysis of occupancy changes\"\n",
        "#         }},\n",
        "#         \"recommendations\": [\"Specific recommendations\"],\n",
        "#         \"session_description\": \"ENHANCEMENT: GPT-4.1_FEEDBACK_LOOP | REPLICATION_STATUS: {success_status} | FINAL_CLAUDE_PROMPT: {final_claude_prompt} | REPLICATION_ATTEMPTS: {len(replication_results['attempts'])} | USER_EDIT: {user_description} | Changes: {total_changes} cells ({change_density*100:.1f}%) | Best match: {best_attempt.get('match_score', 0):.1%}\"\n",
        "#     }}\n",
        "#     \"\"\"\n",
        "\n",
        "#     log_to_file(f\"\"\"FINAL ANALYSIS GENERATION:\n",
        "# Sending final analysis request to GPT-4.1...\n",
        "\n",
        "# PROMPT FOR FINAL ANALYSIS:\n",
        "# {'-'*60}\n",
        "# {analysis_prompt}\n",
        "# {'-'*60}\n",
        "# \"\"\", \"FINAL ANALYSIS GENERATION\")\n",
        "\n",
        "#     try:\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4.1\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": \"You are a data analyst. Return only valid JSON.\"},\n",
        "#                 {\"role\": \"user\", \"content\": analysis_prompt}\n",
        "#             ],\n",
        "#             max_tokens=3000,\n",
        "#             temperature=0.2\n",
        "#         )\n",
        "\n",
        "#         gpt_final_response = response.choices[0].message.content\n",
        "#         log_to_file(f\"\"\"FINAL ANALYSIS GPT-4.1 RESPONSE:\n",
        "# {'-'*60}\n",
        "# {gpt_final_response}\n",
        "# {'-'*60}\n",
        "# \"\"\")\n",
        "\n",
        "#         try:\n",
        "#             import json\n",
        "#             import re\n",
        "#             json_match = re.search(r'{.*}', gpt_final_response, re.DOTALL)\n",
        "#             if json_match:\n",
        "#                 parsed_analysis = json.loads(json_match.group(0))\n",
        "#                 log_to_file(\"✅ Successfully parsed JSON from final analysis response\")\n",
        "#                 return parsed_analysis\n",
        "#             else:\n",
        "#                 log_to_file(\"❌ No JSON found in final analysis response\")\n",
        "#         except Exception as json_error:\n",
        "#             log_to_file(f\"❌ JSON parsing failed: {str(json_error)}\")\n",
        "\n",
        "#     except Exception as api_error:\n",
        "#         log_to_file(f\"❌ Final analysis API call failed: {str(api_error)}\")\n",
        "\n",
        "#     # Fallback\n",
        "#     fallback_analysis = {\n",
        "#         \"change_summary\": f\"ENHANCEMENT: GPT-4.1_FEEDBACK_LOOP | WORKING_CLAUDE_PROMPT: {final_claude_prompt} | GPT4_ANALYSIS_PROMPT: [embedded] | {user_description} - {total_changes} changes with {replication_results['final_success']} replication\",\n",
        "#         \"change_type\": \"data_edit\",\n",
        "#         \"session_description\": f\"ENHANCEMENT: GPT-4.1_FEEDBACK_LOOP | REPLICATION_STATUS: {success_status} | FINAL_CLAUDE_PROMPT: {final_claude_prompt} | USER_EDIT: {user_description} | Changes: {total_changes} cells\",\n",
        "#         \"data_modifications\": {\n",
        "#             \"cells_changed\": total_changes,\n",
        "#             \"total_cells\": total_cells,\n",
        "#             \"change_percentage\": change_density*100\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "#     log_to_file(f\"Using fallback analysis:\\n{json.dumps(fallback_analysis, indent=2)}\")\n",
        "\n",
        "#     return fallback_analysis\n",
        "\n",
        "\n",
        "# # Additional helper functions for the enhanced logging system\n",
        "\n",
        "# def _extract_code_blocks(response_text):\n",
        "#     \"\"\"\n",
        "#     Extract Python code blocks from response text\n",
        "#     \"\"\"\n",
        "#     import re\n",
        "#     code_pattern = r'```(?:python)?\\s*(.*?)```'\n",
        "#     code_blocks = re.findall(code_pattern, response_text, re.DOTALL)\n",
        "#     return [block.strip() for block in code_blocks if block.strip()]\n",
        "\n",
        "\n",
        "# def _calculate_match_score(target_df, result_df):\n",
        "#     \"\"\"\n",
        "#     Calculate how closely the result matches the target with improved scoring.\n",
        "#     Now gives partial credit for near-matches instead of strict 0/1 scoring.\n",
        "#     \"\"\"\n",
        "#     import pandas as pd\n",
        "\n",
        "#     # Handle edge cases\n",
        "#     if target_df.empty and result_df.empty:\n",
        "#         return 1.0\n",
        "#     if target_df.empty or result_df.empty:\n",
        "#         return 0.0\n",
        "\n",
        "#     # 1. Shape similarity score (30% weight)\n",
        "#     target_rows, target_cols = target_df.shape\n",
        "#     result_rows, result_cols = result_df.shape\n",
        "\n",
        "#     # Row similarity (allows for minor filtering differences)\n",
        "#     if target_rows == 0:\n",
        "#         row_score = 1.0 if result_rows == 0 else 0.0\n",
        "#     else:\n",
        "#         row_diff = abs(target_rows - result_rows)\n",
        "#         row_score = max(0, 1.0 - (row_diff / target_rows))\n",
        "\n",
        "#     # Column similarity (should be exact for most use cases)\n",
        "#     col_score = 1.0 if target_cols == result_cols else 0.0\n",
        "\n",
        "#     shape_score = 0.7 * row_score + 0.3 * col_score\n",
        "\n",
        "#     # 2. Column structure score (20% weight)\n",
        "#     target_columns = list(target_df.columns)\n",
        "#     result_columns = list(result_df.columns)\n",
        "\n",
        "#     if target_columns == result_columns:\n",
        "#         column_score = 1.0\n",
        "#     else:\n",
        "#         # Partial credit for column overlap\n",
        "#         common_cols = set(target_columns) & set(result_columns)\n",
        "#         total_unique_cols = set(target_columns) | set(result_columns)\n",
        "#         column_score = len(common_cols) / len(total_unique_cols) if total_unique_cols else 0.0\n",
        "\n",
        "#     # 3. Content similarity score (50% weight)\n",
        "#     content_score = 0.0\n",
        "\n",
        "#     if target_columns == result_columns and len(target_columns) > 0:\n",
        "#         # Compare overlapping rows when columns match\n",
        "#         min_rows = min(len(target_df), len(result_df))\n",
        "\n",
        "#         if min_rows > 0:\n",
        "#             total_cells = 0\n",
        "#             matching_cells = 0\n",
        "\n",
        "#             # Compare cell by cell for overlapping area\n",
        "#             for i in range(min_rows):\n",
        "#                 for col in target_columns:\n",
        "#                     total_cells += 1\n",
        "\n",
        "#                     try:\n",
        "#                         target_val = target_df.iloc[i][col]\n",
        "#                         result_val = result_df.iloc[i][col]\n",
        "\n",
        "#                         # Handle NaN comparisons\n",
        "#                         if pd.isna(target_val) and pd.isna(result_val):\n",
        "#                             matching_cells += 1\n",
        "#                         elif pd.isna(target_val) or pd.isna(result_val):\n",
        "#                             # NaN vs non-NaN = no match\n",
        "#                             continue\n",
        "#                         else:\n",
        "#                             # String comparison with whitespace handling\n",
        "#                             target_str = str(target_val).strip()\n",
        "#                             result_str = str(result_val).strip()\n",
        "\n",
        "#                             if target_str == result_str:\n",
        "#                                 matching_cells += 1\n",
        "#                             else:\n",
        "#                                 # Try numeric comparison for potential formatting differences\n",
        "#                                 try:\n",
        "#                                     target_num = float(target_str.replace(',', '').replace(', ', ''))\n",
        "#                                     result_num = float(result_str.replace(',', '').replace(', ', ''))\n",
        "#                                     if abs(target_num - result_num) < 0.01:  # Allow small floating point differences\n",
        "#                                         matching_cells += 1\n",
        "#                                 except (ValueError, TypeError):\n",
        "#                                     # Not numeric, keep as non-match\n",
        "#                                     continue\n",
        "#                     except (IndexError, KeyError):\n",
        "#                         # Skip invalid cell references\n",
        "#                         continue\n",
        "\n",
        "#             content_score = matching_cells / total_cells if total_cells > 0 else 0.0\n",
        "\n",
        "#             # Bonus for exact row count match when content is high\n",
        "#             if len(target_df) == len(result_df) and content_score > 0.9:\n",
        "#                 content_score = min(1.0, content_score * 1.05)\n",
        "\n",
        "#     else:\n",
        "#         # Different column structures - can only do basic comparison\n",
        "#         if min(len(target_df), len(result_df)) > 0:\n",
        "#             # Give small credit for having some data with similar row count\n",
        "#             row_similarity = 1.0 - abs(len(target_df) - len(result_df)) / max(len(target_df), len(result_df))\n",
        "#             content_score = 0.2 * row_similarity  # Low score for structure mismatch\n",
        "\n",
        "#     # 4. Calculate weighted final score\n",
        "#     final_score = (0.30 * shape_score +\n",
        "#                   0.20 * column_score +\n",
        "#                   0.50 * content_score)\n",
        "\n",
        "#     # Ensure score is between 0 and 1\n",
        "#     return max(0.0, min(1.0, final_score))\n",
        "\n",
        "\n",
        "# def _improve_claude_prompt(current_prompt, target_df, result_df, match_score):\n",
        "#     \"\"\"\n",
        "#     Improve the Claude prompt based on the mismatch (FALLBACK - now replaced by GPT-4.1 enhancement)\n",
        "#     \"\"\"\n",
        "#     feedback = f\"\\nPREVIOUS ATTEMPT FEEDBACK:\\n\"\n",
        "#     feedback += f\"Match score: {match_score:.2%}\\n\"\n",
        "#     feedback += f\"Target shape: {target_df.shape}, Result shape: {result_df.shape}\\n\"\n",
        "\n",
        "#     if target_df.shape != result_df.shape:\n",
        "#         feedback += \"Shape mismatch detected. Please check row filtering logic.\\n\"\n",
        "\n",
        "#     return current_prompt + feedback\n",
        "\n",
        "\n",
        "# # Enhanced save function that also uses the detailed logging\n",
        "# def save_edited_dataframe_enhanced_with_logging(edited_df, description):\n",
        "#     \"\"\"\n",
        "#     Enhanced version that uses the new comprehensive logging system\n",
        "#     for analyzing manual dataframe changes with GPT-4.1 feedback loop.\n",
        "#     \"\"\"\n",
        "#     global app_state, session_recorder\n",
        "\n",
        "#     if edited_df is None or edited_df.empty:\n",
        "#         return \"No data to save\", gr.update()\n",
        "\n",
        "#     try:\n",
        "#         # Convert the edited dataframe to proper pandas DataFrame if needed\n",
        "#         if not isinstance(edited_df, pd.DataFrame):\n",
        "#             edited_df = pd.DataFrame(edited_df)\n",
        "\n",
        "#         # Get the original dataframe for comparison\n",
        "#         original_df = app_state[\"df\"].copy()\n",
        "\n",
        "#         logger.info(\"Analyzing dataframe changes with enhanced GPT-4.1 feedback loop...\")\n",
        "#         print(\"🧠 Analyzing changes with enhanced GPT-4.1 intelligent feedback loop system...\")\n",
        "\n",
        "#         # Use the enhanced analysis function with comprehensive logging and feedback loop\n",
        "#         change_analysis = analyze_dataframe_changes_with_gpt4(\n",
        "#             original_df=original_df,\n",
        "#             modified_df=edited_df,\n",
        "#             user_description=description\n",
        "#         )\n",
        "\n",
        "#         # Generate a meaningful description if not provided\n",
        "#         if not description:\n",
        "#             description = change_analysis.get(\"change_summary\", \"Manual edits via data editor\")\n",
        "\n",
        "#         # Save as new version\n",
        "#         version_name = save_dataframe_version(edited_df, description)\n",
        "\n",
        "#         # Update the app state with the edited dataframe\n",
        "#         app_state[\"df\"] = edited_df\n",
        "\n",
        "#         # Record this in the copiloting session if active\n",
        "#         if session_recorder.current_session_file:\n",
        "#             session_description = change_analysis.get(\"session_description\", f\"Enhanced manual data edits: {description}\")\n",
        "\n",
        "#             # Create detailed session entry with log file reference\n",
        "#             session_entry = f\"\"\"\n",
        "# MANUAL DATA EDIT SESSION WITH ENHANCED GPT-4.1 FEEDBACK LOOP\n",
        "# ============================================================\n",
        "# Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "# Edit Description: {description}\n",
        "# Version Saved: {version_name}\n",
        "# Enhancement Method: GPT-4.1 Intelligent Feedback Loop\n",
        "# Log File: {change_analysis.get('log_file_path', 'N/A')}\n",
        "\n",
        "# ENHANCED GPT-4.1 CHANGE ANALYSIS:\n",
        "# {'-' * 40}\n",
        "# Change Summary: {change_analysis.get('change_summary', 'N/A')}\n",
        "# Change Type: {change_analysis.get('change_type', 'N/A')}\n",
        "# Enhancement Method: {change_analysis.get('enhancement_method', 'gpt4_feedback_loop')}\n",
        "# Replication Success: {change_analysis.get('full_change_statistics', {}).get('replication_success', 'Unknown')}\n",
        "# Replication Attempts: {change_analysis.get('full_change_statistics', {}).get('replication_attempts', 'Unknown')}\n",
        "# Progressive Improvement: {change_analysis.get('full_change_statistics', {}).get('progressive_improvement', 'Unknown')}\n",
        "\n",
        "# Structural Changes:\n",
        "# {json.dumps(change_analysis.get('structural_changes', {}), indent=2)}\n",
        "\n",
        "# Data Modifications:\n",
        "# {json.dumps(change_analysis.get('data_modifications', {}), indent=2)}\n",
        "\n",
        "# Business Impact:\n",
        "# {json.dumps(change_analysis.get('business_impact', {}), indent=2)}\n",
        "\n",
        "# Recommendations:\n",
        "# {chr(10).join([f\"• {rec}\" for rec in change_analysis.get('recommendations', [])])}\n",
        "\n",
        "# Enhanced Technical Statistics:\n",
        "# - Total Cells: {change_analysis.get('full_change_statistics', {}).get('total_cells', 'Unknown')}\n",
        "# - Changed Cells: {change_analysis.get('full_change_statistics', {}).get('total_changes', 'Unknown')}\n",
        "# - Change Density: {change_analysis.get('full_change_statistics', {}).get('change_density', 0)*100:.2f}%\n",
        "# - Affected Rows: {len(change_analysis.get('full_change_statistics', {}).get('affected_rows', []))}\n",
        "# - Affected Columns: {len(change_analysis.get('full_change_statistics', {}).get('affected_columns', []))}\n",
        "# - GPT-4.1 Enhanced Attempts: {len([a for a in change_analysis.get('full_change_statistics', {}).get('replication_attempts', []) if isinstance(a, dict) and a.get('enhancement_type') == 'gpt4_enhanced'])}\n",
        "\n",
        "# Original DataFrame Shape: {original_df.shape}\n",
        "# Modified DataFrame Shape: {edited_df.shape}\n",
        "# {'-' * 80}\n",
        "# \"\"\"\n",
        "\n",
        "#             # Append to session file\n",
        "#             with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "#                 f.write(session_entry + \"\\n\")\n",
        "\n",
        "#             # Record in session data structure\n",
        "#             session_recorder.record_conversation_turn(\n",
        "#                 user_message=f\"MANUAL EDIT (Enhanced GPT-4.1 Feedback): {description}\",\n",
        "#                 ai_response=session_description,\n",
        "#                 action_type=\"manual_data_edit_enhanced_gpt4\",\n",
        "#                 code_executed=None,\n",
        "#                 version_saved=version_name\n",
        "#             )\n",
        "\n",
        "#             # Record the dataframe version change\n",
        "#             session_recorder.record_dataframe_version(\n",
        "#                 version_name=version_name,\n",
        "#                 description=description,\n",
        "#                 shape=list(edited_df.shape),\n",
        "#                 columns=list(edited_df.columns)\n",
        "#             )\n",
        "\n",
        "#             # Record any issues found by GPT-4\n",
        "#             replication_success = change_analysis.get('full_change_statistics', {}).get('replication_success', False)\n",
        "#             if not replication_success:\n",
        "#                 session_recorder.record_issue_found(\n",
        "#                     f\"Enhanced manual edit replication failed: {description}. Check detailed log for GPT-4.1 feedback analysis.\",\n",
        "#                     severity=\"medium\"\n",
        "#                 )\n",
        "\n",
        "#             logger.info(\"Enhanced manual edit analysis with GPT-4.1 feedback loop recorded in copiloting session\")\n",
        "\n",
        "#         # Log the changes\n",
        "#         logger.info(f\"Saved edited dataframe as version {version_name}\")\n",
        "\n",
        "#         # Create detailed success message with log file information\n",
        "#         log_file_path = change_analysis.get('log_file_path', 'N/A')\n",
        "#         replication_success = change_analysis.get('full_change_statistics', {}).get('replication_success', False)\n",
        "#         replication_attempts = change_analysis.get('full_change_statistics', {}).get('replication_attempts', 0)\n",
        "#         progressive_improvement = change_analysis.get('full_change_statistics', {}).get('progressive_improvement', False)\n",
        "\n",
        "#         success_message = f\"\"\"✅ Successfully saved as version {version_name}\n",
        "\n",
        "# 🧠 Enhanced GPT-4.1 Feedback Loop Analysis Summary:\n",
        "# {change_analysis.get('change_summary', 'Changes analyzed')[:200]}...\n",
        "\n",
        "# 📊 Enhanced Change Details:\n",
        "# • Change Type: {change_analysis.get('change_type', 'Unknown')}\n",
        "# • Enhancement Method: {change_analysis.get('enhancement_method', 'gpt4_feedback_loop')}\n",
        "# • Original Shape: {original_df.shape}\n",
        "# • New Shape: {edited_df.shape}\n",
        "# • Replication Success: {'✅ Yes' if replication_success else '❌ Partial/Failed'}\n",
        "# • Replication Attempts: {replication_attempts}\n",
        "# • Progressive Improvement: {'✅ Yes' if progressive_improvement else '❌ No'}\n",
        "\n",
        "# 📝 Session Recording: {'✅ Recorded' if session_recorder.current_session_file else '❌ No active session'}\n",
        "\n",
        "# 📁 Enhanced Analysis Log:\n",
        "# {log_file_path}\n",
        "\n",
        "# This enhanced log contains:\n",
        "# • Complete GPT-4.1 prompts and responses\n",
        "# • All Claude API calls and generated code\n",
        "# • GPT-4.1 gap analysis for failed attempts\n",
        "# • Intelligent prompt improvements between attempts\n",
        "# • Progressive learning from failures\n",
        "# • Step-by-step replication attempts\n",
        "# • Cell-by-cell change analysis\n",
        "# • Business impact assessment\n",
        "# \"\"\"\n",
        "\n",
        "#         # Add recommendations if available\n",
        "#         if change_analysis.get('recommendations'):\n",
        "#             success_message += f\"\\n💡 Enhanced Recommendations:\\n\"\n",
        "#             for rec in change_analysis['recommendations'][:3]:  # Show first 3\n",
        "#                 success_message += f\"• {rec}\\n\"\n",
        "\n",
        "#         # Add note about enhanced capabilities\n",
        "#         success_message += f\"\\n🚀 Enhancement Features Used:\"\n",
        "#         success_message += f\"\\n• GPT-4.1 Gap Analysis: {'✅' if progressive_improvement else '❌'}\"\n",
        "#         success_message += f\"\\n• Intelligent Prompt Improvement: {'✅' if progressive_improvement else '❌'}\"\n",
        "#         success_message += f\"\\n• Progressive Learning: {'✅' if progressive_improvement else '❌'}\"\n",
        "\n",
        "#         # Add note about log file location\n",
        "#         success_message += f\"\\n\\n📄 For complete debugging and enhancement information, check: {log_file_path}\"\n",
        "\n",
        "#         return success_message, gr.update(value=edited_df)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         error_msg = f\"❌ Error saving with enhanced analysis: {str(e)}\"\n",
        "#         logger.error(f\"Error saving edited dataframe with enhanced analysis: {e}\")\n",
        "#         logger.error(traceback.format_exc())\n",
        "\n",
        "#         # Still try to record the error in session\n",
        "#         if session_recorder.current_session_file:\n",
        "#             session_recorder.record_conversation_turn(\n",
        "#                 user_message=f\"MANUAL EDIT FAILED (Enhanced GPT-4.1): {description}\",\n",
        "#                 ai_response=error_msg,\n",
        "#                 action_type=\"manual_edit_error_enhanced_gpt4\",\n",
        "#                 code_executed=None,\n",
        "#                 version_saved=None\n",
        "#             )\n",
        "\n",
        "#         return error_msg, gr.update()\n",
        "\n",
        "\n",
        "# # Function to view and summarize all log files\n",
        "# def get_manual_edit_logs_summary():\n",
        "#     \"\"\"\n",
        "#     Generate a summary of all manual edit analysis log files including enhanced versions\n",
        "#     \"\"\"\n",
        "#     logs_dir = \"manual_edit_analysis_logs\"\n",
        "\n",
        "#     if not os.path.exists(logs_dir):\n",
        "#         return \"No manual edit analysis logs found yet.\"\n",
        "\n",
        "#     try:\n",
        "#         log_files = [f for f in os.listdir(logs_dir) if f.endswith('_detailed_analysis.txt')]\n",
        "\n",
        "#         if not log_files:\n",
        "#             return \"No detailed analysis log files found.\"\n",
        "\n",
        "#         # Sort by creation time (newest first)\n",
        "#         log_files.sort(key=lambda x: os.path.getctime(os.path.join(logs_dir, x)), reverse=True)\n",
        "\n",
        "#         # Count enhanced vs standard logs\n",
        "#         enhanced_logs = [f for f in log_files if f.startswith('enhanced_edit_')]\n",
        "#         standard_logs = [f for f in log_files if f.startswith('manual_edit_')]\n",
        "\n",
        "#         summary = f\"\"\"📁 Enhanced Manual Edit Analysis Logs Summary\n",
        "# Found {len(log_files)} detailed analysis log files:\n",
        "# • Enhanced (GPT-4.1 Feedback Loop): {len(enhanced_logs)}\n",
        "# • Standard: {len(standard_logs)}\n",
        "\n",
        "# Recent Logs:\n",
        "# \"\"\"\n",
        "\n",
        "#         for i, log_file in enumerate(log_files[:10], 1):  # Show last 10\n",
        "#             file_path = os.path.join(logs_dir, log_file)\n",
        "#             file_size = os.path.getsize(file_path)\n",
        "#             created_time = datetime.fromtimestamp(os.path.getctime(file_path))\n",
        "\n",
        "#             # Try to extract session info from filename\n",
        "#             session_id = log_file.replace('_detailed_analysis.txt', '')\n",
        "#             log_type = \"🧠 Enhanced\" if log_file.startswith('enhanced_edit_') else \"📝 Standard\"\n",
        "\n",
        "#             summary += f\"\"\"{i}. {log_type} - {session_id}\n",
        "#    📄 File: {log_file}\n",
        "#    📅 Created: {created_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "#    💾 Size: {file_size:,} bytes\n",
        "#    📁 Path: {file_path}\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "#         if len(log_files) > 10:\n",
        "#             summary += f\"... and {len(log_files) - 10} more log files\\n\"\n",
        "\n",
        "#         summary += f\"\"\"\n",
        "# 📋 Enhanced Log File Contents Include:\n",
        "# • Complete GPT-4.1 prompts and responses\n",
        "# • All Claude API calls and code generation\n",
        "# • GPT-4.1 gap analysis for replication failures\n",
        "# • Intelligent prompt improvement between attempts\n",
        "# • Progressive learning and enhancement tracking\n",
        "# • Step-by-step replication testing results\n",
        "# • Cell-by-cell dataframe comparison analysis\n",
        "# • Business impact and recommendation analysis\n",
        "# • Detailed error messages and debugging information\n",
        "\n",
        "# 🚀 Enhancement Features:\n",
        "# • Gap Analysis: GPT-4.1 identifies specific failure reasons\n",
        "# • Prompt Improvement: Surgical fixes based on analysis\n",
        "# • Progressive Learning: Each attempt gets smarter\n",
        "# • Success Tracking: Monitors improvement across attempts\n",
        "\n",
        "# 📂 All logs are saved in: {logs_dir}/\n",
        "# \"\"\"\n",
        "\n",
        "#         return summary\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"Error reading log files: {str(e)}\"\n",
        "\n",
        "\n",
        "# # Function to read a specific log file\n",
        "# def read_manual_edit_log(session_id):\n",
        "#     \"\"\"\n",
        "#     Read and return the contents of a specific manual edit analysis log\n",
        "#     \"\"\"\n",
        "#     logs_dir = \"manual_edit_analysis_logs\"\n",
        "#     log_file_path = os.path.join(logs_dir, f\"{session_id}_detailed_analysis.txt\")\n",
        "\n",
        "#     if not os.path.exists(log_file_path):\n",
        "#         return f\"Log file not found: {log_file_path}\"\n",
        "\n",
        "#     try:\n",
        "#         with open(log_file_path, 'r', encoding='utf-8') as f:\n",
        "#             content = f.read()\n",
        "\n",
        "#         # Determine if this is an enhanced log\n",
        "#         is_enhanced = \"GPT-4.1 FEEDBACK LOOP\" in content or \"ENHANCED\" in content\n",
        "\n",
        "#         log_type = \"🧠 Enhanced GPT-4.1 Feedback Loop\" if is_enhanced else \"📝 Standard\"\n",
        "\n",
        "#         return f\"\"\"📄 Manual Edit Analysis Log ({log_type}): {session_id}\n",
        "# {'='*80}\n",
        "\n",
        "# {content}\n",
        "\n",
        "# {'='*80}\n",
        "# End of log file: {log_file_path}\n",
        "\n",
        "# Log Type: {log_type}\n",
        "# Enhancement Features: {'Gap Analysis, Prompt Improvement, Progressive Learning' if is_enhanced else 'Basic replication testing'}\n",
        "# \"\"\"\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"Error reading log file: {str(e)}\"\n",
        "\n",
        "\n",
        "# # Function to analyze enhancement performance across logs\n",
        "# def analyze_enhancement_performance():\n",
        "#     \"\"\"\n",
        "#     Analyze the performance difference between standard and enhanced logs\n",
        "#     \"\"\"\n",
        "#     logs_dir = \"manual_edit_analysis_logs\"\n",
        "\n",
        "#     if not os.path.exists(logs_dir):\n",
        "#         return \"No manual edit analysis logs found yet.\"\n",
        "\n",
        "#     try:\n",
        "#         log_files = [f for f in os.listdir(logs_dir) if f.endswith('_detailed_analysis.txt')]\n",
        "\n",
        "#         if not log_files:\n",
        "#             return \"No detailed analysis log files found.\"\n",
        "\n",
        "#         enhanced_stats = []\n",
        "#         standard_stats = []\n",
        "\n",
        "#         for log_file in log_files:\n",
        "#             try:\n",
        "#                 file_path = os.path.join(logs_dir, log_file)\n",
        "#                 with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#                     content = f.read()\n",
        "\n",
        "#                 is_enhanced = \"GPT-4.1 FEEDBACK LOOP\" in content or \"ENHANCED\" in content\n",
        "\n",
        "#                 # Extract success rate\n",
        "#                 if \"REPLICATION SUCCESSFUL\" in content:\n",
        "#                     success = True\n",
        "#                 elif \"REPLICATION TESTING COMPLETE\" in content:\n",
        "#                     success = False\n",
        "#                 else:\n",
        "#                     continue\n",
        "\n",
        "#                 # Extract match score\n",
        "#                 import re\n",
        "#                 match_scores = re.findall(r'Best Match Score: ([\\d.]+)%', content)\n",
        "#                 if match_scores:\n",
        "#                     best_score = float(match_scores[-1])\n",
        "#                 else:\n",
        "#                     best_score = 0.0\n",
        "\n",
        "#                 # Extract attempt count\n",
        "#                 attempt_matches = re.findall(r'Total Attempts: (\\d+)', content)\n",
        "#                 if attempt_matches:\n",
        "#                     attempts = int(attempt_matches[-1])\n",
        "#                 else:\n",
        "#                     attempts = 3\n",
        "\n",
        "#                 stat = {\n",
        "#                     'success': success,\n",
        "#                     'best_score': best_score,\n",
        "#                     'attempts': attempts,\n",
        "#                     'file': log_file\n",
        "#                 }\n",
        "\n",
        "#                 if is_enhanced:\n",
        "#                     enhanced_stats.append(stat)\n",
        "#                 else:\n",
        "#                     standard_stats.append(stat)\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 continue\n",
        "\n",
        "#         # Calculate performance metrics\n",
        "#         def calc_metrics(stats):\n",
        "#             if not stats:\n",
        "#                 return {'success_rate': 0, 'avg_score': 0, 'avg_attempts': 0, 'count': 0}\n",
        "\n",
        "#             success_rate = sum(1 for s in stats if s['success']) / len(stats)\n",
        "#             avg_score = sum(s['best_score'] for s in stats) / len(stats)\n",
        "#             avg_attempts = sum(s['attempts'] for s in stats) / len(stats)\n",
        "\n",
        "#             return {\n",
        "#                 'success_rate': success_rate,\n",
        "#                 'avg_score': avg_score,\n",
        "#                 'avg_attempts': avg_attempts,\n",
        "#                 'count': len(stats)\n",
        "#             }\n",
        "\n",
        "#         enhanced_metrics = calc_metrics(enhanced_stats)\n",
        "#         standard_metrics = calc_metrics(standard_stats)\n",
        "\n",
        "#         summary = f\"\"\"🧠 Enhancement Performance Analysis\n",
        "# {'='*50}\n",
        "\n",
        "# 📊 Enhanced GPT-4.1 Feedback Loop:\n",
        "# • Total Sessions: {enhanced_metrics['count']}\n",
        "# • Success Rate: {enhanced_metrics['success_rate']:.1%}\n",
        "# • Average Best Score: {enhanced_metrics['avg_score']:.1f}%\n",
        "# • Average Attempts: {enhanced_metrics['avg_attempts']:.1f}\n",
        "\n",
        "# 📝 Standard Approach:\n",
        "# • Total Sessions: {standard_metrics['count']}\n",
        "# • Success Rate: {standard_metrics['success_rate']:.1%}\n",
        "# • Average Best Score: {standard_metrics['avg_score']:.1f}%\n",
        "# • Average Attempts: {standard_metrics['avg_attempts']:.1f}\n",
        "\n",
        "# 🚀 Improvement Metrics:\n",
        "# \"\"\"\n",
        "\n",
        "#         if standard_metrics['count'] > 0 and enhanced_metrics['count'] > 0:\n",
        "#             success_improvement = enhanced_metrics['success_rate'] - standard_metrics['success_rate']\n",
        "#             score_improvement = enhanced_metrics['avg_score'] - standard_metrics['avg_score']\n",
        "\n",
        "#             summary += f\"\"\"• Success Rate Improvement: {success_improvement:+.1%}\n",
        "# • Average Score Improvement: {score_improvement:+.1f}%\n",
        "# • Enhanced Success Rate: {enhanced_metrics['success_rate']:.1%} vs Standard: {standard_metrics['success_rate']:.1%}\n",
        "\n",
        "# 💡 Analysis:\n",
        "# The enhanced GPT-4.1 feedback loop shows {'significant improvement' if success_improvement > 0.1 else 'some improvement' if success_improvement > 0 else 'similar performance'}\n",
        "# over the standard approach with {'higher' if score_improvement > 5 else 'similar'} match scores and\n",
        "# {'better' if success_improvement > 0.05 else 'equivalent'} success rates.\n",
        "# \"\"\"\n",
        "#         else:\n",
        "#             summary += \"• Insufficient data for comparison\\n\"\n",
        "\n",
        "#         return summary\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"Error analyzing enhancement performance: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBw7t_p3RhSm"
      },
      "outputs": [],
      "source": [
        "# def analyze_dataframe_changes_with_gpt4(original_df, modified_df, user_description=\"\"):\n",
        "#     \"\"\"\n",
        "#     Use GPT-4.1 to analyze differences between original and modified dataframes,\n",
        "#     generate a detailed description of changes made, AND test Claude prompts.\n",
        "#     NOW USES COMPLETE DATAFRAMES FOR ANALYSIS AND TESTS REPLICATION.\n",
        "#     AUTOMATICALLY SAVES ALL LOGS TO TEXT FILES WHENEVER EXECUTED.\n",
        "#     \"\"\"\n",
        "#     import os\n",
        "#     import json\n",
        "#     import traceback\n",
        "#     from datetime import datetime\n",
        "\n",
        "#     # Create detailed logging directory\n",
        "#     logs_dir = \"manual_edit_analysis_logs\"\n",
        "#     os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "#     # Generate unique log session ID\n",
        "#     log_session_id = f\"manual_edit_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]}\"  # Include milliseconds for uniqueness\n",
        "#     log_file_path = os.path.join(logs_dir, f\"{log_session_id}_detailed_analysis.txt\")\n",
        "\n",
        "#     # Enhanced logging function that ALWAYS saves to file\n",
        "#     def log_to_file(content, section_title=\"\"):\n",
        "#         try:\n",
        "#             with open(log_file_path, 'a', encoding='utf-8') as f:\n",
        "#                 if section_title:\n",
        "#                     f.write(f\"\\n{'='*80}\\n\")\n",
        "#                     f.write(f\"{section_title}\\n\")\n",
        "#                     f.write(f\"{'='*80}\\n\")\n",
        "#                 f.write(f\"{content}\\n\")\n",
        "#                 f.flush()  # Ensure immediate write to disk\n",
        "#         except Exception as e:\n",
        "#             print(f\"ERROR writing to log file: {e}\")\n",
        "\n",
        "#     # ALWAYS initialize comprehensive log - even if function fails later\n",
        "#     try:\n",
        "#         log_to_file(f\"\"\"COMPREHENSIVE MANUAL EDIT ANALYSIS LOG\n",
        "# Session ID: {log_session_id}\n",
        "# Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "# User Description: \"{user_description}\"\n",
        "# Log File Path: {log_file_path}\n",
        "\n",
        "# ANALYSIS OVERVIEW:\n",
        "# This log contains the complete workflow of analyzing manual dataframe edits:\n",
        "# 1. Original vs Modified dataframe comparison\n",
        "# 2. GPT-4.1 prompt generation for Claude\n",
        "# 3. Claude API calls and responses\n",
        "# 4. Code execution attempts and results\n",
        "# 5. Replication success/failure analysis\n",
        "\n",
        "# FUNCTION EXECUTION STATUS: STARTING...\n",
        "# \"\"\", \"SESSION INITIALIZATION\")\n",
        "\n",
        "#         print(f\"📝 Manual edit analysis log initialized: {log_file_path}\")\n",
        "\n",
        "#     except Exception as init_error:\n",
        "#         print(f\"CRITICAL: Could not initialize log file: {init_error}\")\n",
        "#         # Continue execution even if logging fails\n",
        "\n",
        "#     try:\n",
        "#         # ALWAYS log dataframe information - even if analysis fails later\n",
        "#         log_to_file(f\"\"\"ORIGINAL DATAFRAME COMPLETE ANALYSIS:\n",
        "# Shape: {original_df.shape}\n",
        "# Columns: {list(original_df.columns)}\n",
        "# Data Types: {dict(original_df.dtypes.astype(str))}\n",
        "# Memory Usage: {original_df.memory_usage(deep=True).sum()} bytes\n",
        "# Null Counts: {dict(original_df.isnull().sum())}\n",
        "\n",
        "# FIRST 10 ROWS PREVIEW:\n",
        "# {original_df.head(10).to_string()}\n",
        "\n",
        "# COMPLETE ORIGINAL DATAFRAME (ALL ROWS):\n",
        "# {original_df.to_string(max_rows=None, max_cols=None)}\n",
        "\n",
        "# ORIGINAL DATAFRAME AS CSV:\n",
        "# {original_df.to_csv(index=False)}\n",
        "# \"\"\", \"ORIGINAL DATAFRAME ANALYSIS\")\n",
        "\n",
        "#         log_to_file(f\"\"\"MODIFIED DATAFRAME COMPLETE ANALYSIS:\n",
        "# Shape: {modified_df.shape}\n",
        "# Columns: {list(modified_df.columns)}\n",
        "# Data Types: {dict(modified_df.dtypes.astype(str))}\n",
        "# Memory Usage: {modified_df.memory_usage(deep=True).sum()} bytes\n",
        "# Null Counts: {dict(modified_df.isnull().sum())}\n",
        "\n",
        "# FIRST 10 ROWS PREVIEW:\n",
        "# {modified_df.head(10).to_string()}\n",
        "\n",
        "# COMPLETE MODIFIED DATAFRAME (ALL ROWS):\n",
        "# {modified_df.to_string(max_rows=None, max_cols=None)}\n",
        "\n",
        "# MODIFIED DATAFRAME AS CSV:\n",
        "# {modified_df.to_csv(index=False)}\n",
        "# \"\"\", \"MODIFIED DATAFRAME ANALYSIS\")\n",
        "\n",
        "#         # Initialize OpenAI client\n",
        "#         try:\n",
        "#             client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "#             log_to_file(\"✅ OpenAI client initialized successfully\", \"API CLIENT SETUP\")\n",
        "#         except Exception as client_error:\n",
        "#             log_to_file(f\"❌ Failed to initialize OpenAI client: {client_error}\", \"API CLIENT SETUP ERROR\")\n",
        "#             raise client_error\n",
        "\n",
        "#         # Prepare comparison data for GPT-4 - NOW WITH COMPLETE DATAFRAMES\n",
        "#         original_info = {\n",
        "#             \"shape\": original_df.shape,\n",
        "#             \"columns\": list(original_df.columns),\n",
        "#             \"dtypes\": dict(original_df.dtypes.astype(str)),\n",
        "#             \"full_data\": original_df.to_string(max_rows=None, max_cols=None),  # COMPLETE dataframe\n",
        "#             \"full_csv\": original_df.to_csv(index=False),  # Alternative format\n",
        "#             \"null_counts\": dict(original_df.isnull().sum()),\n",
        "#             \"memory_usage\": original_df.memory_usage(deep=True).sum(),\n",
        "#             \"summary_stats\": original_df.describe(include='all').to_string() if len(original_df) > 0 else \"No data\"\n",
        "#         }\n",
        "\n",
        "#         modified_info = {\n",
        "#             \"shape\": modified_df.shape,\n",
        "#             \"columns\": list(modified_df.columns),\n",
        "#             \"dtypes\": dict(modified_df.dtypes.astype(str)),\n",
        "#             \"full_data\": modified_df.to_string(max_rows=None, max_cols=None),  # COMPLETE dataframe\n",
        "#             \"full_csv\": modified_df.to_csv(index=False),  # Alternative format\n",
        "#             \"null_counts\": dict(modified_df.isnull().sum()),\n",
        "#             \"memory_usage\": modified_df.memory_usage(deep=True).sum(),\n",
        "#             \"summary_stats\": modified_df.describe(include='all').to_string() if len(modified_df) > 0 else \"No data\"\n",
        "#         }\n",
        "\n",
        "#         # Detect specific changes\n",
        "#         shape_changed = original_df.shape != modified_df.shape\n",
        "#         columns_changed = set(original_df.columns) != set(modified_df.columns)\n",
        "\n",
        "#         log_to_file(f\"\"\"STRUCTURAL CHANGES DETECTED:\n",
        "# Shape Changed: {shape_changed}\n",
        "# - Original Shape: {original_df.shape}\n",
        "# - Modified Shape: {modified_df.shape}\n",
        "# - Rows Added: {max(0, modified_df.shape[0] - original_df.shape[0])}\n",
        "# - Rows Removed: {max(0, original_df.shape[0] - modified_df.shape[0])}\n",
        "# - Columns Added: {max(0, modified_df.shape[1] - original_df.shape[1])}\n",
        "# - Columns Removed: {max(0, original_df.shape[1] - modified_df.shape[1])}\n",
        "\n",
        "# Columns Changed: {columns_changed}\n",
        "# - Original Columns: {list(original_df.columns)}\n",
        "# - Modified Columns: {list(modified_df.columns)}\n",
        "# - Added Columns: {list(set(modified_df.columns) - set(original_df.columns))}\n",
        "# - Removed Columns: {list(set(original_df.columns) - set(modified_df.columns))}\n",
        "# \"\"\", \"STRUCTURAL CHANGE ANALYSIS\")\n",
        "\n",
        "#         # COMPLETE cell-by-cell comparison - ANALYZE EVERY SINGLE CELL\n",
        "#         data_changes_detected = False\n",
        "#         changed_cells = []\n",
        "#         total_changes = 0\n",
        "#         changed_rows = set()\n",
        "#         changed_columns = set()\n",
        "\n",
        "#         if original_df.shape == modified_df.shape and list(original_df.columns) == list(modified_df.columns):\n",
        "#             print(f\"🔍 Comparing ALL {len(original_df)} rows and {len(original_df.columns)} columns...\")\n",
        "#             log_to_file(f\"\"\"STARTING COMPLETE CELL-BY-CELL COMPARISON:\n",
        "# Total cells to compare: {original_df.shape[0] * original_df.shape[1]}\n",
        "# Comparing {len(original_df)} rows × {len(original_df.columns)} columns\n",
        "# This may take time for large dataframes...\n",
        "# \"\"\", \"CELL-BY-CELL COMPARISON START\")\n",
        "\n",
        "#             # Compare EVERY cell in the entire dataframe\n",
        "#             for i in range(len(original_df)):\n",
        "#                 row_has_changes = False\n",
        "#                 row_changes = []\n",
        "\n",
        "#                 for col in original_df.columns:\n",
        "#                     try:\n",
        "#                         orig_val = original_df.iloc[i][col]\n",
        "#                         mod_val = modified_df.iloc[i][col]\n",
        "\n",
        "#                         # Handle NaN comparisons\n",
        "#                         if pd.isna(orig_val) and pd.isna(mod_val):\n",
        "#                             continue\n",
        "#                         elif pd.isna(orig_val) or pd.isna(mod_val):\n",
        "#                             data_changes_detected = True\n",
        "#                             total_changes += 1\n",
        "#                             row_has_changes = True\n",
        "#                             changed_columns.add(col)\n",
        "#                             change_detail = {\n",
        "#                                 \"row\": i,\n",
        "#                                 \"column\": col,\n",
        "#                                 \"original\": str(orig_val),\n",
        "#                                 \"modified\": str(mod_val),\n",
        "#                                 \"change_type\": \"nan_change\"\n",
        "#                             }\n",
        "#                             row_changes.append(change_detail)\n",
        "#                             # Store ALL changes, not just first 50\n",
        "#                             changed_cells.append(change_detail)\n",
        "#                         elif str(orig_val).strip() != str(mod_val).strip():\n",
        "#                             data_changes_detected = True\n",
        "#                             total_changes += 1\n",
        "#                             row_has_changes = True\n",
        "#                             changed_columns.add(col)\n",
        "#                             change_detail = {\n",
        "#                                 \"row\": i,\n",
        "#                                 \"column\": col,\n",
        "#                                 \"original\": str(orig_val),\n",
        "#                                 \"modified\": str(mod_val),\n",
        "#                                 \"change_type\": \"value_change\"\n",
        "#                             }\n",
        "#                             row_changes.append(change_detail)\n",
        "#                             # Store ALL changes, not just first 50\n",
        "#                             changed_cells.append(change_detail)\n",
        "#                     except Exception as e:\n",
        "#                         log_to_file(f\"ERROR comparing cell at row {i}, column '{col}': {str(e)}\")\n",
        "#                         continue\n",
        "\n",
        "#                 if row_has_changes:\n",
        "#                     changed_rows.add(i)\n",
        "#                     # Log each changed row immediately\n",
        "#                     log_to_file(f\"\"\"ROW {i} CHANGES ({len(row_changes)} changes):\n",
        "# {json.dumps(row_changes, indent=2)}\n",
        "# \"\"\")\n",
        "\n",
        "#                 # Log progress every 100 rows for large dataframes\n",
        "#                 if (i + 1) % 100 == 0:\n",
        "#                     log_to_file(f\"Progress: Processed {i + 1}/{len(original_df)} rows, found {total_changes} changes so far\")\n",
        "\n",
        "#             print(f\"✅ Complete comparison finished: {total_changes} total changes detected across {len(changed_rows)} rows\")\n",
        "\n",
        "#             # Log comprehensive change analysis\n",
        "#             log_to_file(f\"\"\"COMPLETE CELL-BY-CELL COMPARISON RESULTS:\n",
        "# =====================================\n",
        "# Total Changes Detected: {total_changes}\n",
        "# Affected Rows: {len(changed_rows)} out of {original_df.shape[0]} ({len(changed_rows)/original_df.shape[0]*100:.1f}%)\n",
        "# Affected Columns: {len(changed_columns)} out of {len(original_df.columns)} ({len(changed_columns)/len(original_df.columns)*100:.1f}%)\n",
        "\n",
        "# AFFECTED COLUMNS LIST:\n",
        "# {list(changed_columns)}\n",
        "\n",
        "# AFFECTED ROWS LIST:\n",
        "# {sorted(list(changed_rows))}\n",
        "\n",
        "# ALL DETECTED CHANGES ({len(changed_cells)} total):\n",
        "# \"\"\", \"COMPREHENSIVE CHANGE DETECTION RESULTS\")\n",
        "\n",
        "#             # Log ALL changes, not just a sample\n",
        "#             for i, change in enumerate(changed_cells):\n",
        "#                 log_to_file(f\"Change {i+1}: Row {change['row']}, Column '{change['column']}' ({change['change_type']}): '{change['original']}' → '{change['modified']}'\")\n",
        "\n",
        "#         else:\n",
        "#             log_to_file(f\"\"\"CANNOT PERFORM CELL-BY-CELL COMPARISON:\n",
        "# Reason: Shape or column structure differs\n",
        "# Original shape: {original_df.shape}\n",
        "# Modified shape: {modified_df.shape}\n",
        "# Original columns: {list(original_df.columns)}\n",
        "# Modified columns: {list(modified_df.columns)}\n",
        "# \"\"\", \"CELL-BY-CELL COMPARISON SKIPPED\")\n",
        "\n",
        "#         # Calculate change statistics\n",
        "#         total_cells = original_df.shape[0] * original_df.shape[1] if original_df.size > 0 else 1\n",
        "#         change_density = total_changes / total_cells\n",
        "\n",
        "#         log_to_file(f\"\"\"COMPREHENSIVE CHANGE STATISTICS:\n",
        "# ===============================\n",
        "# Total Cells in Original: {total_cells}\n",
        "# Total Cells Changed: {total_changes}\n",
        "# Change Density: {change_density*100:.4f}%\n",
        "# Percentage of Rows Affected: {len(changed_rows)/original_df.shape[0]*100:.2f}% ({len(changed_rows)}/{original_df.shape[0]})\n",
        "# Percentage of Columns Affected: {len(changed_columns)/len(original_df.columns)*100:.2f}% ({len(changed_columns)}/{len(original_df.columns)})\n",
        "\n",
        "# CHANGE PATTERN ANALYSIS:\n",
        "# - NaN Changes: {len([c for c in changed_cells if c.get('change_type') == 'nan_change'])}\n",
        "# - Value Changes: {len([c for c in changed_cells if c.get('change_type') == 'value_change'])}\n",
        "# - Most Affected Columns: {sorted(changed_columns)[:10]}\n",
        "# - Row Change Distribution: Every {original_df.shape[0]//max(1,len(changed_rows)):.0f} rows on average\n",
        "# \"\"\", \"COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
        "\n",
        "#         # STEP 1: Use GPT-4.1 to analyze and generate Claude prompt\n",
        "#         print(\"🧠 GPT-4.1: Analyzing changes and generating Claude prompt...\")\n",
        "#         log_to_file(\"🧠 STARTING GPT-4.1 ANALYSIS TO GENERATE CLAUDE PROMPT...\", \"GPT-4.1 PROMPT GENERATION START\")\n",
        "\n",
        "#         claude_prompt = _generate_claude_prompt_with_gpt4_logged(\n",
        "#             client, original_info, modified_info, user_description,\n",
        "#             total_changes, changed_rows, changed_columns, changed_cells,\n",
        "#             log_to_file\n",
        "#         )\n",
        "\n",
        "#         # STEP 2: Test the Claude prompt by actually running it\n",
        "#         print(\"🤖 Testing Claude prompt replication...\")\n",
        "#         log_to_file(\"🤖 STARTING CLAUDE PROMPT REPLICATION TESTING...\", \"CLAUDE REPLICATION TESTING START\")\n",
        "\n",
        "#         replication_results = _test_claude_prompt_replication_logged(\n",
        "#             original_df, modified_df, claude_prompt, max_attempts=3, log_to_file=log_to_file\n",
        "#         )\n",
        "\n",
        "#         # STEP 3: Get final analysis with working prompts embedded\n",
        "#         log_to_file(\"📊 GENERATING FINAL ANALYSIS WITH GPT-4.1...\", \"FINAL ANALYSIS GENERATION START\")\n",
        "\n",
        "#         final_analysis = _get_final_analysis_with_prompts_logged(\n",
        "#             client, original_info, modified_info, user_description,\n",
        "#             total_changes, changed_rows, changed_columns, changed_cells,\n",
        "#             claude_prompt, replication_results, log_to_file\n",
        "#         )\n",
        "\n",
        "#         # Add technical metadata and log file reference\n",
        "#         final_analysis[\"raw_gpt_response\"] = final_analysis.get(\"raw_gpt_response\", \"\")\n",
        "#         final_analysis[\"complete_comparison_performed\"] = True\n",
        "#         final_analysis[\"log_file_path\"] = log_file_path  # ALWAYS include log file path\n",
        "#         final_analysis[\"full_change_statistics\"] = {\n",
        "#             \"total_cells\": total_cells,\n",
        "#             \"total_changes\": total_changes,\n",
        "#             \"change_density\": change_density,\n",
        "#             \"affected_rows\": list(changed_rows),\n",
        "#             \"affected_columns\": list(changed_columns),\n",
        "#             \"replication_tested\": True,\n",
        "#             \"replication_success\": replication_results[\"final_success\"],\n",
        "#             \"replication_attempts\": len(replication_results[\"attempts\"]),\n",
        "#             \"all_changes\": changed_cells  # Include ALL detected changes\n",
        "#         }\n",
        "\n",
        "#         # Log final comprehensive results\n",
        "#         log_to_file(f\"\"\"ANALYSIS EXECUTION COMPLETED SUCCESSFULLY\n",
        "# ==========================================\n",
        "# Total Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "# Log File Saved: {log_file_path}\n",
        "# Log File Size: {os.path.getsize(log_file_path)} bytes\n",
        "\n",
        "# FINAL RESULTS SUMMARY:\n",
        "# - Replication Success: {replication_results[\"final_success\"]}\n",
        "# - Best Match Score: {replication_results.get(\"best_attempt\", {}).get(\"match_score\", 0):.2%}\n",
        "# - Total Replication Attempts: {len(replication_results[\"attempts\"])}\n",
        "# - Changes Detected: {total_changes}\n",
        "# - Change Density: {change_density*100:.4f}%\n",
        "\n",
        "# COMPLETE FINAL ANALYSIS OBJECT:\n",
        "# {json.dumps(final_analysis, indent=2, default=str)}\n",
        "\n",
        "# ✅ ANALYSIS COMPLETE - ALL LOGS SAVED TO: {log_file_path}\n",
        "# \"\"\", \"FINAL EXECUTION RESULTS\")\n",
        "\n",
        "#         print(f\"📝 Complete analysis with all logs saved to: {log_file_path}\")\n",
        "#         return final_analysis\n",
        "\n",
        "#     except Exception as e:\n",
        "#         # ALWAYS log errors, even if everything else fails\n",
        "#         error_details = f\"\"\"CRITICAL ERROR DURING ANALYSIS EXECUTION\n",
        "# ========================================\n",
        "# Error Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "# Error Type: {type(e).__name__}\n",
        "# Error Message: {str(e)}\n",
        "\n",
        "# FULL STACK TRACE:\n",
        "# {traceback.format_exc()}\n",
        "\n",
        "# EXECUTION CONTEXT:\n",
        "# - User Description: \"{user_description}\"\n",
        "# - Original DF Shape: {original_df.shape if 'original_df' in locals() else 'Unknown'}\n",
        "# - Modified DF Shape: {modified_df.shape if 'modified_df' in locals() else 'Unknown'}\n",
        "# - Log File: {log_file_path}\n",
        "\n",
        "# ❌ ANALYSIS FAILED - ERROR LOGGED TO: {log_file_path}\n",
        "# \"\"\"\n",
        "\n",
        "#         log_to_file(error_details, \"CRITICAL ERROR\")\n",
        "\n",
        "#         error_msg = f\"Error in complete GPT-4 dataframe analysis: {e}\"\n",
        "#         logger.error(error_msg)\n",
        "#         print(f\"❌ Analysis failed but logs saved to: {log_file_path}\")\n",
        "\n",
        "#         return {\n",
        "#             \"change_summary\": f\"Complete dataframe analysis failed: {user_description}\",\n",
        "#             \"change_type\": \"data_edit\",\n",
        "#             \"session_description\": f\"User made changes to entire dataframe. Description: {user_description}. Error in analysis: {str(e)}\",\n",
        "#             \"error\": str(e),\n",
        "#             \"complete_comparison_performed\": False,\n",
        "#             \"log_file_path\": log_file_path,  # ALWAYS include log file path, even on error\n",
        "#             \"error_logged\": True\n",
        "#         }\n",
        "#         error_msg = f\"Error in complete GPT-4 dataframe analysis: {e}\"\n",
        "#         log_to_file(f\"\"\"CRITICAL ERROR:\n",
        "# Error Message: {str(e)}\n",
        "# Error Type: {type(e).__name__}\n",
        "# Stack Trace:\n",
        "# {traceback.format_exc()}\n",
        "# \"\"\", \"ERROR ANALYSIS\")\n",
        "\n",
        "#         logger.error(error_msg)\n",
        "#         return {\n",
        "#             \"change_summary\": f\"Complete dataframe analysis failed: {user_description}\",\n",
        "#             \"change_type\": \"data_edit\",\n",
        "#             \"session_description\": f\"User made changes to entire dataframe. Description: {user_description}. Error in analysis: {str(e)}\",\n",
        "#             \"error\": str(e),\n",
        "#             \"complete_comparison_performed\": False,\n",
        "#             \"log_file\": log_file_path\n",
        "#         }\n",
        "\n",
        "\n",
        "# def _generate_claude_prompt_with_gpt4_logged(client, original_info, modified_info, user_description,\n",
        "#                                            total_changes, changed_rows, changed_columns, changed_cells,\n",
        "#                                            log_to_file):\n",
        "#     \"\"\"\n",
        "#     Enhanced version with comprehensive logging of GPT-4.1 prompt generation\n",
        "#     \"\"\"\n",
        "#     prompt_generation_request = f\"\"\"\n",
        "#     You are an expert at analyzing dataframe changes and generating precise prompts for Claude 3.7 to replicate manual edits.\n",
        "\n",
        "#     ORIGINAL DATAFRAME:\n",
        "#     {original_info['full_data']}\n",
        "\n",
        "#     MODIFIED DATAFRAME:\n",
        "#     {modified_info['full_data']}\n",
        "\n",
        "#     CHANGE ANALYSIS:\n",
        "#     - Total changes: {total_changes}\n",
        "#     - Changed rows: {list(changed_rows)[:20] if changed_rows else []}\n",
        "#     - Changed columns: {list(changed_columns)}\n",
        "#     - Sample changes: {changed_cells[:10]}\n",
        "#     - User description: \"{user_description}\"\n",
        "\n",
        "#     Generate a PRECISE prompt for Claude 3.7 that would replicate these exact changes.\n",
        "#     Focus on:\n",
        "#     1. Specific column names and filtering criteria\n",
        "#     2. Exact transformation logic\n",
        "#     3. Clear, executable instructions\n",
        "#     4. Business context for rent roll data\n",
        "\n",
        "#     Return ONLY the Claude prompt text, nothing else.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Log the complete GPT-4.1 prompt\n",
        "#     log_to_file(f\"\"\"GPT-4.1 PROMPT TO GENERATE CLAUDE INSTRUCTIONS:\n",
        "# Model: gpt-4.1\n",
        "# Temperature: 0.1\n",
        "# Max Tokens: 3000\n",
        "\n",
        "# COMPLETE PROMPT SENT TO GPT-4.1:\n",
        "# {'-'*60}\n",
        "# {prompt_generation_request}\n",
        "# {'-'*60}\n",
        "# \"\"\", \"GPT-4.1 REQUEST\")\n",
        "\n",
        "#     try:\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4.1\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": \"You are an expert at generating precise data transformation prompts. Return only the Claude prompt text.\"},\n",
        "#                 {\"role\": \"user\", \"content\": prompt_generation_request}\n",
        "#             ],\n",
        "#             max_tokens=3000,\n",
        "#             temperature=0.1\n",
        "#         )\n",
        "\n",
        "#         gpt_response = response.choices[0].message.content.strip()\n",
        "\n",
        "#         # Log GPT-4.1 response\n",
        "#         log_to_file(f\"\"\"GPT-4.1 RESPONSE (CLAUDE PROMPT):\n",
        "# Response Length: {len(gpt_response)} characters\n",
        "# Tokens Used: Approximately {len(gpt_response.split())} words\n",
        "\n",
        "# GENERATED CLAUDE PROMPT:\n",
        "# {'-'*60}\n",
        "# {gpt_response}\n",
        "# {'-'*60}\n",
        "# \"\"\", \"GPT-4.1 RESPONSE\")\n",
        "\n",
        "#         return gpt_response\n",
        "\n",
        "#     except Exception as e:\n",
        "#         error_msg = f\"GPT-4.1 API Error: {str(e)}\"\n",
        "#         log_to_file(f\"\"\"GPT-4.1 API CALL FAILED:\n",
        "# Error: {error_msg}\n",
        "# Fallback: Using basic prompt\n",
        "# \"\"\")\n",
        "#         return f\"Replicate the manual edits described as: {user_description}\"\n",
        "\n",
        "\n",
        "# def _test_claude_prompt_replication_logged(original_df, target_df, claude_prompt, max_attempts=3, log_to_file=None):\n",
        "#     \"\"\"\n",
        "#     Enhanced version with comprehensive logging of Claude replication attempts\n",
        "#     \"\"\"\n",
        "#     attempts = []\n",
        "#     current_prompt = claude_prompt\n",
        "\n",
        "#     log_to_file(f\"\"\"CLAUDE REPLICATION TESTING STARTED:\n",
        "# Maximum Attempts: {max_attempts}\n",
        "# Target DataFrame Shape: {target_df.shape}\n",
        "# Original DataFrame Shape: {original_df.shape}\n",
        "\n",
        "# INITIAL CLAUDE PROMPT TO TEST:\n",
        "# {'-'*60}\n",
        "# {claude_prompt}\n",
        "# {'-'*60}\n",
        "# \"\"\", \"REPLICATION TESTING INITIALIZATION\")\n",
        "\n",
        "#     for attempt in range(max_attempts):\n",
        "#         print(f\"🔄 Attempt {attempt + 1}/{max_attempts}\")\n",
        "#         log_to_file(f\"Starting attempt {attempt + 1}/{max_attempts}...\", f\"ATTEMPT {attempt + 1}\")\n",
        "\n",
        "#         try:\n",
        "#             # Call actual Claude API with the generated prompt\n",
        "#             claude_response = _call_actual_claude_logged(current_prompt, original_df, log_to_file, attempt + 1)\n",
        "\n",
        "#             # Extract and execute code\n",
        "#             code_blocks = _extract_code_blocks(claude_response)\n",
        "\n",
        "#             log_to_file(f\"\"\"CODE EXTRACTION RESULTS:\n",
        "# Found {len(code_blocks)} code blocks\n",
        "# Code Blocks:\n",
        "# \"\"\")\n",
        "\n",
        "#             for i, code in enumerate(code_blocks):\n",
        "#                 log_to_file(f\"Code Block {i+1}:\\n```python\\n{code}\\n```\\n\")\n",
        "\n",
        "#             if not code_blocks:\n",
        "#                 attempts.append({\n",
        "#                     \"attempt\": attempt + 1,\n",
        "#                     \"success\": False,\n",
        "#                     \"error\": \"No code found\",\n",
        "#                     \"prompt_used\": current_prompt\n",
        "#                 })\n",
        "#                 log_to_file(\"No code blocks found in Claude response\")\n",
        "#                 continue\n",
        "\n",
        "#             # Execute the code with proper globals\n",
        "#             test_df = original_df.copy()\n",
        "#             exec_globals = {\n",
        "#                 \"df\": test_df,\n",
        "#                 \"pd\": pd,\n",
        "#                 \"np\": np,\n",
        "#                 \"os\": os,\n",
        "#                 \"datetime\": datetime\n",
        "#             }\n",
        "\n",
        "#             log_to_file(\"Starting code execution...\")\n",
        "#             execution_output = \"\"\n",
        "\n",
        "#             for i, code in enumerate(code_blocks):\n",
        "#                 try:\n",
        "#                     log_to_file(f\"Executing code block {i+1}...\")\n",
        "#                     exec(code, exec_globals)\n",
        "#                     execution_output += f\"Code block {i+1} executed successfully\\n\"\n",
        "#                 except Exception as exec_error:\n",
        "#                     execution_output += f\"Code block {i+1} failed: {str(exec_error)}\\n\"\n",
        "#                     log_to_file(f\"Code block {i+1} execution error: {str(exec_error)}\")\n",
        "\n",
        "#             result_df = exec_globals[\"df\"]\n",
        "#             log_to_file(f\"Execution completed. Result DataFrame shape: {result_df.shape}\")\n",
        "\n",
        "#             # Calculate match score\n",
        "#             match_score = _calculate_match_score(target_df, result_df)\n",
        "\n",
        "#             attempts.append({\n",
        "#                 \"attempt\": attempt + 1,\n",
        "#                 \"success\": match_score >= 0.95,\n",
        "#                 \"match_score\": match_score,\n",
        "#                 \"generated_code\": code_blocks,\n",
        "#                 \"prompt_used\": current_prompt,\n",
        "#                 \"result_shape\": result_df.shape,\n",
        "#                 \"target_shape\": target_df.shape,\n",
        "#                 \"execution_output\": execution_output\n",
        "#             })\n",
        "\n",
        "#             log_to_file(f\"\"\"ATTEMPT {attempt + 1} RESULTS:\n",
        "# Match Score: {match_score:.2%}\n",
        "# Success Threshold (95%): {'✅ PASSED' if match_score >= 0.95 else '❌ FAILED'}\n",
        "# Result Shape: {result_df.shape}\n",
        "# Target Shape: {target_df.shape}\n",
        "# Execution Output:\n",
        "# {execution_output}\n",
        "# \"\"\")\n",
        "\n",
        "#             print(f\"📊 Match score: {match_score:.2%}\")\n",
        "\n",
        "#             if match_score >= 0.95:\n",
        "#                 print(\"✅ Replication successful!\")\n",
        "#                 log_to_file(\"🎉 REPLICATION SUCCESSFUL! Stopping attempts.\")\n",
        "#                 break\n",
        "#             else:\n",
        "#                 # Improve prompt for next attempt\n",
        "#                 current_prompt = _improve_claude_prompt(current_prompt, target_df, result_df, match_score)\n",
        "#                 log_to_file(f\"Match score below threshold. Improving prompt for next attempt:\\n{current_prompt}\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             error_msg = str(e)\n",
        "#             attempts.append({\n",
        "#                 \"attempt\": attempt + 1,\n",
        "#                 \"success\": False,\n",
        "#                 \"error\": error_msg,\n",
        "#                 \"prompt_used\": current_prompt\n",
        "#             })\n",
        "#             log_to_file(f\"ATTEMPT {attempt + 1} FAILED with error: {error_msg}\")\n",
        "#             print(f\"❌ Error: {error_msg}\")\n",
        "\n",
        "#     final_success = any(attempt[\"success\"] for attempt in attempts)\n",
        "#     best_attempt = max(attempts, key=lambda x: x.get(\"match_score\", 0)) if attempts else None\n",
        "\n",
        "#     log_to_file(f\"\"\"REPLICATION TESTING COMPLETE:\n",
        "# Final Success: {final_success}\n",
        "# Best Match Score: {best_attempt.get('match_score', 0):.2% if best_attempt else 'N/A'}\n",
        "# Total Attempts: {len(attempts)}\n",
        "\n",
        "# ALL ATTEMPTS SUMMARY:\n",
        "# \"\"\", \"REPLICATION TESTING RESULTS\")\n",
        "\n",
        "#     for i, attempt in enumerate(attempts, 1):\n",
        "#         log_to_file(f\"Attempt {i}: Success={attempt.get('success', False)}, Score={attempt.get('match_score', 0):.2%}, Error={attempt.get('error', 'None')}\")\n",
        "\n",
        "#     return {\n",
        "#         \"attempts\": attempts,\n",
        "#         \"final_success\": final_success,\n",
        "#         \"best_attempt\": best_attempt,\n",
        "#         \"final_prompt\": best_attempt[\"prompt_used\"] if best_attempt else claude_prompt\n",
        "#     }\n",
        "\n",
        "\n",
        "# def _call_actual_claude_logged(prompt, original_df, log_to_file, attempt_number):\n",
        "#     \"\"\"\n",
        "#     Enhanced version with comprehensive logging of Claude API calls\n",
        "#     \"\"\"\n",
        "#     log_to_file(f\"\"\"CALLING CLAUDE API - ATTEMPT {attempt_number}:\n",
        "# Model: claude-3-7-sonnet-20250219\n",
        "# Temperature: 0.2\n",
        "# Max Tokens: 2000\n",
        "\n",
        "# PROMPT BEING SENT TO CLAUDE:\n",
        "# {'-'*60}\n",
        "# {prompt}\n",
        "# {'-'*60}\n",
        "\n",
        "# DATAFRAME CONTEXT BEING SENT:\n",
        "# Shape: {original_df.shape}\n",
        "# Columns: {list(original_df.columns)}\n",
        "# First 10 rows:\n",
        "# {original_df.head(10).to_string()}\n",
        "# \"\"\", f\"CLAUDE API CALL {attempt_number}\")\n",
        "\n",
        "#     try:\n",
        "#         # Get Anthropic client\n",
        "#         anthropic_client = Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "#         # Prepare dataframe context for Claude\n",
        "#         df_summary = f\"\"\"\n",
        "#         DATAFRAME CONTENT:\n",
        "#         {original_df.to_string(max_rows=50)}\n",
        "\n",
        "#         DATAFRAME STATISTICS:\n",
        "#         - Shape: {original_df.shape}\n",
        "#         - Columns: {list(original_df.columns)}\n",
        "#         - Data types: {dict(original_df.dtypes)}\n",
        "#         - Null values per column: {dict(original_df.isnull().sum())}\n",
        "#         \"\"\"\n",
        "\n",
        "#         # System prompt for Claude\n",
        "#         claude_system_prompt = \"\"\"You are an expert Python data analyst.\n",
        "#         You will receive a dataframe that is already loaded as 'df' and a specific task to perform.\n",
        "#         Generate Python pandas code to accomplish the task.\n",
        "\n",
        "#         IMPORTANT RULES:\n",
        "#         1. The dataframe 'df' is already loaded - do not load or import it\n",
        "#         2. Wrap all code in ```python and ``` blocks\n",
        "#         3. Do not use try-except blocks - let errors propagate naturally\n",
        "#         4. Be precise and specific in your transformations\n",
        "#         5. Provide working, executable code\n",
        "#         6. Focus on the exact transformation requested\"\"\"\n",
        "\n",
        "#         # Prepare messages for Claude\n",
        "#         claude_messages = [\n",
        "#             {\n",
        "#                 \"role\": \"user\",\n",
        "#                 \"content\": f\"Here is the dataframe that's already loaded as 'df':\\n\\n{df_summary}\\n\\nTASK: {prompt}\\n\\nGenerate Python code to accomplish this task.\"\n",
        "#             }\n",
        "#         ]\n",
        "\n",
        "#         # Call Claude\n",
        "#         claude_response = anthropic_client.messages.create(\n",
        "#             model=\"claude-sonnet-4-20250514\",  # Use the latest Claude model\n",
        "#             system=claude_system_prompt,\n",
        "#             messages=claude_messages,\n",
        "#             max_tokens=2000,\n",
        "#             temperature=0.2\n",
        "#         )\n",
        "\n",
        "#         # Extract response text\n",
        "#         response_text = claude_response.content[0].text\n",
        "\n",
        "#         log_to_file(f\"\"\"CLAUDE API RESPONSE - ATTEMPT {attempt_number}:\n",
        "# Response Length: {len(response_text)} characters\n",
        "# Response Received Successfully: ✅\n",
        "\n",
        "# FULL CLAUDE RESPONSE:\n",
        "# {'-'*60}\n",
        "# {response_text}\n",
        "# {'-'*60}\n",
        "# \"\"\")\n",
        "\n",
        "#         return response_text\n",
        "\n",
        "#     except Exception as e:\n",
        "#         error_msg = f\"Claude API Error: {str(e)}\"\n",
        "#         log_to_file(f\"\"\"CLAUDE API CALL FAILED - ATTEMPT {attempt_number}:\n",
        "# Error: {error_msg}\n",
        "# Using fallback response\n",
        "# \"\"\")\n",
        "\n",
        "#         # Fallback response if Claude API fails\n",
        "#         fallback_response = f\"\"\"\n",
        "#         I'll help you with this task. Here's the code:\n",
        "\n",
        "#         ```python\n",
        "#         # Error calling Claude API: {str(e)}\n",
        "#         # Fallback code\n",
        "#         print(\"Claude API error, using fallback\")\n",
        "#         print(f\"Dataframe shape: {{df.shape}}\")\n",
        "#         print(df.head())\n",
        "#         ```\n",
        "#         \"\"\"\n",
        "\n",
        "#         log_to_file(f\"Fallback response generated:\\n{fallback_response}\")\n",
        "#         return fallback_response\n",
        "\n",
        "\n",
        "# def _get_final_analysis_with_prompts_logged(client, original_info, modified_info, user_description,\n",
        "#                                           total_changes, changed_rows, changed_columns, changed_cells,\n",
        "#                                           claude_prompt, replication_results, log_to_file):\n",
        "#     \"\"\"\n",
        "#     Enhanced version with comprehensive logging of final analysis generation\n",
        "#     \"\"\"\n",
        "#     best_attempt = replication_results.get(\"best_attempt\", {})\n",
        "#     final_claude_prompt = replication_results.get(\"final_prompt\", claude_prompt)\n",
        "#     success_status = \"SUCCESS\" if replication_results[\"final_success\"] else \"PARTIAL\"\n",
        "\n",
        "#     # Calculate statistics\n",
        "#     total_cells = original_info[\"shape\"][0] * original_info[\"shape\"][1] if original_info[\"shape\"][0] > 0 else 1\n",
        "#     change_density = total_changes / total_cells\n",
        "\n",
        "#     analysis_prompt = f\"\"\"\n",
        "#     Analyze this dataframe change and provide a comprehensive summary.\n",
        "\n",
        "#     ORIGINAL: {original_info['shape']} with data:\n",
        "#     {original_info['full_data'][:2000]}...\n",
        "\n",
        "#     MODIFIED: {modified_info['shape']} with data:\n",
        "#     {modified_info['full_data'][:2000]}...\n",
        "\n",
        "#     CHANGES: {total_changes} cells changed ({change_density*100:.1f}%)\n",
        "#     USER DESCRIPTION: {user_description}\n",
        "\n",
        "#     TESTED REPLICATION: {success_status}\n",
        "#     - Attempts: {len(replication_results['attempts'])}\n",
        "#     - Best match: {best_attempt.get('match_score', 0):.1%}\n",
        "\n",
        "#     Provide analysis in this exact JSON format:\n",
        "#     {{\n",
        "#         \"change_summary\": \"Detailed technical summary including WORKING_CLAUDE_PROMPT: {final_claude_prompt} and GPT4_ANALYSIS_PROMPT: [the prompt used to generate the Claude prompt] - describe what changed and how replication performed\",\n",
        "#         \"change_type\": \"data_edit|structure_change|mixed\",\n",
        "#         \"structural_changes\": {{\n",
        "#             \"rows_added\": {max(0, modified_info['shape'][0] - original_info['shape'][0])},\n",
        "#             \"rows_removed\": {max(0, original_info['shape'][0] - modified_info['shape'][0])},\n",
        "#             \"columns_added\": [],\n",
        "#             \"columns_removed\": []\n",
        "#         }},\n",
        "#         \"data_modifications\": {{\n",
        "#             \"cells_changed\": {total_changes},\n",
        "#             \"total_cells\": {total_cells},\n",
        "#             \"change_percentage\": {change_density*100:.2f},\n",
        "#             \"rows_affected\": {len(changed_rows)},\n",
        "#             \"columns_affected\": {list(changed_columns)},\n",
        "#             \"common_patterns\": [\"Patterns identified\"],\n",
        "#             \"data_quality_impact\": \"improved|degraded|neutral\"\n",
        "#         }},\n",
        "#         \"business_impact\": {{\n",
        "#             \"rent_calculations_affected\": \"Analysis of rent impact\",\n",
        "#             \"tenant_information_updated\": \"Analysis of tenant data changes\",\n",
        "#             \"occupancy_status_changed\": \"Analysis of occupancy changes\"\n",
        "#         }},\n",
        "#         \"recommendations\": [\"Specific recommendations\"],\n",
        "#         \"session_description\": \"REPLICATION_STATUS: {success_status} | FINAL_CLAUDE_PROMPT: {final_claude_prompt} | REPLICATION_ATTEMPTS: {len(replication_results['attempts'])} | USER_EDIT: {user_description} | Changes: {total_changes} cells ({change_density*100:.1f}%) | Best match: {best_attempt.get('match_score', 0):.1%}\"\n",
        "#     }}\n",
        "#     \"\"\"\n",
        "\n",
        "#     log_to_file(f\"\"\"FINAL ANALYSIS GENERATION:\n",
        "# Sending final analysis request to GPT-4.1...\n",
        "\n",
        "# PROMPT FOR FINAL ANALYSIS:\n",
        "# {'-'*60}\n",
        "# {analysis_prompt}\n",
        "# {'-'*60}\n",
        "# \"\"\", \"FINAL ANALYSIS GENERATION\")\n",
        "\n",
        "#     try:\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4.1\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"system\", \"content\": \"You are a data analyst. Return only valid JSON.\"},\n",
        "#                 {\"role\": \"user\", \"content\": analysis_prompt}\n",
        "#             ],\n",
        "#             max_tokens=3000,\n",
        "#             temperature=0.2\n",
        "#         )\n",
        "\n",
        "#         gpt_final_response = response.choices[0].message.content\n",
        "#         log_to_file(f\"\"\"FINAL ANALYSIS GPT-4.1 RESPONSE:\n",
        "# {'-'*60}\n",
        "# {gpt_final_response}\n",
        "# {'-'*60}\n",
        "# \"\"\")\n",
        "\n",
        "#         try:\n",
        "#             import json\n",
        "#             import re\n",
        "#             json_match = re.search(r'{.*}', gpt_final_response, re.DOTALL)\n",
        "#             if json_match:\n",
        "#                 parsed_analysis = json.loads(json_match.group(0))\n",
        "#                 log_to_file(\"✅ Successfully parsed JSON from final analysis response\")\n",
        "#                 return parsed_analysis\n",
        "#             else:\n",
        "#                 log_to_file(\"❌ No JSON found in final analysis response\")\n",
        "#         except Exception as json_error:\n",
        "#             log_to_file(f\"❌ JSON parsing failed: {str(json_error)}\")\n",
        "\n",
        "#     except Exception as api_error:\n",
        "#         log_to_file(f\"❌ Final analysis API call failed: {str(api_error)}\")\n",
        "\n",
        "#     # Fallback\n",
        "#     fallback_analysis = {\n",
        "#         \"change_summary\": f\"WORKING_CLAUDE_PROMPT: {final_claude_prompt} | GPT4_ANALYSIS_PROMPT: [embedded] | {user_description} - {total_changes} changes with {replication_results['final_success']} replication\",\n",
        "#         \"change_type\": \"data_edit\",\n",
        "#         \"session_description\": f\"REPLICATION_STATUS: {success_status} | FINAL_CLAUDE_PROMPT: {final_claude_prompt} | USER_EDIT: {user_description} | Changes: {total_changes} cells\",\n",
        "#         \"data_modifications\": {\n",
        "#             \"cells_changed\": total_changes,\n",
        "#             \"total_cells\": total_cells,\n",
        "#             \"change_percentage\": change_density*100\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "#     log_to_file(f\"Using fallback analysis:\\n{json.dumps(fallback_analysis, indent=2)}\")\n",
        "\n",
        "#     return fallback_analysis\n",
        "\n",
        "\n",
        "# # Additional helper functions for the enhanced logging system\n",
        "\n",
        "# def _extract_code_blocks(response_text):\n",
        "#     \"\"\"\n",
        "#     Extract Python code blocks from response text\n",
        "#     \"\"\"\n",
        "#     import re\n",
        "#     code_pattern = r'```(?:python)?\\s*(.*?)```'\n",
        "#     code_blocks = re.findall(code_pattern, response_text, re.DOTALL)\n",
        "#     return [block.strip() for block in code_blocks if block.strip()]\n",
        "\n",
        "\n",
        "# def _calculate_match_score(target_df, result_df):\n",
        "#     \"\"\"\n",
        "#     Calculate how closely the result matches the target with improved scoring.\n",
        "#     Now gives partial credit for near-matches instead of strict 0/1 scoring.\n",
        "#     \"\"\"\n",
        "#     import pandas as pd\n",
        "\n",
        "#     # Handle edge cases\n",
        "#     if target_df.empty and result_df.empty:\n",
        "#         return 1.0\n",
        "#     if target_df.empty or result_df.empty:\n",
        "#         return 0.0\n",
        "\n",
        "#     # 1. Shape similarity score (30% weight)\n",
        "#     target_rows, target_cols = target_df.shape\n",
        "#     result_rows, result_cols = result_df.shape\n",
        "\n",
        "#     # Row similarity (allows for minor filtering differences)\n",
        "#     if target_rows == 0:\n",
        "#         row_score = 1.0 if result_rows == 0 else 0.0\n",
        "#     else:\n",
        "#         row_diff = abs(target_rows - result_rows)\n",
        "#         row_score = max(0, 1.0 - (row_diff / target_rows))\n",
        "\n",
        "#     # Column similarity (should be exact for most use cases)\n",
        "#     col_score = 1.0 if target_cols == result_cols else 0.0\n",
        "\n",
        "#     shape_score = 0.7 * row_score + 0.3 * col_score\n",
        "\n",
        "#     # 2. Column structure score (20% weight)\n",
        "#     target_columns = list(target_df.columns)\n",
        "#     result_columns = list(result_df.columns)\n",
        "\n",
        "#     if target_columns == result_columns:\n",
        "#         column_score = 1.0\n",
        "#     else:\n",
        "#         # Partial credit for column overlap\n",
        "#         common_cols = set(target_columns) & set(result_columns)\n",
        "#         total_unique_cols = set(target_columns) | set(result_columns)\n",
        "#         column_score = len(common_cols) / len(total_unique_cols) if total_unique_cols else 0.0\n",
        "\n",
        "#     # 3. Content similarity score (50% weight)\n",
        "#     content_score = 0.0\n",
        "\n",
        "#     if target_columns == result_columns and len(target_columns) > 0:\n",
        "#         # Compare overlapping rows when columns match\n",
        "#         min_rows = min(len(target_df), len(result_df))\n",
        "\n",
        "#         if min_rows > 0:\n",
        "#             total_cells = 0\n",
        "#             matching_cells = 0\n",
        "\n",
        "#             # Compare cell by cell for overlapping area\n",
        "#             for i in range(min_rows):\n",
        "#                 for col in target_columns:\n",
        "#                     total_cells += 1\n",
        "\n",
        "#                     try:\n",
        "#                         target_val = target_df.iloc[i][col]\n",
        "#                         result_val = result_df.iloc[i][col]\n",
        "\n",
        "#                         # Handle NaN comparisons\n",
        "#                         if pd.isna(target_val) and pd.isna(result_val):\n",
        "#                             matching_cells += 1\n",
        "#                         elif pd.isna(target_val) or pd.isna(result_val):\n",
        "#                             # NaN vs non-NaN = no match\n",
        "#                             continue\n",
        "#                         else:\n",
        "#                             # String comparison with whitespace handling\n",
        "#                             target_str = str(target_val).strip()\n",
        "#                             result_str = str(result_val).strip()\n",
        "\n",
        "#                             if target_str == result_str:\n",
        "#                                 matching_cells += 1\n",
        "#                             else:\n",
        "#                                 # Try numeric comparison for potential formatting differences\n",
        "#                                 try:\n",
        "#                                     target_num = float(target_str.replace(',', '').replace('$', ''))\n",
        "#                                     result_num = float(result_str.replace(',', '').replace('$', ''))\n",
        "#                                     if abs(target_num - result_num) < 0.01:  # Allow small floating point differences\n",
        "#                                         matching_cells += 1\n",
        "#                                 except (ValueError, TypeError):\n",
        "#                                     # Not numeric, keep as non-match\n",
        "#                                     continue\n",
        "#                     except (IndexError, KeyError):\n",
        "#                         # Skip invalid cell references\n",
        "#                         continue\n",
        "\n",
        "#             content_score = matching_cells / total_cells if total_cells > 0 else 0.0\n",
        "\n",
        "#             # Bonus for exact row count match when content is high\n",
        "#             if len(target_df) == len(result_df) and content_score > 0.9:\n",
        "#                 content_score = min(1.0, content_score * 1.05)\n",
        "\n",
        "#     else:\n",
        "#         # Different column structures - can only do basic comparison\n",
        "#         if min(len(target_df), len(result_df)) > 0:\n",
        "#             # Give small credit for having some data with similar row count\n",
        "#             row_similarity = 1.0 - abs(len(target_df) - len(result_df)) / max(len(target_df), len(result_df))\n",
        "#             content_score = 0.2 * row_similarity  # Low score for structure mismatch\n",
        "\n",
        "#     # 4. Calculate weighted final score\n",
        "#     final_score = (0.30 * shape_score +\n",
        "#                   0.20 * column_score +\n",
        "#                   0.50 * content_score)\n",
        "\n",
        "#     # Ensure score is between 0 and 1\n",
        "#     return max(0.0, min(1.0, final_score))\n",
        "\n",
        "\n",
        "\n",
        "# def _improve_claude_prompt(current_prompt, target_df, result_df, match_score):\n",
        "#     \"\"\"\n",
        "#     Improve the Claude prompt based on the mismatch\n",
        "#     \"\"\"\n",
        "#     feedback = f\"\\nPREVIOUS ATTEMPT FEEDBACK:\\n\"\n",
        "#     feedback += f\"Match score: {match_score:.2%}\\n\"\n",
        "#     feedback += f\"Target shape: {target_df.shape}, Result shape: {result_df.shape}\\n\"\n",
        "\n",
        "#     if target_df.shape != result_df.shape:\n",
        "#         feedback += \"Shape mismatch detected. Please check row filtering logic.\\n\"\n",
        "\n",
        "#     return current_prompt + feedback\n",
        "\n",
        "\n",
        "# # Enhanced save function that also uses the detailed logging\n",
        "# def save_edited_dataframe_enhanced_with_logging(edited_df, description):\n",
        "#     \"\"\"\n",
        "#     Enhanced version that uses the new comprehensive logging system\n",
        "#     for analyzing manual dataframe changes.\n",
        "#     \"\"\"\n",
        "#     global app_state, session_recorder\n",
        "\n",
        "#     if edited_df is None or edited_df.empty:\n",
        "#         return \"No data to save\", gr.update()\n",
        "\n",
        "#     try:\n",
        "#         # Convert the edited dataframe to proper pandas DataFrame if needed\n",
        "#         if not isinstance(edited_df, pd.DataFrame):\n",
        "#             edited_df = pd.DataFrame(edited_df)\n",
        "\n",
        "#         # Get the original dataframe for comparison\n",
        "#         original_df = app_state[\"df\"].copy()\n",
        "\n",
        "#         logger.info(\"Analyzing dataframe changes with enhanced GPT-4.1 logging...\")\n",
        "#         print(\"🤖 Analyzing changes with enhanced GPT-4.1 logging system...\")\n",
        "\n",
        "#         # Use the enhanced analysis function with comprehensive logging\n",
        "#         change_analysis = analyze_dataframe_changes_with_gpt4(\n",
        "#             original_df=original_df,\n",
        "#             modified_df=edited_df,\n",
        "#             user_description=description\n",
        "#         )\n",
        "\n",
        "#         # Generate a meaningful description if not provided\n",
        "#         if not description:\n",
        "#             description = change_analysis.get(\"change_summary\", \"Manual edits via data editor\")\n",
        "\n",
        "#         # Save as new version\n",
        "#         version_name = save_dataframe_version(edited_df, description)\n",
        "\n",
        "#         # Update the app state with the edited dataframe\n",
        "#         app_state[\"df\"] = edited_df\n",
        "\n",
        "#         # Record this in the copiloting session if active\n",
        "#         if session_recorder.current_session_file:\n",
        "#             session_description = change_analysis.get(\"session_description\", f\"Manual data edits: {description}\")\n",
        "\n",
        "#             # Create detailed session entry with log file reference\n",
        "#             session_entry = f\"\"\"\n",
        "# MANUAL DATA EDIT SESSION WITH ENHANCED LOGGING\n",
        "# ==============================================\n",
        "# Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "# Edit Description: {description}\n",
        "# Version Saved: {version_name}\n",
        "# Log File: {change_analysis.get('log_file', 'N/A')}\n",
        "\n",
        "# ENHANCED GPT-4.1 CHANGE ANALYSIS:\n",
        "# {'-' * 40}\n",
        "# Change Summary: {change_analysis.get('change_summary', 'N/A')}\n",
        "# Change Type: {change_analysis.get('change_type', 'N/A')}\n",
        "# Replication Success: {change_analysis.get('full_change_statistics', {}).get('replication_success', 'Unknown')}\n",
        "# Replication Attempts: {change_analysis.get('full_change_statistics', {}).get('replication_attempts', 'Unknown')}\n",
        "\n",
        "# Structural Changes:\n",
        "# {json.dumps(change_analysis.get('structural_changes', {}), indent=2)}\n",
        "\n",
        "# Data Modifications:\n",
        "# {json.dumps(change_analysis.get('data_modifications', {}), indent=2)}\n",
        "\n",
        "# Business Impact:\n",
        "# {json.dumps(change_analysis.get('business_impact', {}), indent=2)}\n",
        "\n",
        "# Recommendations:\n",
        "# {chr(10).join([f\"• {rec}\" for rec in change_analysis.get('recommendations', [])])}\n",
        "\n",
        "# Technical Statistics:\n",
        "# - Total Cells: {change_analysis.get('full_change_statistics', {}).get('total_cells', 'Unknown')}\n",
        "# - Changed Cells: {change_analysis.get('full_change_statistics', {}).get('total_changes', 'Unknown')}\n",
        "# - Change Density: {change_analysis.get('full_change_statistics', {}).get('change_density', 0)*100:.2f}%\n",
        "# - Affected Rows: {len(change_analysis.get('full_change_statistics', {}).get('affected_rows', []))}\n",
        "# - Affected Columns: {len(change_analysis.get('full_change_statistics', {}).get('affected_columns', []))}\n",
        "\n",
        "# Original DataFrame Shape: {original_df.shape}\n",
        "# Modified DataFrame Shape: {edited_df.shape}\n",
        "# {'-' * 80}\n",
        "# \"\"\"\n",
        "\n",
        "#             # Append to session file\n",
        "#             with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "#                 f.write(session_entry + \"\\n\")\n",
        "\n",
        "#             # Record in session data structure\n",
        "#             session_recorder.record_conversation_turn(\n",
        "#                 user_message=f\"MANUAL EDIT (Enhanced Logging): {description}\",\n",
        "#                 ai_response=session_description,\n",
        "#                 action_type=\"manual_data_edit_enhanced\",\n",
        "#                 code_executed=None,\n",
        "#                 version_saved=version_name\n",
        "#             )\n",
        "\n",
        "#             # Record the dataframe version change\n",
        "#             session_recorder.record_dataframe_version(\n",
        "#                 version_name=version_name,\n",
        "#                 description=description,\n",
        "#                 shape=list(edited_df.shape),\n",
        "#                 columns=list(edited_df.columns)\n",
        "#             )\n",
        "\n",
        "#             # Record any issues found by GPT-4\n",
        "#             replication_success = change_analysis.get('full_change_statistics', {}).get('replication_success', False)\n",
        "#             if not replication_success:\n",
        "#                 session_recorder.record_issue_found(\n",
        "#                     f\"Manual edit replication failed: {description}. Check detailed log for analysis.\",\n",
        "#                     severity=\"medium\"\n",
        "#                 )\n",
        "\n",
        "#             logger.info(\"Enhanced manual edit analysis recorded in copiloting session\")\n",
        "\n",
        "#         # Log the changes\n",
        "#         logger.info(f\"Saved edited dataframe as version {version_name}\")\n",
        "\n",
        "#         # Create detailed success message with log file information\n",
        "#         log_file_path = change_analysis.get('log_file', 'N/A')\n",
        "#         replication_success = change_analysis.get('full_change_statistics', {}).get('replication_success', False)\n",
        "#         replication_attempts = change_analysis.get('full_change_statistics', {}).get('replication_attempts', 0)\n",
        "\n",
        "#         success_message = f\"\"\"✅ Successfully saved as version {version_name}\n",
        "\n",
        "# 🤖 Enhanced GPT-4.1 Analysis Summary:\n",
        "# {change_analysis.get('change_summary', 'Changes analyzed')[:200]}...\n",
        "\n",
        "# 📊 Change Details:\n",
        "# • Change Type: {change_analysis.get('change_type', 'Unknown')}\n",
        "# • Original Shape: {original_df.shape}\n",
        "# • New Shape: {edited_df.shape}\n",
        "# • Replication Success: {'✅ Yes' if replication_success else '❌ Partial/Failed'}\n",
        "# • Replication Attempts: {replication_attempts}\n",
        "\n",
        "# 📝 Session Recording: {'✅ Recorded' if session_recorder.current_session_file else '❌ No active session'}\n",
        "\n",
        "# 📁 Detailed Analysis Log:\n",
        "# {log_file_path}\n",
        "\n",
        "# This log contains:\n",
        "# • Complete GPT-4.1 prompts and responses\n",
        "# • All Claude API calls and generated code\n",
        "# • Step-by-step replication attempts\n",
        "# • Cell-by-cell change analysis\n",
        "# • Business impact assessment\n",
        "# \"\"\"\n",
        "\n",
        "#         # Add recommendations if available\n",
        "#         if change_analysis.get('recommendations'):\n",
        "#             success_message += f\"\\n💡 Recommendations:\\n\"\n",
        "#             for rec in change_analysis['recommendations'][:3]:  # Show first 3\n",
        "#                 success_message += f\"• {rec}\\n\"\n",
        "\n",
        "#         # Add note about log file location\n",
        "#         success_message += f\"\\n📄 For complete debugging information, check: {log_file_path}\"\n",
        "\n",
        "#         return success_message, gr.update(value=edited_df)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         error_msg = f\"❌ Error saving: {str(e)}\"\n",
        "#         logger.error(f\"Error saving edited dataframe: {e}\")\n",
        "#         logger.error(traceback.format_exc())\n",
        "\n",
        "#         # Still try to record the error in session\n",
        "#         if session_recorder.current_session_file:\n",
        "#             session_recorder.record_conversation_turn(\n",
        "#                 user_message=f\"MANUAL EDIT FAILED (Enhanced): {description}\",\n",
        "#                 ai_response=error_msg,\n",
        "#                 action_type=\"manual_edit_error_enhanced\",\n",
        "#                 code_executed=None,\n",
        "#                 version_saved=None\n",
        "#             )\n",
        "\n",
        "#         return error_msg, gr.update()\n",
        "\n",
        "\n",
        "# # Function to view and summarize all log files\n",
        "# def get_manual_edit_logs_summary():\n",
        "#     \"\"\"\n",
        "#     Generate a summary of all manual edit analysis log files\n",
        "#     \"\"\"\n",
        "#     logs_dir = \"manual_edit_analysis_logs\"\n",
        "\n",
        "#     if not os.path.exists(logs_dir):\n",
        "#         return \"No manual edit analysis logs found yet.\"\n",
        "\n",
        "#     try:\n",
        "#         log_files = [f for f in os.listdir(logs_dir) if f.endswith('_detailed_analysis.txt')]\n",
        "\n",
        "#         if not log_files:\n",
        "#             return \"No detailed analysis log files found.\"\n",
        "\n",
        "#         # Sort by creation time (newest first)\n",
        "#         log_files.sort(key=lambda x: os.path.getctime(os.path.join(logs_dir, x)), reverse=True)\n",
        "\n",
        "#         summary = f\"\"\"📁 Manual Edit Analysis Logs Summary\n",
        "# Found {len(log_files)} detailed analysis log files:\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "#         for i, log_file in enumerate(log_files[:10], 1):  # Show last 10\n",
        "#             file_path = os.path.join(logs_dir, log_file)\n",
        "#             file_size = os.path.getsize(file_path)\n",
        "#             created_time = datetime.fromtimestamp(os.path.getctime(file_path))\n",
        "\n",
        "#             # Try to extract session info from filename\n",
        "#             session_id = log_file.replace('_detailed_analysis.txt', '')\n",
        "\n",
        "#             summary += f\"\"\"{i}. {session_id}\n",
        "#    📄 File: {log_file}\n",
        "#    📅 Created: {created_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "#    💾 Size: {file_size:,} bytes\n",
        "#    📁 Path: {file_path}\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "#         if len(log_files) > 10:\n",
        "#             summary += f\"... and {len(log_files) - 10} more log files\\n\"\n",
        "\n",
        "#         summary += f\"\"\"\n",
        "# 📋 Log File Contents Include:\n",
        "# • Complete GPT-4.1 prompts and responses\n",
        "# • All Claude API calls and code generation\n",
        "# • Step-by-step replication testing results\n",
        "# • Cell-by-cell dataframe comparison analysis\n",
        "# • Business impact and recommendation analysis\n",
        "# • Detailed error messages and debugging information\n",
        "\n",
        "# 📂 All logs are saved in: {logs_dir}/\n",
        "# \"\"\"\n",
        "\n",
        "#         return summary\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"Error reading log files: {str(e)}\"\n",
        "\n",
        "\n",
        "# # Function to read a specific log file\n",
        "# def read_manual_edit_log(session_id):\n",
        "#     \"\"\"\n",
        "#     Read and return the contents of a specific manual edit analysis log\n",
        "#     \"\"\"\n",
        "#     logs_dir = \"manual_edit_analysis_logs\"\n",
        "#     log_file_path = os.path.join(logs_dir, f\"{session_id}_detailed_analysis.txt\")\n",
        "\n",
        "#     if not os.path.exists(log_file_path):\n",
        "#         return f\"Log file not found: {log_file_path}\"\n",
        "\n",
        "#     try:\n",
        "#         with open(log_file_path, 'r', encoding='utf-8') as f:\n",
        "#             content = f.read()\n",
        "\n",
        "#         return f\"\"\"📄 Manual Edit Analysis Log: {session_id}\n",
        "# {'='*80}\n",
        "\n",
        "# {content}\n",
        "\n",
        "# {'='*80}\n",
        "# End of log file: {log_file_path}\n",
        "# \"\"\"\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return f\"Error reading log file: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boE57lJDwQrW"
      },
      "outputs": [],
      "source": [
        "def analyze_dataframe_changes_with_gpt4(original_df, modified_df, user_description=\"\"):\n",
        "    \"\"\"\n",
        "    Enhanced version with ALL CRITICAL FIXES APPLIED:\n",
        "    1. Fixed logging bug with proper string formatting\n",
        "    2. Focus on data type/formatting precision\n",
        "    3. Implement attempt-to-attempt learning\n",
        "    4. Add data type validation for exact target schema matching\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    import traceback\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create detailed logging directory\n",
        "    logs_dir = \"manual_edit_analysis_logs\"\n",
        "    os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "    # Generate unique log session ID\n",
        "    log_session_id = f\"manual_edit_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]}\"\n",
        "    log_file_path = os.path.join(logs_dir, f\"{log_session_id}_detailed_analysis.txt\")\n",
        "\n",
        "    # FIXED LOGGING FUNCTION - Proper string formatting\n",
        "    def log_to_file(content, section_title=\"\"):\n",
        "        try:\n",
        "            with open(log_file_path, 'a', encoding='utf-8') as f:\n",
        "                if section_title:\n",
        "                    f.write(f\"\\n{'='*80}\\n\")\n",
        "                    f.write(f\"{section_title}\\n\")\n",
        "                    f.write(f\"{'='*80}\\n\")\n",
        "                f.write(f\"{content}\\n\")\n",
        "                f.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR writing to log file: {e}\")\n",
        "\n",
        "    # Initialize comprehensive log\n",
        "    try:\n",
        "        log_to_file(f\"\"\"COMPREHENSIVE MANUAL EDIT ANALYSIS LOG - ENHANCED VERSION\n",
        "Session ID: {log_session_id}\n",
        "Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "User Description: \"{user_description}\"\n",
        "Log File Path: {log_file_path}\n",
        "\n",
        "ENHANCEMENTS APPLIED:\n",
        "1. ✅ Fixed logging string formatting bug\n",
        "2. ✅ Enhanced data type/formatting precision matching\n",
        "3. ✅ Implemented attempt-to-attempt learning system\n",
        "4. ✅ Added comprehensive data type validation\n",
        "\n",
        "ANALYSIS OVERVIEW:\n",
        "This log contains the complete workflow of analyzing manual dataframe edits:\n",
        "1. Original vs Modified dataframe comparison with data type analysis\n",
        "2. GPT-4.1 prompt generation with schema validation\n",
        "3. Claude API calls with learning feedback loops\n",
        "4. Code execution with precision validation\n",
        "5. Replication success/failure analysis with detailed diagnostics\n",
        "\n",
        "FUNCTION EXECUTION STATUS: STARTING...\n",
        "\"\"\", \"SESSION INITIALIZATION\")\n",
        "\n",
        "        print(f\"📝 Enhanced manual edit analysis log initialized: {log_file_path}\")\n",
        "\n",
        "    except Exception as init_error:\n",
        "        print(f\"CRITICAL: Could not initialize log file: {init_error}\")\n",
        "\n",
        "    try:\n",
        "        # ENHANCED DATAFRAME ANALYSIS with data type focus\n",
        "        log_to_file(f\"\"\"ORIGINAL DATAFRAME COMPLETE ANALYSIS:\n",
        "Shape: {original_df.shape}\n",
        "Columns: {list(original_df.columns)}\n",
        "Data Types: {dict(original_df.dtypes.astype(str))}\n",
        "Data Type Details: {json.dumps({col: str(dtype) for col, dtype in original_df.dtypes.items()}, indent=2)}\n",
        "Memory Usage: {original_df.memory_usage(deep=True).sum()} bytes\n",
        "Null Counts: {dict(original_df.isnull().sum())}\n",
        "Index Type: {type(original_df.index).__name__}\n",
        "Column Order: {list(original_df.columns)}\n",
        "\n",
        "FIRST 10 ROWS PREVIEW:\n",
        "{original_df.head(10).to_string()}\n",
        "\n",
        "COMPLETE ORIGINAL DATAFRAME (ALL ROWS):\n",
        "{original_df.to_string(max_rows=None, max_cols=None)}\n",
        "\n",
        "ORIGINAL DATAFRAME AS CSV:\n",
        "{original_df.to_csv(index=False)}\n",
        "\n",
        "DATA TYPE ANALYSIS:\n",
        "{_analyze_dataframe_schema(original_df, \"ORIGINAL\")}\n",
        "\"\"\", \"ORIGINAL DATAFRAME ANALYSIS\")\n",
        "\n",
        "        log_to_file(f\"\"\"MODIFIED DATAFRAME COMPLETE ANALYSIS:\n",
        "Shape: {modified_df.shape}\n",
        "Columns: {list(modified_df.columns)}\n",
        "Data Types: {dict(modified_df.dtypes.astype(str))}\n",
        "Data Type Details: {json.dumps({col: str(dtype) for col, dtype in modified_df.dtypes.items()}, indent=2)}\n",
        "Memory Usage: {modified_df.memory_usage(deep=True).sum()} bytes\n",
        "Null Counts: {dict(modified_df.isnull().sum())}\n",
        "Index Type: {type(modified_df.index).__name__}\n",
        "Column Order: {list(modified_df.columns)}\n",
        "\n",
        "FIRST 10 ROWS PREVIEW:\n",
        "{modified_df.head(10).to_string()}\n",
        "\n",
        "COMPLETE MODIFIED DATAFRAME (ALL ROWS):\n",
        "{modified_df.to_string(max_rows=None, max_cols=None)}\n",
        "\n",
        "MODIFIED DATAFRAME AS CSV:\n",
        "{modified_df.to_csv(index=False)}\n",
        "\n",
        "DATA TYPE ANALYSIS:\n",
        "{_analyze_dataframe_schema(modified_df, \"MODIFIED\")}\n",
        "\"\"\", \"MODIFIED DATAFRAME ANALYSIS\")\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        try:\n",
        "            client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "            log_to_file(\"✅ OpenAI client initialized successfully\", \"API CLIENT SETUP\")\n",
        "        except Exception as client_error:\n",
        "            log_to_file(f\"❌ Failed to initialize OpenAI client: {client_error}\", \"API CLIENT SETUP ERROR\")\n",
        "            raise client_error\n",
        "\n",
        "        # ENHANCED SCHEMA COMPARISON\n",
        "        schema_comparison = _compare_dataframe_schemas(original_df, modified_df)\n",
        "        log_to_file(f\"\"\"ENHANCED SCHEMA COMPARISON:\n",
        "{json.dumps(schema_comparison, indent=2)}\n",
        "\"\"\", \"SCHEMA COMPARISON ANALYSIS\")\n",
        "\n",
        "        # Prepare enhanced comparison data for GPT-4\n",
        "        original_info = {\n",
        "            \"shape\": original_df.shape,\n",
        "            \"columns\": list(original_df.columns),\n",
        "            \"dtypes\": dict(original_df.dtypes.astype(str)),\n",
        "            \"dtype_details\": {col: str(dtype) for col, dtype in original_df.dtypes.items()},\n",
        "            \"full_data\": original_df.to_string(max_rows=None, max_cols=None),\n",
        "            \"full_csv\": original_df.to_csv(index=False),\n",
        "            \"null_counts\": dict(original_df.isnull().sum()),\n",
        "            \"memory_usage\": original_df.memory_usage(deep=True).sum(),\n",
        "            \"schema_analysis\": _analyze_dataframe_schema(original_df, \"ORIGINAL\"),\n",
        "            \"summary_stats\": original_df.describe(include='all').to_string() if len(original_df) > 0 else \"No data\"\n",
        "        }\n",
        "\n",
        "        modified_info = {\n",
        "            \"shape\": modified_df.shape,\n",
        "            \"columns\": list(modified_df.columns),\n",
        "            \"dtypes\": dict(modified_df.dtypes.astype(str)),\n",
        "            \"dtype_details\": {col: str(dtype) for col, dtype in modified_df.dtypes.items()},\n",
        "            \"full_data\": modified_df.to_string(max_rows=None, max_cols=None),\n",
        "            \"full_csv\": modified_df.to_csv(index=False),\n",
        "            \"null_counts\": dict(modified_df.isnull().sum()),\n",
        "            \"memory_usage\": modified_df.memory_usage(deep=True).sum(),\n",
        "            \"schema_analysis\": _analyze_dataframe_schema(modified_df, \"MODIFIED\"),\n",
        "            \"summary_stats\": modified_df.describe(include='all').to_string() if len(modified_df) > 0 else \"No data\"\n",
        "        }\n",
        "\n",
        "        # Enhanced structural change detection\n",
        "        shape_changed = original_df.shape != modified_df.shape\n",
        "        columns_changed = set(original_df.columns) != set(modified_df.columns)\n",
        "        schema_changes = schema_comparison\n",
        "\n",
        "        log_to_file(f\"\"\"ENHANCED STRUCTURAL CHANGE ANALYSIS:\n",
        "Shape Changed: {shape_changed}\n",
        "- Original Shape: {original_df.shape}\n",
        "- Modified Shape: {modified_df.shape}\n",
        "- Rows Added: {max(0, modified_df.shape[0] - original_df.shape[0])}\n",
        "- Rows Removed: {max(0, original_df.shape[0] - modified_df.shape[0])}\n",
        "- Columns Added: {max(0, modified_df.shape[1] - original_df.shape[1])}\n",
        "- Columns Removed: {max(0, original_df.shape[1] - modified_df.shape[1])}\n",
        "\n",
        "Columns Changed: {columns_changed}\n",
        "- Original Columns: {list(original_df.columns)}\n",
        "- Modified Columns: {list(modified_df.columns)}\n",
        "- Added Columns: {list(set(modified_df.columns) - set(original_df.columns))}\n",
        "- Removed Columns: {list(set(original_df.columns) - set(modified_df.columns))}\n",
        "\n",
        "SCHEMA CHANGES DETECTED:\n",
        "{json.dumps(schema_changes, indent=2)}\n",
        "\"\"\", \"ENHANCED STRUCTURAL CHANGE ANALYSIS\")\n",
        "\n",
        "        # Enhanced cell-by-cell comparison with data type focus\n",
        "        data_changes_detected = False\n",
        "        changed_cells = []\n",
        "        total_changes = 0\n",
        "        changed_rows = set()\n",
        "        changed_columns = set()\n",
        "        data_type_mismatches = []\n",
        "\n",
        "        if original_df.shape == modified_df.shape and list(original_df.columns) == list(modified_df.columns):\n",
        "            print(f\"🔍 Enhanced comparison: {len(original_df)} rows × {len(original_df.columns)} columns...\")\n",
        "            log_to_file(f\"\"\"STARTING ENHANCED CELL-BY-CELL COMPARISON:\n",
        "Total cells to compare: {original_df.shape[0] * original_df.shape[1]}\n",
        "Comparing {len(original_df)} rows × {len(original_df.columns)} columns\n",
        "Enhanced features: Data type validation, formatting precision, null handling\n",
        "This may take time for large dataframes...\n",
        "\"\"\", \"ENHANCED CELL-BY-CELL COMPARISON START\")\n",
        "\n",
        "            # Enhanced cell comparison with data type awareness\n",
        "            for i in range(len(original_df)):\n",
        "                row_has_changes = False\n",
        "                row_changes = []\n",
        "\n",
        "                for col in original_df.columns:\n",
        "                    try:\n",
        "                        orig_val = original_df.iloc[i][col]\n",
        "                        mod_val = modified_df.iloc[i][col]\n",
        "\n",
        "                        # Enhanced comparison with data type awareness\n",
        "                        change_detected, change_detail = _enhanced_cell_comparison(\n",
        "                            orig_val, mod_val, i, col,\n",
        "                            original_df.dtypes[col], modified_df.dtypes[col]\n",
        "                        )\n",
        "\n",
        "                        if change_detected:\n",
        "                            data_changes_detected = True\n",
        "                            total_changes += 1\n",
        "                            row_has_changes = True\n",
        "                            changed_columns.add(col)\n",
        "                            row_changes.append(change_detail)\n",
        "                            changed_cells.append(change_detail)\n",
        "\n",
        "                            # Track data type mismatches\n",
        "                            if change_detail.get('data_type_changed'):\n",
        "                                data_type_mismatches.append(change_detail)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        log_to_file(f\"ERROR comparing cell at row {i}, column '{col}': {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if row_has_changes:\n",
        "                    changed_rows.add(i)\n",
        "\n",
        "                # Log progress every 100 rows\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    log_to_file(f\"Progress: Processed {i + 1}/{len(original_df)} rows, found {total_changes} changes so far\")\n",
        "\n",
        "            print(f\"✅ Enhanced comparison finished: {total_changes} changes, {len(data_type_mismatches)} data type mismatches\")\n",
        "\n",
        "            # Log comprehensive enhanced change analysis\n",
        "            log_to_file(f\"\"\"ENHANCED CELL-BY-CELL COMPARISON RESULTS:\n",
        "=====================================\n",
        "Total Changes Detected: {total_changes}\n",
        "Data Type Mismatches: {len(data_type_mismatches)}\n",
        "Affected Rows: {len(changed_rows)} out of {original_df.shape[0]} ({len(changed_rows)/original_df.shape[0]*100:.1f}%)\n",
        "Affected Columns: {len(changed_columns)} out of {len(original_df.columns)} ({len(changed_columns)/len(original_df.columns)*100:.1f}%)\n",
        "\n",
        "AFFECTED COLUMNS LIST:\n",
        "{list(changed_columns)}\n",
        "\n",
        "AFFECTED ROWS LIST:\n",
        "{sorted(list(changed_rows))[:50]}  # Show first 50 rows\n",
        "\n",
        "DATA TYPE MISMATCHES:\n",
        "{json.dumps(data_type_mismatches, indent=2) if data_type_mismatches else \"None detected\"}\n",
        "\n",
        "ALL DETECTED CHANGES ({len(changed_cells)} total):\n",
        "\"\"\", \"ENHANCED COMPREHENSIVE CHANGE DETECTION RESULTS\")\n",
        "\n",
        "            # Log ALL changes with enhanced details\n",
        "            for i, change in enumerate(changed_cells[:100]):  # Show first 100 changes\n",
        "                change_type = change.get('change_type', 'unknown')\n",
        "                data_type_info = f\" [DType: {change.get('original_dtype', 'unknown')} → {change.get('modified_dtype', 'unknown')}]\" if change.get('data_type_changed') else \"\"\n",
        "                log_to_file(f\"Change {i+1}: Row {change['row']}, Column '{change['column']}' ({change_type}){data_type_info}: '{change['original']}' → '{change['modified']}'\")\n",
        "\n",
        "        else:\n",
        "            log_to_file(f\"\"\"CANNOT PERFORM CELL-BY-CELL COMPARISON:\n",
        "Reason: Shape or column structure differs\n",
        "Original shape: {original_df.shape}\n",
        "Modified shape: {modified_df.shape}\n",
        "Original columns: {list(original_df.columns)}\n",
        "Modified columns: {list(modified_df.columns)}\n",
        "\"\"\", \"CELL-BY-CELL COMPARISON SKIPPED\")\n",
        "\n",
        "        # Enhanced change statistics\n",
        "        total_cells = original_df.shape[0] * original_df.shape[1] if original_df.size > 0 else 1\n",
        "        change_density = total_changes / total_cells\n",
        "\n",
        "        log_to_file(f\"\"\"ENHANCED COMPREHENSIVE STATISTICAL ANALYSIS:\n",
        "===============================\n",
        "Total Cells in Original: {total_cells}\n",
        "Total Cells Changed: {total_changes}\n",
        "Change Density: {change_density*100:.4f}%\n",
        "Data Type Mismatches: {len(data_type_mismatches)}\n",
        "Percentage of Rows Affected: {len(changed_rows)/original_df.shape[0]*100:.2f}% ({len(changed_rows)}/{original_df.shape[0]})\n",
        "Percentage of Columns Affected: {len(changed_columns)/len(original_df.columns)*100:.2f}% ({len(changed_columns)}/{len(original_df.columns)})\n",
        "\n",
        "ENHANCED CHANGE PATTERN ANALYSIS:\n",
        "- NaN Changes: {len([c for c in changed_cells if c.get('change_type') == 'nan_change'])}\n",
        "- Value Changes: {len([c for c in changed_cells if c.get('change_type') == 'value_change'])}\n",
        "- Data Type Changes: {len([c for c in changed_cells if c.get('data_type_changed')])}\n",
        "- Formatting Changes: {len([c for c in changed_cells if c.get('change_type') == 'formatting_change'])}\n",
        "- Most Affected Columns: {sorted(changed_columns)[:10]}\n",
        "- Row Change Distribution: Every {original_df.shape[0]//max(1,len(changed_rows)):.0f} rows on average\n",
        "\n",
        "SCHEMA COMPATIBILITY ANALYSIS:\n",
        "{json.dumps(schema_comparison, indent=2)}\n",
        "\"\"\", \"ENHANCED COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
        "\n",
        "        # STEP 1: Enhanced GPT-4.1 prompt generation with schema awareness\n",
        "        print(\"🧠 Enhanced GPT-4.1: Analyzing changes with schema validation...\")\n",
        "        log_to_file(\"🧠 STARTING ENHANCED GPT-4.1 ANALYSIS WITH SCHEMA VALIDATION...\", \"ENHANCED GPT-4.1 PROMPT GENERATION START\")\n",
        "\n",
        "        claude_prompt = _generate_enhanced_claude_prompt_with_gpt4_logged(\n",
        "            client, original_info, modified_info, user_description,\n",
        "            total_changes, changed_rows, changed_columns, changed_cells,\n",
        "            schema_comparison, data_type_mismatches, log_to_file\n",
        "        )\n",
        "\n",
        "        # STEP 2: Enhanced Claude prompt testing with learning system\n",
        "        print(\"🤖 Enhanced Claude testing with learning system...\")\n",
        "        log_to_file(\"🤖 STARTING ENHANCED CLAUDE REPLICATION WITH LEARNING SYSTEM...\", \"ENHANCED CLAUDE REPLICATION TESTING START\")\n",
        "\n",
        "        replication_results = _test_claude_prompt_replication_with_learning_logged(\n",
        "            original_df, modified_df, claude_prompt, schema_comparison, max_attempts=3, log_to_file=log_to_file\n",
        "        )\n",
        "\n",
        "        # STEP 3: Enhanced final analysis\n",
        "        log_to_file(\"📊 GENERATING ENHANCED FINAL ANALYSIS...\", \"ENHANCED FINAL ANALYSIS GENERATION START\")\n",
        "\n",
        "        final_analysis = _get_enhanced_final_analysis_with_prompts_logged(\n",
        "            client, original_info, modified_info, user_description,\n",
        "            total_changes, changed_rows, changed_columns, changed_cells,\n",
        "            schema_comparison, data_type_mismatches, claude_prompt, replication_results, log_to_file\n",
        "        )\n",
        "\n",
        "        # Enhanced metadata\n",
        "        final_analysis[\"raw_gpt_response\"] = final_analysis.get(\"raw_gpt_response\", \"\")\n",
        "        final_analysis[\"complete_comparison_performed\"] = True\n",
        "        final_analysis[\"enhanced_features_applied\"] = True\n",
        "        final_analysis[\"log_file_path\"] = log_file_path\n",
        "        final_analysis[\"full_change_statistics\"] = {\n",
        "            \"total_cells\": total_cells,\n",
        "            \"total_changes\": total_changes,\n",
        "            \"change_density\": change_density,\n",
        "            \"data_type_mismatches\": len(data_type_mismatches),\n",
        "            \"affected_rows\": list(changed_rows),\n",
        "            \"affected_columns\": list(changed_columns),\n",
        "            \"schema_changes\": schema_comparison,\n",
        "            \"replication_tested\": True,\n",
        "            \"replication_success\": replication_results[\"final_success\"],\n",
        "            \"replication_attempts\": len(replication_results[\"attempts\"]),\n",
        "            \"learning_applied\": True,\n",
        "            \"all_changes\": changed_cells\n",
        "        }\n",
        "\n",
        "        # FIXED: Proper string formatting in final log\n",
        "        best_attempt_score = replication_results.get(\"best_attempt\", {}).get(\"match_score\", 0)\n",
        "        best_score_formatted = f\"{best_attempt_score:.2%}\" if best_attempt_score > 0 else \"N/A\"\n",
        "\n",
        "        log_to_file(f\"\"\"ENHANCED ANALYSIS EXECUTION COMPLETED SUCCESSFULLY\n",
        "==========================================\n",
        "Total Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Log File Saved: {log_file_path}\n",
        "Log File Size: {os.path.getsize(log_file_path)} bytes\n",
        "\n",
        "ENHANCED FINAL RESULTS SUMMARY:\n",
        "- Replication Success: {replication_results[\"final_success\"]}\n",
        "- Best Match Score: {best_score_formatted}\n",
        "- Total Replication Attempts: {len(replication_results[\"attempts\"])}\n",
        "- Changes Detected: {total_changes}\n",
        "- Data Type Mismatches: {len(data_type_mismatches)}\n",
        "- Change Density: {change_density*100:.4f}%\n",
        "- Learning System Applied: ✅\n",
        "- Schema Validation Applied: ✅\n",
        "\n",
        "ENHANCEMENTS SUCCESSFULLY APPLIED:\n",
        "1. ✅ Fixed logging string formatting bug\n",
        "2. ✅ Enhanced data type/formatting precision matching\n",
        "3. ✅ Implemented attempt-to-attempt learning system\n",
        "4. ✅ Added comprehensive data type validation\n",
        "\n",
        "COMPLETE ENHANCED FINAL ANALYSIS OBJECT:\n",
        "{json.dumps(final_analysis, indent=2, default=str)}\n",
        "\n",
        "✅ ENHANCED ANALYSIS COMPLETE - ALL LOGS SAVED TO: {log_file_path}\n",
        "\"\"\", \"ENHANCED FINAL EXECUTION RESULTS\")\n",
        "\n",
        "        print(f\"📝 Enhanced analysis with all fixes applied - logs saved to: {log_file_path}\")\n",
        "        return final_analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        # FIXED: Proper error logging without string formatting issues\n",
        "        error_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        error_type = type(e).__name__\n",
        "        error_message = str(e)\n",
        "        stack_trace = traceback.format_exc()\n",
        "\n",
        "        original_shape = original_df.shape if 'original_df' in locals() else 'Unknown'\n",
        "        modified_shape = modified_df.shape if 'modified_df' in locals() else 'Unknown'\n",
        "\n",
        "        error_details = f\"\"\"CRITICAL ERROR DURING ENHANCED ANALYSIS EXECUTION\n",
        "========================================\n",
        "Error Time: {error_time}\n",
        "Error Type: {error_type}\n",
        "Error Message: {error_message}\n",
        "\n",
        "FULL STACK TRACE:\n",
        "{stack_trace}\n",
        "\n",
        "EXECUTION CONTEXT:\n",
        "- User Description: \"{user_description}\"\n",
        "- Original DF Shape: {original_shape}\n",
        "- Modified DF Shape: {modified_shape}\n",
        "- Log File: {log_file_path}\n",
        "\n",
        "❌ ENHANCED ANALYSIS FAILED - ERROR LOGGED TO: {log_file_path}\n",
        "\"\"\"\n",
        "\n",
        "        log_to_file(error_details, \"CRITICAL ERROR\")\n",
        "\n",
        "        error_msg = f\"Error in enhanced GPT-4 dataframe analysis: {e}\"\n",
        "        logger.error(error_msg)\n",
        "        print(f\"❌ Enhanced analysis failed but logs saved to: {log_file_path}\")\n",
        "\n",
        "        return {\n",
        "            \"change_summary\": f\"Enhanced dataframe analysis failed: {user_description}\",\n",
        "            \"change_type\": \"data_edit\",\n",
        "            \"session_description\": f\"User made changes to entire dataframe. Description: {user_description}. Error in enhanced analysis: {error_message}\",\n",
        "            \"error\": error_message,\n",
        "            \"enhanced_features_applied\": False,\n",
        "            \"complete_comparison_performed\": False,\n",
        "            \"log_file_path\": log_file_path,\n",
        "            \"error_logged\": True\n",
        "        }\n",
        "\n",
        "\n",
        "# ENHANCEMENT 1: Data Type Schema Analysis Functions\n",
        "def _analyze_dataframe_schema(df, label=\"\"):\n",
        "    \"\"\"\n",
        "    Analyze dataframe schema with detailed data type information\n",
        "    \"\"\"\n",
        "    schema_analysis = {\n",
        "        \"label\": label,\n",
        "        \"shape\": df.shape,\n",
        "        \"column_count\": len(df.columns),\n",
        "        \"row_count\": len(df),\n",
        "        \"columns\": {}\n",
        "    }\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_data = df[col]\n",
        "        schema_analysis[\"columns\"][col] = {\n",
        "            \"dtype\": str(col_data.dtype),\n",
        "            \"python_type\": str(type(col_data.dtype)),\n",
        "            \"null_count\": int(col_data.isnull().sum()),\n",
        "            \"null_percentage\": float(col_data.isnull().sum() / len(df) * 100) if len(df) > 0 else 0,\n",
        "            \"unique_count\": int(col_data.nunique()),\n",
        "            \"memory_usage\": int(col_data.memory_usage(deep=True)),\n",
        "            \"sample_values\": [str(val) for val in col_data.dropna().head(3).tolist()],\n",
        "            \"is_numeric\": pd.api.types.is_numeric_dtype(col_data),\n",
        "            \"is_datetime\": pd.api.types.is_datetime64_any_dtype(col_data),\n",
        "            \"is_categorical\": pd.api.types.is_categorical_dtype(col_data),\n",
        "            \"is_object\": pd.api.types.is_object_dtype(col_data)\n",
        "        }\n",
        "\n",
        "    return schema_analysis\n",
        "\n",
        "\n",
        "def _compare_dataframe_schemas(original_df, modified_df):\n",
        "    \"\"\"\n",
        "    Compare schemas between original and modified dataframes\n",
        "    \"\"\"\n",
        "    orig_schema = _analyze_dataframe_schema(original_df, \"ORIGINAL\")\n",
        "    mod_schema = _analyze_dataframe_schema(modified_df, \"MODIFIED\")\n",
        "\n",
        "    comparison = {\n",
        "        \"shape_changed\": orig_schema[\"shape\"] != mod_schema[\"shape\"],\n",
        "        \"column_count_changed\": orig_schema[\"column_count\"] != mod_schema[\"column_count\"],\n",
        "        \"row_count_changed\": orig_schema[\"row_count\"] != mod_schema[\"row_count\"],\n",
        "        \"column_changes\": {},\n",
        "        \"schema_compatibility_score\": 0.0,\n",
        "        \"critical_issues\": []\n",
        "    }\n",
        "\n",
        "    # Analyze column-by-column changes\n",
        "    all_columns = set(original_df.columns) | set(modified_df.columns)\n",
        "    compatible_columns = 0\n",
        "\n",
        "    for col in all_columns:\n",
        "        if col in original_df.columns and col in modified_df.columns:\n",
        "            orig_col = orig_schema[\"columns\"][col]\n",
        "            mod_col = mod_schema[\"columns\"][col]\n",
        "\n",
        "            column_change = {\n",
        "                \"exists_in_both\": True,\n",
        "                \"dtype_changed\": orig_col[\"dtype\"] != mod_col[\"dtype\"],\n",
        "                \"original_dtype\": orig_col[\"dtype\"],\n",
        "                \"modified_dtype\": mod_col[\"dtype\"],\n",
        "                \"null_count_changed\": orig_col[\"null_count\"] != mod_col[\"null_count\"],\n",
        "                \"type_compatibility\": _check_type_compatibility(orig_col[\"dtype\"], mod_col[\"dtype\"])\n",
        "            }\n",
        "\n",
        "            if column_change[\"type_compatibility\"]:\n",
        "                compatible_columns += 1\n",
        "            else:\n",
        "                comparison[\"critical_issues\"].append(f\"Column '{col}': Incompatible type change {orig_col['dtype']} → {mod_col['dtype']}\")\n",
        "\n",
        "        elif col in original_df.columns:\n",
        "            column_change = {\n",
        "                \"exists_in_both\": False,\n",
        "                \"removed\": True,\n",
        "                \"original_dtype\": orig_schema[\"columns\"][col][\"dtype\"]\n",
        "            }\n",
        "            comparison[\"critical_issues\"].append(f\"Column '{col}' was removed\")\n",
        "\n",
        "        else:\n",
        "            column_change = {\n",
        "                \"exists_in_both\": False,\n",
        "                \"added\": True,\n",
        "                \"modified_dtype\": mod_schema[\"columns\"][col][\"dtype\"]\n",
        "            }\n",
        "            comparison[\"critical_issues\"].append(f\"Column '{col}' was added\")\n",
        "\n",
        "        comparison[\"column_changes\"][col] = column_change\n",
        "\n",
        "    # Calculate compatibility score\n",
        "    total_columns = len(all_columns)\n",
        "    comparison[\"schema_compatibility_score\"] = compatible_columns / total_columns if total_columns > 0 else 1.0\n",
        "\n",
        "    return comparison\n",
        "\n",
        "\n",
        "def _check_type_compatibility(orig_dtype, mod_dtype):\n",
        "    \"\"\"\n",
        "    Check if two data types are compatible for replication\n",
        "    \"\"\"\n",
        "    # Exact match\n",
        "    if str(orig_dtype) == str(mod_dtype):\n",
        "        return True\n",
        "\n",
        "    # Compatible numeric types\n",
        "    numeric_types = ['int64', 'int32', 'float64', 'float32', 'number']\n",
        "    if any(t in str(orig_dtype).lower() for t in numeric_types) and any(t in str(mod_dtype).lower() for t in numeric_types):\n",
        "        return True\n",
        "\n",
        "    # Compatible string/object types\n",
        "    string_types = ['object', 'string', 'str']\n",
        "    if any(t in str(orig_dtype).lower() for t in string_types) and any(t in str(mod_dtype).lower() for t in string_types):\n",
        "        return True\n",
        "\n",
        "    # Compatible datetime types\n",
        "    datetime_types = ['datetime', 'timestamp']\n",
        "    if any(t in str(orig_dtype).lower() for t in datetime_types) and any(t in str(mod_dtype).lower() for t in datetime_types):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# ENHANCEMENT 2: Enhanced Cell Comparison with Data Type Awareness\n",
        "def _enhanced_cell_comparison(orig_val, mod_val, row_idx, col_name, orig_dtype, mod_dtype):\n",
        "    \"\"\"\n",
        "    Enhanced cell comparison with data type awareness and precision handling\n",
        "    \"\"\"\n",
        "    change_detail = {\n",
        "        \"row\": row_idx,\n",
        "        \"column\": col_name,\n",
        "        \"original\": str(orig_val),\n",
        "        \"modified\": str(mod_val),\n",
        "        \"original_dtype\": str(orig_dtype),\n",
        "        \"modified_dtype\": str(mod_dtype),\n",
        "        \"data_type_changed\": str(orig_dtype) != str(mod_dtype),\n",
        "        \"change_type\": \"no_change\"\n",
        "    }\n",
        "\n",
        "    # Handle NaN comparisons\n",
        "    if pd.isna(orig_val) and pd.isna(mod_val):\n",
        "        return False, change_detail\n",
        "    elif pd.isna(orig_val) or pd.isna(mod_val):\n",
        "        change_detail[\"change_type\"] = \"nan_change\"\n",
        "        return True, change_detail\n",
        "\n",
        "    # Data type change detection\n",
        "    if str(orig_dtype) != str(mod_dtype):\n",
        "        change_detail[\"change_type\"] = \"data_type_change\"\n",
        "        # Still check if values are equivalent despite type change\n",
        "        if _are_values_equivalent(orig_val, mod_val):\n",
        "            change_detail[\"change_type\"] = \"data_type_change_equivalent_value\"\n",
        "            return True, change_detail\n",
        "        else:\n",
        "            return True, change_detail\n",
        "\n",
        "    # Enhanced value comparison with precision handling\n",
        "    if _are_values_equivalent(orig_val, mod_val):\n",
        "        return False, change_detail\n",
        "\n",
        "    # Check for formatting differences\n",
        "    if _are_formatting_differences_only(orig_val, mod_val):\n",
        "        change_detail[\"change_type\"] = \"formatting_change\"\n",
        "        return True, change_detail\n",
        "\n",
        "    # Significant value change\n",
        "    change_detail[\"change_type\"] = \"value_change\"\n",
        "    return True, change_detail\n",
        "\n",
        "\n",
        "def _are_values_equivalent(val1, val2):\n",
        "    \"\"\"\n",
        "    Check if two values are equivalent despite potential formatting differences\n",
        "    \"\"\"\n",
        "    # Exact string match after stripping\n",
        "    if str(val1).strip() == str(val2).strip():\n",
        "        return True\n",
        "\n",
        "    # Numeric equivalence check\n",
        "    try:\n",
        "        # Handle currency and comma formatting\n",
        "        clean_val1 = str(val1).replace(',', '').replace('$', '').strip()\n",
        "        clean_val2 = str(val2).replace(',', '').replace('$', '').strip()\n",
        "\n",
        "        num1 = float(clean_val1)\n",
        "        num2 = float(clean_val2)\n",
        "\n",
        "        # Allow small floating point differences\n",
        "        return abs(num1 - num2) < 1e-10\n",
        "\n",
        "    except (ValueError, TypeError):\n",
        "        pass\n",
        "\n",
        "    # Date equivalence check\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        date1 = pd.to_datetime(val1)\n",
        "        date2 = pd.to_datetime(val2)\n",
        "        return date1 == date2\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def _are_formatting_differences_only(val1, val2):\n",
        "    \"\"\"\n",
        "    Check if differences are only formatting (whitespace, case, etc.)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Case-insensitive comparison\n",
        "        if str(val1).strip().lower() == str(val2).strip().lower():\n",
        "            return True\n",
        "\n",
        "        # Numeric formatting differences\n",
        "        clean_val1 = str(val1).replace(',', '').replace('$', '').replace(' ', '').strip()\n",
        "        clean_val2 = str(val2).replace(',', '').replace('$', '').replace(' ', '').strip()\n",
        "\n",
        "        if clean_val1 == clean_val2:\n",
        "            return True\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# ENHANCEMENT 3: Enhanced GPT-4.1 Prompt Generation with Schema Awareness\n",
        "def _generate_enhanced_claude_prompt_with_gpt4_logged(client, original_info, modified_info, user_description,\n",
        "                                                     total_changes, changed_rows, changed_columns, changed_cells,\n",
        "                                                     schema_comparison, data_type_mismatches, log_to_file):\n",
        "    \"\"\"\n",
        "    Enhanced GPT-4.1 prompt generation with comprehensive schema awareness and data type focus\n",
        "    \"\"\"\n",
        "    # Prepare detailed schema information for GPT-4.1\n",
        "    schema_issues = schema_comparison.get(\"critical_issues\", [])\n",
        "    compatibility_score = schema_comparison.get(\"schema_compatibility_score\", 0)\n",
        "\n",
        "    sample_changes = changed_cells[:15] if changed_cells else []\n",
        "    sample_type_mismatches = data_type_mismatches[:5] if data_type_mismatches else []\n",
        "\n",
        "    enhanced_prompt_request = f\"\"\"\n",
        "    You are an expert at analyzing dataframe changes and generating PRECISE prompts for Claude 3.7 to replicate manual edits with EXACT data type and formatting precision.\n",
        "\n",
        "    CRITICAL REQUIREMENTS:\n",
        "    1. Focus on DATA TYPE PRECISION - ensure exact dtype matching\n",
        "    2. Handle FORMATTING with precision (decimals, currency, dates)\n",
        "    3. Provide SPECIFIC transformation logic, not general filtering\n",
        "    4. Include EXPLICIT data type conversion instructions\n",
        "    5. Address schema compatibility issues\n",
        "\n",
        "    ORIGINAL DATAFRAME:\n",
        "    Shape: {original_info['shape']}\n",
        "    Columns: {original_info['columns']}\n",
        "    Data Types: {original_info['dtype_details']}\n",
        "\n",
        "    Data Sample:\n",
        "    {original_info['full_data'][:3000]}\n",
        "\n",
        "    MODIFIED DATAFRAME:\n",
        "    Shape: {modified_info['shape']}\n",
        "    Columns: {modified_info['columns']}\n",
        "    Data Types: {modified_info['dtype_details']}\n",
        "\n",
        "    Data Sample:\n",
        "    {modified_info['full_data'][:3000]}\n",
        "\n",
        "    ENHANCED CHANGE ANALYSIS:\n",
        "    - Total changes: {total_changes}\n",
        "    - Schema compatibility score: {compatibility_score:.2f}\n",
        "    - Data type mismatches: {len(data_type_mismatches)}\n",
        "    - Critical schema issues: {schema_issues}\n",
        "    - Changed rows: {list(changed_rows)[:20] if changed_rows else []}\n",
        "    - Changed columns: {list(changed_columns)}\n",
        "\n",
        "    SAMPLE DETAILED CHANGES:\n",
        "    {json.dumps(sample_changes, indent=2)}\n",
        "\n",
        "    DATA TYPE MISMATCHES DETECTED:\n",
        "    {json.dumps(sample_type_mismatches, indent=2)}\n",
        "\n",
        "    USER DESCRIPTION: \"{user_description}\"\n",
        "\n",
        "    Generate a PRECISE prompt for Claude 3.7 that will replicate these exact changes with:\n",
        "\n",
        "    1. EXACT DATA TYPE SPECIFICATIONS:\n",
        "       - Include explicit dtype conversion commands\n",
        "       - Specify decimal precision for floats\n",
        "       - Handle string formatting requirements\n",
        "\n",
        "    2. SPECIFIC TRANSFORMATION LOGIC:\n",
        "       - Exact cell selection criteria\n",
        "       - Precise value transformation formulas\n",
        "       - Row-by-row or column-by-column instructions if needed\n",
        "\n",
        "    3. SCHEMA VALIDATION:\n",
        "       - Ensure output matches target schema exactly\n",
        "       - Include data type verification steps\n",
        "\n",
        "    4. BUSINESS CONTEXT for rent roll data:\n",
        "       - Consider tenant information updates\n",
        "       - Handle rent calculations precisely\n",
        "       - Manage occupancy status changes\n",
        "\n",
        "    The prompt should be executable Python pandas code that produces the EXACT modified dataframe.\n",
        "\n",
        "    Return ONLY the Claude prompt text that focuses on precision and data type accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Log the enhanced GPT-4.1 prompt\n",
        "    log_to_file(f\"\"\"ENHANCED GPT-4.1 PROMPT TO GENERATE CLAUDE INSTRUCTIONS:\n",
        "Model: gpt-4.1\n",
        "Temperature: 0.05 (Lower for precision)\n",
        "Max Tokens: 4000\n",
        "\n",
        "ENHANCED PROMPT FEATURES:\n",
        "- Schema compatibility analysis included\n",
        "- Data type mismatch focus\n",
        "- Formatting precision requirements\n",
        "- Sample change details provided\n",
        "\n",
        "COMPLETE ENHANCED PROMPT SENT TO GPT-4.1:\n",
        "{'-'*60}\n",
        "{enhanced_prompt_request}\n",
        "{'-'*60}\n",
        "\"\"\", \"ENHANCED GPT-4.1 REQUEST\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert at generating precise data transformation prompts with exact data type and formatting specifications. Focus on precision and schema accuracy.\"},\n",
        "                {\"role\": \"user\", \"content\": enhanced_prompt_request}\n",
        "            ],\n",
        "            max_tokens=4000,\n",
        "            temperature=0.05  # Lower temperature for more precision\n",
        "        )\n",
        "\n",
        "        enhanced_gpt_response = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Log enhanced GPT-4.1 response\n",
        "        log_to_file(f\"\"\"ENHANCED GPT-4.1 RESPONSE (CLAUDE PROMPT):\n",
        "Response Length: {len(enhanced_gpt_response)} characters\n",
        "Tokens Used: Approximately {len(enhanced_gpt_response.split())} words\n",
        "Schema Focus: ✅ Applied\n",
        "Data Type Precision: ✅ Applied\n",
        "\n",
        "GENERATED ENHANCED CLAUDE PROMPT:\n",
        "{'-'*60}\n",
        "{enhanced_gpt_response}\n",
        "{'-'*60}\n",
        "\"\"\", \"ENHANCED GPT-4.1 RESPONSE\")\n",
        "\n",
        "        return enhanced_gpt_response\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Enhanced GPT-4.1 API Error: {str(e)}\"\n",
        "        log_to_file(f\"\"\"ENHANCED GPT-4.1 API CALL FAILED:\n",
        "Error: {error_msg}\n",
        "Fallback: Using enhanced basic prompt with schema awareness\n",
        "\"\"\")\n",
        "\n",
        "        # Enhanced fallback prompt with schema awareness\n",
        "        fallback_prompt = f\"\"\"Replicate the manual edits described as: {user_description}\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Maintain exact data types: {modified_info['dtype_details']}\n",
        "- Total changes to make: {total_changes}\n",
        "- Focus on columns: {list(changed_columns)}\n",
        "- Ensure precise formatting and data type conversion\n",
        "- Output must match target schema exactly\n",
        "\n",
        "Sample changes for reference:\n",
        "{json.dumps(sample_changes[:5], indent=2)}\n",
        "\"\"\"\n",
        "        return fallback_prompt\n",
        "\n",
        "\n",
        "# ENHANCEMENT 4: Claude Testing with Learning System\n",
        "def _test_claude_prompt_replication_with_learning_logged(original_df, target_df, claude_prompt, schema_comparison, max_attempts=3, log_to_file=None):\n",
        "    \"\"\"\n",
        "    Enhanced Claude testing with attempt-to-attempt learning system\n",
        "    \"\"\"\n",
        "    attempts = []\n",
        "    current_prompt = claude_prompt\n",
        "    learning_history = []\n",
        "\n",
        "    log_to_file(f\"\"\"ENHANCED CLAUDE REPLICATION TESTING WITH LEARNING STARTED:\n",
        "Maximum Attempts: {max_attempts}\n",
        "Target DataFrame Shape: {target_df.shape}\n",
        "Original DataFrame Shape: {original_df.shape}\n",
        "Schema Compatibility Score: {schema_comparison.get('schema_compatibility_score', 0):.2f}\n",
        "Critical Schema Issues: {len(schema_comparison.get('critical_issues', []))}\n",
        "\n",
        "LEARNING SYSTEM FEATURES:\n",
        "✅ Attempt-to-attempt feedback loop\n",
        "✅ Schema validation focus\n",
        "✅ Data type precision tracking\n",
        "✅ Progressive prompt refinement\n",
        "\n",
        "INITIAL ENHANCED CLAUDE PROMPT TO TEST:\n",
        "{'-'*60}\n",
        "{claude_prompt}\n",
        "{'-'*60}\n",
        "\"\"\", \"ENHANCED REPLICATION TESTING INITIALIZATION\")\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        print(f\"🔄 Enhanced Attempt {attempt + 1}/{max_attempts} with learning system\")\n",
        "        log_to_file(f\"Starting enhanced attempt {attempt + 1}/{max_attempts} with accumulated learning...\", f\"ENHANCED ATTEMPT {attempt + 1}\")\n",
        "\n",
        "        try:\n",
        "            # Apply learning from previous attempts\n",
        "            if attempt > 0:\n",
        "                current_prompt = _apply_learning_to_prompt(current_prompt, learning_history, target_df, log_to_file)\n",
        "\n",
        "            # Call Claude with enhanced logging\n",
        "            claude_response = _call_enhanced_claude_logged(current_prompt, original_df, target_df, schema_comparison, log_to_file, attempt + 1)\n",
        "\n",
        "            # Enhanced code extraction\n",
        "            code_blocks = _extract_enhanced_code_blocks(claude_response)\n",
        "\n",
        "            log_to_file(f\"\"\"ENHANCED CODE EXTRACTION RESULTS:\n",
        "Found {len(code_blocks)} code blocks\n",
        "Enhanced Features: Schema validation, data type checking\n",
        "\n",
        "Code Blocks Extracted:\n",
        "\"\"\")\n",
        "\n",
        "            for i, code in enumerate(code_blocks):\n",
        "                log_to_file(f\"Enhanced Code Block {i+1}:\\n```python\\n{code}\\n```\\n\")\n",
        "\n",
        "            if not code_blocks:\n",
        "                attempt_result = {\n",
        "                    \"attempt\": attempt + 1,\n",
        "                    \"success\": False,\n",
        "                    \"error\": \"No code found in Claude response\",\n",
        "                    \"prompt_used\": current_prompt,\n",
        "                    \"learning_applied\": len(learning_history) > 0\n",
        "                }\n",
        "                attempts.append(attempt_result)\n",
        "                learning_history.append({\n",
        "                    \"attempt\": attempt + 1,\n",
        "                    \"issue\": \"no_code_generated\",\n",
        "                    \"resolution\": \"request_explicit_code_blocks\"\n",
        "                })\n",
        "                log_to_file(\"No code blocks found - adding to learning history\")\n",
        "                continue\n",
        "\n",
        "            # Enhanced code execution with schema validation\n",
        "            test_df = original_df.copy()\n",
        "            exec_globals = {\n",
        "                \"df\": test_df,\n",
        "                \"pd\": pd,\n",
        "                \"np\": np,\n",
        "                \"os\": os,\n",
        "                \"datetime\": datetime\n",
        "            }\n",
        "\n",
        "            log_to_file(\"Starting enhanced code execution with schema validation...\")\n",
        "            execution_output = \"\"\n",
        "            execution_success = True\n",
        "\n",
        "            for i, code in enumerate(code_blocks):\n",
        "                try:\n",
        "                    log_to_file(f\"Executing enhanced code block {i+1}...\")\n",
        "                    exec(code, exec_globals)\n",
        "                    execution_output += f\"Enhanced code block {i+1} executed successfully\\n\"\n",
        "                except Exception as exec_error:\n",
        "                    execution_output += f\"Enhanced code block {i+1} failed: {str(exec_error)}\\n\"\n",
        "                    log_to_file(f\"Enhanced code block {i+1} execution error: {str(exec_error)}\")\n",
        "                    execution_success = False\n",
        "                    learning_history.append({\n",
        "                        \"attempt\": attempt + 1,\n",
        "                        \"issue\": f\"execution_error_{i+1}\",\n",
        "                        \"error\": str(exec_error),\n",
        "                        \"resolution\": \"fix_syntax_and_logic\"\n",
        "                    })\n",
        "\n",
        "            result_df = exec_globals[\"df\"]\n",
        "            log_to_file(f\"Enhanced execution completed. Result DataFrame shape: {result_df.shape}\")\n",
        "\n",
        "            # Enhanced validation with schema checking\n",
        "            enhanced_validation = _enhanced_result_validation(target_df, result_df, schema_comparison)\n",
        "            match_score = enhanced_validation[\"overall_score\"]\n",
        "            schema_match = enhanced_validation[\"schema_match\"]\n",
        "            data_type_match = enhanced_validation[\"data_type_match\"]\n",
        "\n",
        "            attempt_result = {\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"success\": match_score >= 0.95 and schema_match,\n",
        "                \"match_score\": match_score,\n",
        "                \"schema_match\": schema_match,\n",
        "                \"data_type_match\": data_type_match,\n",
        "                \"enhanced_validation\": enhanced_validation,\n",
        "                \"generated_code\": code_blocks,\n",
        "                \"prompt_used\": current_prompt,\n",
        "                \"result_shape\": result_df.shape,\n",
        "                \"target_shape\": target_df.shape,\n",
        "                \"execution_output\": execution_output,\n",
        "                \"execution_success\": execution_success,\n",
        "                \"learning_applied\": len(learning_history) > 0\n",
        "            }\n",
        "\n",
        "            attempts.append(attempt_result)\n",
        "\n",
        "            log_to_file(f\"\"\"ENHANCED ATTEMPT {attempt + 1} RESULTS:\n",
        "Overall Match Score: {match_score:.2%}\n",
        "Schema Match: {'✅ PASSED' if schema_match else '❌ FAILED'}\n",
        "Data Type Match: {'✅ PASSED' if data_type_match else '❌ FAILED'}\n",
        "Success Threshold (95% + Schema): {'✅ PASSED' if (match_score >= 0.95 and schema_match) else '❌ FAILED'}\n",
        "Result Shape: {result_df.shape}\n",
        "Target Shape: {target_df.shape}\n",
        "Execution Success: {'✅ YES' if execution_success else '❌ NO'}\n",
        "Learning Applied: {'✅ YES' if len(learning_history) > 0 else '❌ NO'}\n",
        "\n",
        "ENHANCED VALIDATION DETAILS:\n",
        "{json.dumps(enhanced_validation, indent=2)}\n",
        "\n",
        "Execution Output:\n",
        "{execution_output}\n",
        "\"\"\")\n",
        "\n",
        "            print(f\"📊 Enhanced Match Score: {match_score:.2%}, Schema: {'✅' if schema_match else '❌'}\")\n",
        "\n",
        "            # Add learning insights for next attempt\n",
        "            if match_score < 0.95 or not schema_match:\n",
        "                learning_insights = _analyze_failure_for_learning(target_df, result_df, enhanced_validation)\n",
        "                learning_history.extend(learning_insights)\n",
        "                log_to_file(f\"Learning insights added: {json.dumps(learning_insights, indent=2)}\")\n",
        "\n",
        "            if match_score >= 0.95 and schema_match:\n",
        "                print(\"✅ Enhanced replication successful!\")\n",
        "                log_to_file(\"🎉 ENHANCED REPLICATION SUCCESSFUL! Stopping attempts.\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            attempt_result = {\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"success\": False,\n",
        "                \"error\": error_msg,\n",
        "                \"prompt_used\": current_prompt,\n",
        "                \"learning_applied\": len(learning_history) > 0\n",
        "            }\n",
        "            attempts.append(attempt_result)\n",
        "            learning_history.append({\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"issue\": \"execution_exception\",\n",
        "                \"error\": error_msg,\n",
        "                \"resolution\": \"improve_error_handling\"\n",
        "            })\n",
        "            log_to_file(f\"ENHANCED ATTEMPT {attempt + 1} FAILED with error: {error_msg}\")\n",
        "            print(f\"❌ Enhanced Error: {error_msg}\")\n",
        "\n",
        "    final_success = any(attempt[\"success\"] for attempt in attempts)\n",
        "    best_attempt = max(attempts, key=lambda x: x.get(\"match_score\", 0)) if attempts else None\n",
        "\n",
        "    log_to_file(f\"\"\"ENHANCED REPLICATION TESTING COMPLETE:\n",
        "Final Success: {final_success}\n",
        "Best Match Score: {best_attempt.get('match_score', 0):.2% if best_attempt else 'N/A'}\n",
        "Best Schema Match: {best_attempt.get('schema_match', False) if best_attempt else False}\n",
        "Total Attempts: {len(attempts)}\n",
        "Total Learning Insights: {len(learning_history)}\n",
        "\n",
        "LEARNING HISTORY SUMMARY:\n",
        "{json.dumps(learning_history, indent=2)}\n",
        "\n",
        "ALL ENHANCED ATTEMPTS SUMMARY:\n",
        "\"\"\", \"ENHANCED REPLICATION TESTING RESULTS\")\n",
        "\n",
        "    for i, attempt in enumerate(attempts, 1):\n",
        "        success = attempt.get('success', False)\n",
        "        score = attempt.get('match_score', 0)\n",
        "        schema_match = attempt.get('schema_match', False)\n",
        "        learning = attempt.get('learning_applied', False)\n",
        "        error = attempt.get('error', 'None')\n",
        "\n",
        "        log_to_file(f\"Enhanced Attempt {i}: Success={success}, Score={score:.2%}, Schema={schema_match}, Learning={learning}, Error={error}\")\n",
        "\n",
        "    return {\n",
        "        \"attempts\": attempts,\n",
        "        \"final_success\": final_success,\n",
        "        \"best_attempt\": best_attempt,\n",
        "        \"final_prompt\": best_attempt[\"prompt_used\"] if best_attempt else claude_prompt,\n",
        "        \"learning_history\": learning_history,\n",
        "        \"learning_applied\": len(learning_history) > 0\n",
        "    }\n",
        "\n",
        "\n",
        "# Learning System Helper Functions\n",
        "def _apply_learning_to_prompt(base_prompt, learning_history, target_df, log_to_file):\n",
        "    \"\"\"\n",
        "    Apply accumulated learning to improve the prompt\n",
        "    \"\"\"\n",
        "    if not learning_history:\n",
        "        return base_prompt\n",
        "\n",
        "    # Analyze learning patterns\n",
        "    common_issues = {}\n",
        "    for learning in learning_history:\n",
        "        issue = learning.get(\"issue\", \"unknown\")\n",
        "        if issue in common_issues:\n",
        "            common_issues[issue] += 1\n",
        "        else:\n",
        "            common_issues[issue] = 1\n",
        "\n",
        "    # Generate improvement suggestions based on learning\n",
        "    improvements = []\n",
        "\n",
        "    if \"execution_error\" in [l.get(\"issue\", \"\") for l in learning_history]:\n",
        "        improvements.append(\"Focus on syntactically correct pandas operations\")\n",
        "\n",
        "    if \"schema_mismatch\" in [l.get(\"issue\", \"\") for l in learning_history]:\n",
        "        improvements.append(f\"Ensure exact data types: {dict(target_df.dtypes)}\")\n",
        "\n",
        "    if \"data_type_mismatch\" in [l.get(\"issue\", \"\") for l in learning_history]:\n",
        "        improvements.append(\"Include explicit dtype conversions using .astype()\")\n",
        "\n",
        "    if \"no_code_generated\" in [l.get(\"issue\", \"\") for l in learning_history]:\n",
        "        improvements.append(\"Generate code in ```python ``` blocks\")\n",
        "\n",
        "    # Apply improvements to prompt\n",
        "    if improvements:\n",
        "        learning_section = f\"\"\"\n",
        "\n",
        "LEARNING FROM PREVIOUS ATTEMPTS:\n",
        "Based on previous attempts, please pay special attention to:\n",
        "{chr(10).join([f\"• {improvement}\" for improvement in improvements])}\n",
        "\n",
        "COMMON ISSUES TO AVOID:\n",
        "{chr(10).join([f\"• {issue}: occurred {count} time(s)\" for issue, count in common_issues.items()])}\n",
        "\"\"\"\n",
        "        improved_prompt = base_prompt + learning_section\n",
        "\n",
        "        log_to_file(f\"\"\"LEARNING APPLIED TO PROMPT:\n",
        "Improvements Added: {len(improvements)}\n",
        "Common Issues Addressed: {len(common_issues)}\n",
        "\n",
        "LEARNING SECTION ADDED:\n",
        "{learning_section}\n",
        "\"\"\")\n",
        "\n",
        "        return improved_prompt\n",
        "\n",
        "    return base_prompt\n",
        "\n",
        "\n",
        "def _analyze_failure_for_learning(target_df, result_df, enhanced_validation):\n",
        "    \"\"\"\n",
        "    Analyze failure to extract learning insights for next attempt\n",
        "    \"\"\"\n",
        "    insights = []\n",
        "\n",
        "    # Schema issues\n",
        "    if not enhanced_validation.get(\"schema_match\", True):\n",
        "        if enhanced_validation.get(\"shape_mismatch\", False):\n",
        "            insights.append({\n",
        "                \"issue\": \"shape_mismatch\",\n",
        "                \"details\": f\"Target: {target_df.shape}, Got: {result_df.shape}\",\n",
        "                \"resolution\": \"check_row_filtering_and_column_selection\"\n",
        "            })\n",
        "\n",
        "        if enhanced_validation.get(\"column_mismatch\", False):\n",
        "            insights.append({\n",
        "                \"issue\": \"column_mismatch\",\n",
        "                \"details\": f\"Target cols: {list(target_df.columns)}, Got cols: {list(result_df.columns)}\",\n",
        "                \"resolution\": \"ensure_exact_column_names_and_order\"\n",
        "            })\n",
        "\n",
        "    # Data type issues\n",
        "    if not enhanced_validation.get(\"data_type_match\", True):\n",
        "        type_mismatches = enhanced_validation.get(\"data_type_details\", {})\n",
        "        for col, details in type_mismatches.items():\n",
        "            if details.get(\"mismatch\", False):\n",
        "                insights.append({\n",
        "                    \"issue\": \"data_type_mismatch\",\n",
        "                    \"column\": col,\n",
        "                    \"expected\": details.get(\"target_dtype\"),\n",
        "                    \"got\": details.get(\"result_dtype\"),\n",
        "                    \"resolution\": f\"add_explicit_conversion: df['{col}'] = df['{col}'].astype('{details.get('target_dtype')}')\"\n",
        "                })\n",
        "\n",
        "    # Content issues\n",
        "    content_score = enhanced_validation.get(\"content_score\", 1.0)\n",
        "    if content_score < 0.9:\n",
        "        insights.append({\n",
        "            \"issue\": \"content_mismatch\",\n",
        "            \"score\": content_score,\n",
        "            \"resolution\": \"review_transformation_logic_for_cell_values\"\n",
        "        })\n",
        "\n",
        "    return insights\n",
        "\n",
        "\n",
        "# Enhanced Result Validation\n",
        "def _enhanced_result_validation(target_df, result_df, schema_comparison):\n",
        "    \"\"\"\n",
        "    Enhanced validation with comprehensive schema and data type checking\n",
        "    \"\"\"\n",
        "    validation = {\n",
        "        \"overall_score\": 0.0,\n",
        "        \"schema_match\": False,\n",
        "        \"data_type_match\": False,\n",
        "        \"content_score\": 0.0,\n",
        "        \"shape_mismatch\": False,\n",
        "        \"column_mismatch\": False,\n",
        "        \"data_type_details\": {}\n",
        "    }\n",
        "\n",
        "    # Shape validation\n",
        "    if target_df.shape != result_df.shape:\n",
        "        validation[\"shape_mismatch\"] = True\n",
        "        validation[\"overall_score\"] = 0.0\n",
        "        return validation\n",
        "\n",
        "    # Column validation\n",
        "    if list(target_df.columns) != list(result_df.columns):\n",
        "        validation[\"column_mismatch\"] = True\n",
        "        validation[\"overall_score\"] = 0.1\n",
        "        return validation\n",
        "\n",
        "    validation[\"schema_match\"] = True\n",
        "\n",
        "    # Data type validation\n",
        "    type_match_count = 0\n",
        "    total_columns = len(target_df.columns)\n",
        "\n",
        "    for col in target_df.columns:\n",
        "        target_dtype = str(target_df[col].dtype)\n",
        "        result_dtype = str(result_df[col].dtype)\n",
        "\n",
        "        type_compatible = _check_type_compatibility(target_dtype, result_dtype)\n",
        "\n",
        "        validation[\"data_type_details\"][col] = {\n",
        "            \"target_dtype\": target_dtype,\n",
        "            \"result_dtype\": result_dtype,\n",
        "            \"exact_match\": target_dtype == result_dtype,\n",
        "            \"compatible\": type_compatible,\n",
        "            \"mismatch\": not type_compatible\n",
        "        }\n",
        "\n",
        "        if type_compatible:\n",
        "            type_match_count += 1\n",
        "\n",
        "    validation[\"data_type_match\"] = type_match_count == total_columns\n",
        "\n",
        "    # Content validation using the enhanced comparison\n",
        "    if validation[\"schema_match\"] and len(target_df) > 0:\n",
        "        validation[\"content_score\"] = _calculate_enhanced_match_score(target_df, result_df)\n",
        "    else:\n",
        "        validation[\"content_score\"] = 0.0\n",
        "\n",
        "    # Overall score calculation\n",
        "    schema_weight = 0.3\n",
        "    dtype_weight = 0.3\n",
        "    content_weight = 0.4\n",
        "\n",
        "    schema_score = 1.0 if validation[\"schema_match\"] else 0.0\n",
        "    dtype_score = type_match_count / total_columns if total_columns > 0 else 1.0\n",
        "\n",
        "    validation[\"overall_score\"] = (\n",
        "        schema_weight * schema_score +\n",
        "        dtype_weight * dtype_score +\n",
        "        content_weight * validation[\"content_score\"]\n",
        "    )\n",
        "\n",
        "    return validation\n",
        "\n",
        "\n",
        "def _calculate_enhanced_match_score(target_df, result_df):\n",
        "    \"\"\"\n",
        "    Enhanced match score calculation with improved precision\n",
        "    \"\"\"\n",
        "    if target_df.empty and result_df.empty:\n",
        "        return 1.0\n",
        "    if target_df.empty or result_df.empty:\n",
        "        return 0.0\n",
        "    if target_df.shape != result_df.shape:\n",
        "        return 0.0\n",
        "    if list(target_df.columns) != list(result_df.columns):\n",
        "        return 0.0\n",
        "\n",
        "    total_cells = 0\n",
        "    matching_cells = 0\n",
        "\n",
        "    for i in range(len(target_df)):\n",
        "        for col in target_df.columns:\n",
        "            total_cells += 1\n",
        "\n",
        "            try:\n",
        "                target_val = target_df.iloc[i][col]\n",
        "                result_val = result_df.iloc[i][col]\n",
        "\n",
        "                if _are_values_equivalent(target_val, result_val):\n",
        "                    matching_cells += 1\n",
        "\n",
        "            except (IndexError, KeyError):\n",
        "                continue\n",
        "\n",
        "    return matching_cells / total_cells if total_cells > 0 else 0.0\n",
        "\n",
        "\n",
        "# Enhanced Claude API Call\n",
        "def _call_enhanced_claude_logged(prompt, original_df, target_df, schema_comparison, log_to_file, attempt_number):\n",
        "    \"\"\"\n",
        "    Enhanced Claude API call with schema context and improved prompting\n",
        "    \"\"\"\n",
        "    target_schema_info = {\n",
        "        \"shape\": target_df.shape,\n",
        "        \"dtypes\": dict(target_df.dtypes),\n",
        "        \"columns\": list(target_df.columns),\n",
        "        \"sample_data\": target_df.head(3).to_string()\n",
        "    }\n",
        "\n",
        "    log_to_file(f\"\"\"CALLING ENHANCED CLAUDE API - ATTEMPT {attempt_number}:\n",
        "Model: claude-sonnet-4-20250514\n",
        "Temperature: 0.1 (Optimized for precision)\n",
        "Max Tokens: 3000\n",
        "\n",
        "ENHANCED FEATURES:\n",
        "✅ Target schema context provided\n",
        "✅ Data type precision focus\n",
        "✅ Schema compatibility warnings included\n",
        "\n",
        "PROMPT BEING SENT TO CLAUDE:\n",
        "{'-'*60}\n",
        "{prompt}\n",
        "{'-'*60}\n",
        "\n",
        "ENHANCED CONTEXT PROVIDED:\n",
        "Original DataFrame Shape: {original_df.shape}\n",
        "Target DataFrame Shape: {target_df.shape}\n",
        "Schema Compatibility Score: {schema_comparison.get('schema_compatibility_score', 0):.2f}\n",
        "Critical Issues: {len(schema_comparison.get('critical_issues', []))}\n",
        "\n",
        "TARGET SCHEMA REQUIREMENTS:\n",
        "{json.dumps(target_schema_info, indent=2, default=str)}\n",
        "\"\"\", f\"ENHANCED CLAUDE API CALL {attempt_number}\")\n",
        "\n",
        "    try:\n",
        "        # Get Anthropic client\n",
        "        anthropic_client = Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "        # Enhanced dataframe context with target schema\n",
        "        enhanced_df_summary = f\"\"\"\n",
        "        CURRENT DATAFRAME CONTENT:\n",
        "        {original_df.to_string(max_rows=50)}\n",
        "\n",
        "        CURRENT DATAFRAME STATISTICS:\n",
        "        - Shape: {original_df.shape}\n",
        "        - Columns: {list(original_df.columns)}\n",
        "        - Data types: {dict(original_df.dtypes)}\n",
        "        - Null values per column: {dict(original_df.isnull().sum())}\n",
        "\n",
        "        TARGET SCHEMA REQUIREMENTS (CRITICAL):\n",
        "        - Target Shape: {target_df.shape}\n",
        "        - Target Columns: {list(target_df.columns)}\n",
        "        - Target Data Types: {dict(target_df.dtypes)}\n",
        "        - Schema Compatibility Issues: {schema_comparison.get('critical_issues', [])}\n",
        "\n",
        "        SAMPLE TARGET DATA:\n",
        "        {target_df.head(3).to_string()}\n",
        "        \"\"\"\n",
        "\n",
        "        # Enhanced system prompt\n",
        "        enhanced_claude_system_prompt = \"\"\"You are an expert Python data analyst with specialization in precise dataframe transformations.\n",
        "        You will receive a dataframe that is already loaded as 'df' and specific transformation requirements.\n",
        "\n",
        "        CRITICAL PRECISION REQUIREMENTS:\n",
        "        1. The dataframe 'df' is already loaded - do not load or import it\n",
        "        2. Generate code that produces EXACTLY the target schema and data types\n",
        "        3. Wrap all code in ```python and ``` blocks\n",
        "        4. Include explicit data type conversions using .astype() when needed\n",
        "        5. Focus on precision over filtering - make exact transformations\n",
        "        6. Validate your output matches the target schema exactly\n",
        "        7. Handle null values, formatting, and data types with precision\n",
        "\n",
        "        SCHEMA MATCHING PRIORITY:\n",
        "        - Exact column names and order\n",
        "        - Exact data types for each column\n",
        "        - Exact row count and content\n",
        "        - Proper handling of null values and edge cases\"\"\"\n",
        "\n",
        "        # Enhanced messages for Claude\n",
        "        enhanced_claude_messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Here is the enhanced context:\\n\\n{enhanced_df_summary}\\n\\nTASK WITH PRECISION REQUIREMENTS: {prompt}\\n\\nGenerate precise Python code to accomplish this exact transformation with schema validation.\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Call Claude\n",
        "        claude_response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            system=enhanced_claude_system_prompt,\n",
        "            messages=enhanced_claude_messages,\n",
        "            max_tokens=3000,\n",
        "            temperature=0.1  # Lower temperature for more precision\n",
        "        )\n",
        "\n",
        "        # Extract response text\n",
        "        response_text = claude_response.content[0].text\n",
        "\n",
        "        log_to_file(f\"\"\"ENHANCED CLAUDE API RESPONSE - ATTEMPT {attempt_number}:\n",
        "Response Length: {len(response_text)} characters\n",
        "Enhanced Features Applied: ✅\n",
        "Response Received Successfully: ✅\n",
        "\n",
        "FULL ENHANCED CLAUDE RESPONSE:\n",
        "{'-'*60}\n",
        "{response_text}\n",
        "{'-'*60}\n",
        "\"\"\")\n",
        "\n",
        "        return response_text\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Enhanced Claude API Error: {str(e)}\"\n",
        "        log_to_file(f\"\"\"ENHANCED CLAUDE API CALL FAILED - ATTEMPT {attempt_number}:\n",
        "Error: {error_msg}\n",
        "Using enhanced fallback response\n",
        "\"\"\")\n",
        "\n",
        "        # Enhanced fallback response\n",
        "        enhanced_fallback_response = f\"\"\"\n",
        "        I'll help you with this enhanced transformation task. Here's the precise code:\n",
        "\n",
        "        ```python\n",
        "        # Enhanced error calling Claude API: {str(e)}\n",
        "        # Enhanced fallback code with schema awareness\n",
        "        print(\"Enhanced Claude API error, using schema-aware fallback\")\n",
        "        print(f\"Current dataframe shape: {{df.shape}}\")\n",
        "        print(f\"Target shape should be: {target_df.shape}\")\n",
        "        print(f\"Target dtypes: {dict(target_df.dtypes)}\")\n",
        "        print(\"Please review transformation logic manually\")\n",
        "\n",
        "        # Basic schema validation\n",
        "        if df.shape != {target_df.shape}:\n",
        "            print(\"Warning: Shape mismatch detected\")\n",
        "\n",
        "        # Show current vs target schema\n",
        "        print(\"\\\\nCurrent dtypes:\", dict(df.dtypes))\n",
        "        print(\"Target dtypes:\", {dict(target_df.dtypes)})\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        log_to_file(f\"Enhanced fallback response generated:\\n{enhanced_fallback_response}\")\n",
        "        return enhanced_fallback_response\n",
        "\n",
        "\n",
        "def _extract_enhanced_code_blocks(response_text):\n",
        "    \"\"\"\n",
        "    Enhanced code block extraction with validation\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Enhanced pattern matching for code blocks\n",
        "    patterns = [\n",
        "        r'```python\\s*(.*?)```',\n",
        "        r'```\\s*python\\s*(.*?)```',\n",
        "        r'```\\s*(.*?)```',\n",
        "        r'`([^`]+)`'  # Single backticks as fallback\n",
        "    ]\n",
        "\n",
        "    code_blocks = []\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            cleaned_code = match.strip()\n",
        "            if cleaned_code and len(cleaned_code) > 10:  # Filter out very short matches\n",
        "                code_blocks.append(cleaned_code)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    unique_blocks = []\n",
        "    for block in code_blocks:\n",
        "        if block not in seen:\n",
        "            seen.add(block)\n",
        "            unique_blocks.append(block)\n",
        "\n",
        "    return unique_blocks\n",
        "\n",
        "\n",
        "# Enhanced Final Analysis Generation\n",
        "def _get_enhanced_final_analysis_with_prompts_logged(client, original_info, modified_info, user_description,\n",
        "                                                   total_changes, changed_rows, changed_columns, changed_cells,\n",
        "                                                   schema_comparison, data_type_mismatches, claude_prompt,\n",
        "                                                   replication_results, log_to_file):\n",
        "    \"\"\"\n",
        "    Enhanced final analysis with comprehensive learning and schema insights\n",
        "    \"\"\"\n",
        "    best_attempt = replication_results.get(\"best_attempt\", {})\n",
        "    final_claude_prompt = replication_results.get(\"final_prompt\", claude_prompt)\n",
        "    learning_applied = replication_results.get(\"learning_applied\", False)\n",
        "    learning_history = replication_results.get(\"learning_history\", [])\n",
        "\n",
        "    success_status = \"SUCCESS\" if replication_results[\"final_success\"] else \"PARTIAL\"\n",
        "\n",
        "    # Enhanced statistics\n",
        "    total_cells = original_info[\"shape\"][0] * original_info[\"shape\"][1] if original_info[\"shape\"][0] > 0 else 1\n",
        "    change_density = total_changes / total_cells\n",
        "\n",
        "    enhanced_analysis_prompt = f\"\"\"\n",
        "    Analyze this enhanced dataframe change with comprehensive schema and learning insights.\n",
        "\n",
        "    ORIGINAL DATAFRAME:\n",
        "    Shape: {original_info['shape']}\n",
        "    Schema: {original_info['dtype_details']}\n",
        "    Data Sample: {original_info['full_data'][:2000]}...\n",
        "\n",
        "    MODIFIED DATAFRAME:\n",
        "    Shape: {modified_info['shape']}\n",
        "    Schema: {modified_info['dtype_details']}\n",
        "    Data Sample: {modified_info['full_data'][:2000]}...\n",
        "\n",
        "    ENHANCED CHANGE ANALYSIS:\n",
        "    - Total changes: {total_changes} cells ({change_density*100:.1f}%)\n",
        "    - Data type mismatches: {len(data_type_mismatches)}\n",
        "    - Schema compatibility: {schema_comparison.get('schema_compatibility_score', 0):.1%}\n",
        "    - Critical schema issues: {len(schema_comparison.get('critical_issues', []))}\n",
        "    - User description: \"{user_description}\"\n",
        "\n",
        "    REPLICATION TESTING RESULTS:\n",
        "    - Status: {success_status}\n",
        "    - Attempts: {len(replication_results['attempts'])}\n",
        "    - Best match: {best_attempt.get('match_score', 0):.1%}\n",
        "    - Schema match: {best_attempt.get('schema_match', False)}\n",
        "    - Learning applied: {learning_applied}\n",
        "    - Learning insights: {len(learning_history)}\n",
        "\n",
        "    FINAL WORKING PROMPT: {final_claude_prompt}\n",
        "\n",
        "    Provide comprehensive analysis in this exact JSON format:\n",
        "    {{\n",
        "        \"change_summary\": \"ENHANCED ANALYSIS: [comprehensive summary including WORKING_CLAUDE_PROMPT: {final_claude_prompt}] and detailed technical findings with schema validation results\",\n",
        "        \"change_type\": \"data_edit|structure_change|mixed\",\n",
        "        \"structural_changes\": {{\n",
        "            \"rows_added\": {max(0, modified_info['shape'][0] - original_info['shape'][0])},\n",
        "            \"rows_removed\": {max(0, original_info['shape'][0] - modified_info['shape'][0])},\n",
        "            \"columns_added\": {list(set(modified_info['columns']) - set(original_info['columns']))},\n",
        "            \"columns_removed\": {list(set(original_info['columns']) - set(modified_info['columns']))}\n",
        "        }},\n",
        "        \"enhanced_data_modifications\": {{\n",
        "            \"cells_changed\": {total_changes},\n",
        "            \"total_cells\": {total_cells},\n",
        "            \"change_percentage\": {change_density*100:.2f},\n",
        "            \"data_type_mismatches\": {len(data_type_mismatches)},\n",
        "            \"rows_affected\": {len(changed_rows)},\n",
        "            \"columns_affected\": {list(changed_columns)},\n",
        "            \"schema_compatibility_score\": {schema_comparison.get('schema_compatibility_score', 0):.2f},\n",
        "            \"critical_schema_issues\": {len(schema_comparison.get('critical_issues', []))},\n",
        "            \"precision_patterns\": [\"Enhanced patterns identified\"],\n",
        "            \"data_quality_impact\": \"improved|degraded|neutral\"\n",
        "        }},\n",
        "        \"replication_analysis\": {{\n",
        "            \"final_success\": {replication_results['final_success']},\n",
        "            \"best_match_score\": {best_attempt.get('match_score', 0):.2f},\n",
        "            \"schema_validation_passed\": {best_attempt.get('schema_match', False)},\n",
        "            \"learning_system_applied\": {learning_applied},\n",
        "            \"total_learning_insights\": {len(learning_history)},\n",
        "            \"common_issues_resolved\": [\"Issues identified and resolved\"]\n",
        "        }},\n",
        "        \"business_impact\": {{\n",
        "            \"rent_calculations_affected\": \"Enhanced analysis of rent impact with precision focus\",\n",
        "            \"tenant_information_updated\": \"Enhanced analysis of tenant data changes\",\n",
        "            \"occupancy_status_changed\": \"Enhanced analysis of occupancy changes\",\n",
        "            \"data_integrity_maintained\": \"Assessment of data integrity after changes\"\n",
        "        }},\n",
        "        \"enhanced_recommendations\": [\n",
        "            \"Enhanced recommendations based on learning insights\",\n",
        "            \"Schema validation recommendations\",\n",
        "            \"Data type precision improvements\"\n",
        "        ],\n",
        "        \"session_description\": \"ENHANCED_REPLICATION_STATUS: {success_status} | FINAL_CLAUDE_PROMPT: {final_claude_prompt} | LEARNING_APPLIED: {learning_applied} | REPLICATION_ATTEMPTS: {len(replication_results['attempts'])} | USER_EDIT: {user_description} | Changes: {total_changes} cells ({change_density*100:.1f}%) | Best match: {best_attempt.get('match_score', 0):.1%} | Schema: {best_attempt.get('schema_match', False)}\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    log_to_file(f\"\"\"ENHANCED FINAL ANALYSIS GENERATION:\n",
        "Sending enhanced final analysis request to GPT-4.1...\n",
        "Features: Schema insights, learning history, precision focus\n",
        "\n",
        "ENHANCED PROMPT FOR FINAL ANALYSIS:\n",
        "{'-'*60}\n",
        "{enhanced_analysis_prompt}\n",
        "{'-'*60}\n",
        "\"\"\", \"ENHANCED FINAL ANALYSIS GENERATION\")\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an enhanced data analyst with schema validation expertise. Return only valid JSON with comprehensive insights.\"},\n",
        "                {\"role\": \"user\", \"content\": enhanced_analysis_prompt}\n",
        "            ],\n",
        "            max_tokens=4000,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        enhanced_gpt_final_response = response.choices[0].message.content\n",
        "        log_to_file(f\"\"\"ENHANCED FINAL ANALYSIS GPT-4.1 RESPONSE:\n",
        "{'-'*60}\n",
        "{enhanced_gpt_final_response}\n",
        "{'-'*60}\n",
        "\"\"\")\n",
        "\n",
        "        try:\n",
        "            import json\n",
        "            import re\n",
        "            json_match = re.search(r'{.*}', enhanced_gpt_final_response, re.DOTALL)\n",
        "            if json_match:\n",
        "                enhanced_parsed_analysis = json.loads(json_match.group(0))\n",
        "                log_to_file(\"✅ Successfully parsed JSON from enhanced final analysis response\")\n",
        "                return enhanced_parsed_analysis\n",
        "            else:\n",
        "                log_to_file(\"❌ No JSON found in enhanced final analysis response\")\n",
        "        except Exception as json_error:\n",
        "            log_to_file(f\"❌ Enhanced JSON parsing failed: {str(json_error)}\")\n",
        "\n",
        "    except Exception as api_error:\n",
        "        log_to_file(f\"❌ Enhanced final analysis API call failed: {str(api_error)}\")\n",
        "\n",
        "    # Enhanced fallback analysis\n",
        "    enhanced_fallback_analysis = {\n",
        "        \"change_summary\": f\"ENHANCED_WORKING_CLAUDE_PROMPT: {final_claude_prompt} | ENHANCED_GPT4_ANALYSIS: [embedded] | {user_description} - {total_changes} changes with {replication_results['final_success']} replication and {learning_applied} learning\",\n",
        "        \"change_type\": \"data_edit\",\n",
        "        \"enhanced_data_modifications\": {\n",
        "            \"cells_changed\": total_changes,\n",
        "            \"total_cells\": total_cells,\n",
        "            \"change_percentage\": change_density*100,\n",
        "            \"data_type_mismatches\": len(data_type_mismatches),\n",
        "            \"schema_compatibility_score\": schema_comparison.get('schema_compatibility_score', 0)\n",
        "        },\n",
        "        \"replication_analysis\": {\n",
        "            \"final_success\": replication_results['final_success'],\n",
        "            \"best_match_score\": best_attempt.get('match_score', 0),\n",
        "            \"learning_system_applied\": learning_applied,\n",
        "            \"total_learning_insights\": len(learning_history)\n",
        "        },\n",
        "        \"session_description\": f\"ENHANCED_REPLICATION_STATUS: {success_status} | FINAL_CLAUDE_PROMPT: {final_claude_prompt} | LEARNING_APPLIED: {learning_applied} | USER_EDIT: {user_description} | Changes: {total_changes} cells\"\n",
        "    }\n",
        "\n",
        "    log_to_file(f\"Using enhanced fallback analysis:\\n{json.dumps(enhanced_fallback_analysis, indent=2)}\")\n",
        "    return enhanced_fallback_analysis\n",
        "\n",
        "\n",
        "# Enhanced Save Function with All Fixes Applied\n",
        "def save_edited_dataframe_enhanced_with_all_fixes(edited_df, description):\n",
        "    \"\"\"\n",
        "    ULTIMATE enhanced version with ALL CRITICAL FIXES APPLIED:\n",
        "    1. ✅ Fixed logging string formatting bug\n",
        "    2. ✅ Enhanced data type/formatting precision matching\n",
        "    3. ✅ Implemented attempt-to-attempt learning system\n",
        "    4. ✅ Added comprehensive data type validation\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder\n",
        "\n",
        "    if edited_df is None or edited_df.empty:\n",
        "        return \"No data to save\", gr.update()\n",
        "\n",
        "    try:\n",
        "        # Convert the edited dataframe to proper pandas DataFrame if needed\n",
        "        if not isinstance(edited_df, pd.DataFrame):\n",
        "            edited_df = pd.DataFrame(edited_df)\n",
        "\n",
        "        # Get the original dataframe for comparison\n",
        "        original_df = app_state[\"df\"].copy()\n",
        "\n",
        "        logger.info(\"Analyzing dataframe changes with ALL ENHANCED FIXES...\")\n",
        "        print(\"🚀 Analyzing changes with ALL CRITICAL FIXES APPLIED...\")\n",
        "\n",
        "        # Use the ULTIMATE enhanced analysis function with all fixes\n",
        "        change_analysis = analyze_dataframe_changes_with_gpt4(\n",
        "            original_df=original_df,\n",
        "            modified_df=edited_df,\n",
        "            user_description=description\n",
        "        )\n",
        "\n",
        "        # Generate meaningful description if not provided\n",
        "        if not description:\n",
        "            description = change_analysis.get(\"change_summary\", \"Manual edits via enhanced data editor\")\n",
        "\n",
        "        # Save as new version\n",
        "        version_name = save_dataframe_version(edited_df, description)\n",
        "\n",
        "        # Update app state\n",
        "        app_state[\"df\"] = edited_df\n",
        "\n",
        "        # Enhanced session recording with all fixes\n",
        "        if session_recorder.current_session_file:\n",
        "            session_description = change_analysis.get(\"session_description\", f\"Enhanced manual data edits: {description}\")\n",
        "\n",
        "            # Create comprehensive session entry with all enhancements\n",
        "            enhanced_session_entry = f\"\"\"\n",
        "ULTIMATE ENHANCED MANUAL DATA EDIT SESSION - ALL FIXES APPLIED\n",
        "============================================================\n",
        "Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "Edit Description: {description}\n",
        "Version Saved: {version_name}\n",
        "Log File: {change_analysis.get('log_file_path', 'N/A')}\n",
        "\n",
        "ALL CRITICAL FIXES APPLIED:\n",
        "✅ 1. Fixed logging string formatting bug\n",
        "✅ 2. Enhanced data type/formatting precision matching\n",
        "✅ 3. Implemented attempt-to-attempt learning system\n",
        "✅ 4. Added comprehensive data type validation\n",
        "\n",
        "ULTIMATE ENHANCED GPT-4.1 CHANGE ANALYSIS:\n",
        "{'-' * 50}\n",
        "Change Summary: {change_analysis.get('change_summary', 'N/A')}\n",
        "Change Type: {change_analysis.get('change_type', 'N/A')}\n",
        "Enhanced Features Applied: {change_analysis.get('enhanced_features_applied', False)}\n",
        "\n",
        "Replication Analysis:\n",
        "{json.dumps(change_analysis.get('replication_analysis', {}), indent=2)}\n",
        "\n",
        "Enhanced Data Modifications:\n",
        "{json.dumps(change_analysis.get('enhanced_data_modifications', {}), indent=2)}\n",
        "\n",
        "Structural Changes:\n",
        "{json.dumps(change_analysis.get('structural_changes', {}), indent=2)}\n",
        "\n",
        "Business Impact:\n",
        "{json.dumps(change_analysis.get('business_impact', {}), indent=2)}\n",
        "\n",
        "Enhanced Recommendations:\n",
        "{chr(10).join([f\"• {rec}\" for rec in change_analysis.get('enhanced_recommendations', [])])}\n",
        "\n",
        "Ultimate Technical Statistics:\n",
        "- Total Cells: {change_analysis.get('full_change_statistics', {}).get('total_cells', 'Unknown')}\n",
        "- Changed Cells: {change_analysis.get('full_change_statistics', {}).get('total_changes', 'Unknown')}\n",
        "- Change Density: {change_analysis.get('full_change_statistics', {}).get('change_density', 0)*100:.2f}%\n",
        "- Data Type Mismatches: {change_analysis.get('full_change_statistics', {}).get('data_type_mismatches', 'Unknown')}\n",
        "- Schema Compatibility: {change_analysis.get('full_change_statistics', {}).get('schema_changes', {}).get('schema_compatibility_score', 'Unknown')}\n",
        "- Learning Applied: {change_analysis.get('full_change_statistics', {}).get('learning_applied', False)}\n",
        "- Affected Rows: {len(change_analysis.get('full_change_statistics', {}).get('affected_rows', []))}\n",
        "- Affected Columns: {len(change_analysis.get('full_change_statistics', {}).get('affected_columns', []))}\n",
        "\n",
        "Original DataFrame Shape: {original_df.shape}\n",
        "Modified DataFrame Shape: {edited_df.shape}\n",
        "{'-' * 80}\n",
        "\"\"\"\n",
        "\n",
        "            # Append to session file\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(enhanced_session_entry + \"\\n\")\n",
        "\n",
        "            # Record in session data structure\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=f\"ULTIMATE ENHANCED MANUAL EDIT: {description}\",\n",
        "                ai_response=session_description,\n",
        "                action_type=\"manual_data_edit_ultimate_enhanced\",\n",
        "                code_executed=None,\n",
        "                version_saved=version_name\n",
        "            )\n",
        "\n",
        "            # Record dataframe version change\n",
        "            session_recorder.record_dataframe_version(\n",
        "                version_name=version_name,\n",
        "                description=description,\n",
        "                shape=list(edited_df.shape),\n",
        "                columns=list(edited_df.columns)\n",
        "            )\n",
        "\n",
        "            # Record issues with enhanced analysis\n",
        "            replication_success = change_analysis.get('full_change_statistics', {}).get('replication_success', False)\n",
        "            learning_applied = change_analysis.get('full_change_statistics', {}).get('learning_applied', False)\n",
        "\n",
        "            if not replication_success:\n",
        "                session_recorder.record_issue_found(\n",
        "                    f\"Enhanced manual edit replication failed: {description}. Check detailed log for comprehensive analysis.\",\n",
        "                    severity=\"medium\"\n",
        "                )\n",
        "            elif learning_applied:\n",
        "                session_recorder.record_issue_found(\n",
        "                    f\"Manual edit required learning system intervention: {description}. Replication succeeded after learning.\",\n",
        "                    severity=\"low\"\n",
        "                )\n",
        "\n",
        "            logger.info(\"Ultimate enhanced manual edit analysis recorded in copiloting session\")\n",
        "\n",
        "        # Log the changes\n",
        "        logger.info(f\"Saved edited dataframe as version {version_name} with all enhancements\")\n",
        "\n",
        "        # Create comprehensive success message with all enhancements\n",
        "        log_file_path = change_analysis.get('log_file_path', 'N/A')\n",
        "        replication_success = change_analysis.get('full_change_statistics', {}).get('replication_success', False)\n",
        "        replication_attempts = change_analysis.get('full_change_statistics', {}).get('replication_attempts', 0)\n",
        "        learning_applied = change_analysis.get('full_change_statistics', {}).get('learning_applied', False)\n",
        "        schema_score = change_analysis.get('full_change_statistics', {}).get('schema_changes', {}).get('schema_compatibility_score', 0)\n",
        "        data_type_mismatches = change_analysis.get('full_change_statistics', {}).get('data_type_mismatches', 0)\n",
        "\n",
        "        ultimate_success_message = f\"\"\"✅ Successfully saved as version {version_name}\n",
        "\n",
        "🚀 ULTIMATE ENHANCED GPT-4.1 Analysis Summary:\n",
        "{change_analysis.get('change_summary', 'Changes analyzed with all enhancements')[:300]}...\n",
        "\n",
        "📊 Enhanced Change Details:\n",
        "• Change Type: {change_analysis.get('change_type', 'Unknown')}\n",
        "• Original Shape: {original_df.shape}\n",
        "• New Shape: {edited_df.shape}\n",
        "• Replication Success: {'✅ Yes' if replication_success else '❌ Partial/Failed'}\n",
        "• Replication Attempts: {replication_attempts}\n",
        "• Learning System Applied: {'✅ Yes' if learning_applied else '❌ No'}\n",
        "• Schema Compatibility: {schema_score:.1%}\n",
        "• Data Type Mismatches: {data_type_mismatches}\n",
        "\n",
        "🔧 ALL CRITICAL FIXES APPLIED:\n",
        "✅ 1. Fixed logging string formatting bug\n",
        "✅ 2. Enhanced data type/formatting precision matching\n",
        "✅ 3. Implemented attempt-to-attempt learning system\n",
        "✅ 4. Added comprehensive data type validation\n",
        "\n",
        "📝 Session Recording: {'✅ Recorded' if session_recorder.current_session_file else '❌ No active session'}\n",
        "\n",
        "📁 Comprehensive Analysis Log:\n",
        "{log_file_path}\n",
        "\n",
        "This ultimate enhanced log contains:\n",
        "• Complete GPT-4.1 prompts and responses with schema focus\n",
        "• All Claude API calls with learning system feedback\n",
        "• Step-by-step replication attempts with precision validation\n",
        "• Cell-by-cell change analysis with data type awareness\n",
        "• Business impact assessment with enhanced insights\n",
        "• Learning system evolution and improvements\n",
        "• Comprehensive error handling and debugging information\n",
        "\"\"\"\n",
        "\n",
        "        # Add enhanced recommendations if available\n",
        "        if change_analysis.get('enhanced_recommendations'):\n",
        "            ultimate_success_message += f\"\\n💡 Enhanced Recommendations:\\n\"\n",
        "            for rec in change_analysis['enhanced_recommendations'][:3]:\n",
        "                ultimate_success_message += f\"• {rec}\\n\"\n",
        "\n",
        "        # Add comprehensive log file information\n",
        "        ultimate_success_message += f\"\\n📄 For complete enhanced debugging and learning insights: {log_file_path}\"\n",
        "\n",
        "        # Add replication analysis summary\n",
        "        replication_analysis = change_analysis.get('replication_analysis', {})\n",
        "        if replication_analysis:\n",
        "            ultimate_success_message += f\"\\n\\n🤖 Replication Analysis:\"\n",
        "            ultimate_success_message += f\"\\n• Best Match Score: {replication_analysis.get('best_match_score', 0):.1%}\"\n",
        "            ultimate_success_message += f\"\\n• Schema Validation: {'✅ Passed' if replication_analysis.get('schema_validation_passed', False) else '❌ Failed'}\"\n",
        "            ultimate_success_message += f\"\\n• Learning Insights Generated: {replication_analysis.get('total_learning_insights', 0)}\"\n",
        "\n",
        "        return ultimate_success_message, gr.update(value=edited_df)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error in ultimate enhanced save: {str(e)}\"\n",
        "        logger.error(f\"Error in ultimate enhanced save: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        # Enhanced error recording in session\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=f\"ULTIMATE ENHANCED MANUAL EDIT FAILED: {description}\",\n",
        "                ai_response=error_msg,\n",
        "                action_type=\"manual_edit_error_ultimate_enhanced\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        return error_msg, gr.update()\n",
        "\n",
        "\n",
        "# Enhanced Log Summary with All Fixes\n",
        "def get_enhanced_manual_edit_logs_summary_with_all_fixes():\n",
        "    \"\"\"\n",
        "    Generate enhanced summary of all manual edit analysis log files with comprehensive insights\n",
        "    \"\"\"\n",
        "    logs_dir = \"manual_edit_analysis_logs\"\n",
        "\n",
        "    if not os.path.exists(logs_dir):\n",
        "        return \"No enhanced manual edit analysis logs found yet.\"\n",
        "\n",
        "    try:\n",
        "        log_files = [f for f in os.listdir(logs_dir) if f.endswith('_detailed_analysis.txt')]\n",
        "\n",
        "        if not log_files:\n",
        "            return \"No enhanced detailed analysis log files found.\"\n",
        "\n",
        "        # Sort by creation time (newest first)\n",
        "        log_files.sort(key=lambda x: os.path.getctime(os.path.join(logs_dir, x)), reverse=True)\n",
        "\n",
        "        enhanced_summary = f\"\"\"📁 ULTIMATE ENHANCED Manual Edit Analysis Logs Summary\n",
        "Found {len(log_files)} comprehensive analysis log files with ALL FIXES APPLIED:\n",
        "\n",
        "🔧 CRITICAL FIXES INCLUDED IN LOGS:\n",
        "✅ 1. Fixed logging string formatting bug\n",
        "✅ 2. Enhanced data type/formatting precision matching\n",
        "✅ 3. Implemented attempt-to-attempt learning system\n",
        "✅ 4. Added comprehensive data type validation\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        total_size = 0\n",
        "        enhanced_features_count = 0\n",
        "        learning_applied_count = 0\n",
        "\n",
        "        for i, log_file in enumerate(log_files[:15], 1):  # Show last 15\n",
        "            file_path = os.path.join(logs_dir, log_file)\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            total_size += file_size\n",
        "            created_time = datetime.fromtimestamp(os.path.getctime(file_path))\n",
        "\n",
        "            # Try to extract enhanced information from log file\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    log_content = f.read()\n",
        "\n",
        "                # Check for enhanced features\n",
        "                has_enhanced_features = \"ENHANCED FEATURES APPLIED\" in log_content\n",
        "                has_learning = \"LEARNING SYSTEM\" in log_content\n",
        "                has_schema_validation = \"SCHEMA VALIDATION\" in log_content\n",
        "\n",
        "                if has_enhanced_features:\n",
        "                    enhanced_features_count += 1\n",
        "                if has_learning:\n",
        "                    learning_applied_count += 1\n",
        "\n",
        "            except Exception:\n",
        "                has_enhanced_features = False\n",
        "                has_learning = False\n",
        "                has_schema_validation = False\n",
        "\n",
        "            # Extract session info from filename\n",
        "            session_id = log_file.replace('_detailed_analysis.txt', '')\n",
        "\n",
        "            enhanced_summary += f\"\"\"{i}. {session_id}\n",
        "   📄 File: {log_file}\n",
        "   📅 Created: {created_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "   💾 Size: {file_size:,} bytes\n",
        "   🔧 Enhanced Features: {'✅ Yes' if has_enhanced_features else '❌ No'}\n",
        "   🧠 Learning Applied: {'✅ Yes' if has_learning else '❌ No'}\n",
        "   📋 Schema Validation: {'✅ Yes' if has_schema_validation else '❌ No'}\n",
        "   📁 Path: {file_path}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        if len(log_files) > 15:\n",
        "            enhanced_summary += f\"... and {len(log_files) - 15} more enhanced log files\\n\"\n",
        "\n",
        "        enhanced_summary += f\"\"\"\n",
        "📈 ENHANCED LOGS STATISTICS:\n",
        "• Total Log Files: {len(log_files)}\n",
        "• Total Storage Used: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\n",
        "• Files with Enhanced Features: {enhanced_features_count}/{len(log_files)} ({enhanced_features_count/len(log_files)*100:.1f}%)\n",
        "• Files with Learning System: {learning_applied_count}/{len(log_files)} ({learning_applied_count/len(log_files)*100:.1f}%)\n",
        "\n",
        "📋 ULTIMATE ENHANCED LOG CONTENTS INCLUDE:\n",
        "• Complete GPT-4.1 prompts and responses with schema awareness\n",
        "• All Claude API calls with learning system feedback loops\n",
        "• Step-by-step replication testing with precision validation\n",
        "• Cell-by-cell dataframe comparison with data type analysis\n",
        "• Schema compatibility analysis and validation results\n",
        "• Business impact assessment with enhanced insights\n",
        "• Learning system evolution and improvement tracking\n",
        "• Data type mismatch detection and resolution strategies\n",
        "• Comprehensive error handling and debugging information\n",
        "• Attempt-to-attempt learning and prompt refinement details\n",
        "\n",
        "🔧 CRITICAL FIXES VERIFICATION:\n",
        "1. ✅ Logging String Formatting: All logs use proper string formatting\n",
        "2. ✅ Data Type Precision: Enhanced schema validation throughout\n",
        "3. ✅ Learning System: Progressive improvement across attempts\n",
        "4. ✅ Type Validation: Comprehensive data type compatibility checking\n",
        "\n",
        "📂 All enhanced logs saved in: {logs_dir}/\n",
        "\"\"\"\n",
        "\n",
        "        return enhanced_summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error reading enhanced log files: {str(e)}\"\n",
        "\n",
        "\n",
        "# Enhanced Log Reader\n",
        "def read_enhanced_manual_edit_log(session_id):\n",
        "    \"\"\"\n",
        "    Read and return enhanced log with analysis insights\n",
        "    \"\"\"\n",
        "    logs_dir = \"manual_edit_analysis_logs\"\n",
        "    log_file_path = os.path.join(logs_dir, f\"{session_id}_detailed_analysis.txt\")\n",
        "\n",
        "    if not os.path.exists(log_file_path):\n",
        "        return f\"Enhanced log file not found: {log_file_path}\"\n",
        "\n",
        "    try:\n",
        "        with open(log_file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Analyze log content for enhanced insights\n",
        "        enhanced_features_detected = \"ENHANCED FEATURES APPLIED\" in content\n",
        "        learning_system_detected = \"LEARNING SYSTEM\" in content\n",
        "        schema_validation_detected = \"SCHEMA VALIDATION\" in content\n",
        "        replication_success = \"REPLICATION SUCCESSFUL\" in content\n",
        "\n",
        "        file_size = len(content)\n",
        "        line_count = content.count('\\n')\n",
        "\n",
        "        return f\"\"\"📄 ULTIMATE ENHANCED Manual Edit Analysis Log: {session_id}\n",
        "{'='*80}\n",
        "\n",
        "🔧 CRITICAL FIXES STATUS:\n",
        "✅ Enhanced Features Applied: {enhanced_features_detected}\n",
        "✅ Learning System Active: {learning_system_detected}\n",
        "✅ Schema Validation Included: {schema_validation_detected}\n",
        "✅ Replication Successful: {replication_success}\n",
        "\n",
        "📊 LOG STATISTICS:\n",
        "• File Size: {file_size:,} characters ({file_size/1024:.1f} KB)\n",
        "• Total Lines: {line_count:,}\n",
        "• Enhanced Analysis: {'✅ Complete' if enhanced_features_detected else '❌ Basic'}\n",
        "\n",
        "{'='*80}\n",
        "\n",
        "{content}\n",
        "\n",
        "{'='*80}\n",
        "End of ultimate enhanced log file: {log_file_path}\n",
        "\n",
        "🔍 ANALYSIS SUMMARY:\n",
        "This log contains {'comprehensive enhanced analysis' if enhanced_features_detected else 'basic analysis'} with:\n",
        "• {'✅' if 'GPT-4.1' in content else '❌'} GPT-4.1 API interactions\n",
        "• {'✅' if 'CLAUDE API' in content else '❌'} Claude API calls and responses\n",
        "• {'✅' if learning_system_detected else '❌'} Learning system feedback loops\n",
        "• {'✅' if schema_validation_detected else '❌'} Schema compatibility validation\n",
        "• {'✅' if 'DATA TYPE' in content else '❌'} Data type precision analysis\n",
        "• {'✅' if replication_success else '❌'} Successful replication testing\n",
        "\"\"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error reading enhanced log file: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ahMIzhPwO5y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hPSLoUqgFXm"
      },
      "outputs": [],
      "source": [
        "def save_edited_dataframe_enhanced(edited_df, description):\n",
        "    \"\"\"\n",
        "    Enhanced version that analyzes changes with GPT-4.1 and records in session.\n",
        "    \"\"\"\n",
        "    global app_state, session_recorder\n",
        "\n",
        "    if edited_df is None or edited_df.empty:\n",
        "        return \"No data to save\", gr.update()\n",
        "\n",
        "    try:\n",
        "        # Convert the edited dataframe to proper pandas DataFrame if needed\n",
        "        if not isinstance(edited_df, pd.DataFrame):\n",
        "            edited_df = pd.DataFrame(edited_df)\n",
        "\n",
        "        # Get the original dataframe for comparison\n",
        "        original_df = app_state[\"df\"].copy()\n",
        "\n",
        "        logger.info(\"Analyzing dataframe changes with GPT-4.1...\")\n",
        "        print(\"🤖 Analyzing changes with GPT-4.1...\")\n",
        "\n",
        "        # Use GPT-4.1 to analyze the differences\n",
        "        change_analysis = analyze_dataframe_changes_with_gpt4(\n",
        "            original_df=original_df,\n",
        "            modified_df=edited_df,\n",
        "            user_description=description\n",
        "        )\n",
        "\n",
        "        # Generate a meaningful description if not provided\n",
        "        if not description:\n",
        "            description = change_analysis.get(\"change_summary\", \"Manual edits via data editor\")\n",
        "\n",
        "        # Save as new version\n",
        "        version_name = save_dataframe_version(edited_df, description)\n",
        "\n",
        "        # Update the app state with the edited dataframe\n",
        "        app_state[\"df\"] = edited_df\n",
        "\n",
        "        # Record this in the copiloting session if active\n",
        "        if session_recorder.current_session_file:\n",
        "            session_description = change_analysis.get(\"session_description\", f\"Manual data edits: {description}\")\n",
        "\n",
        "            # Create detailed session entry\n",
        "            session_entry = f\"\"\"\n",
        "MANUAL DATA EDIT SESSION\n",
        "========================\n",
        "Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "Edit Description: {description}\n",
        "Version Saved: {version_name}\n",
        "\n",
        "GPT-4.1 CHANGE ANALYSIS:\n",
        "{'-' * 40}\n",
        "Change Summary: {change_analysis.get('change_summary', 'N/A')}\n",
        "Change Type: {change_analysis.get('change_type', 'N/A')}\n",
        "\n",
        "Structural Changes:\n",
        "{json.dumps(change_analysis.get('structural_changes', {}), indent=2)}\n",
        "\n",
        "Data Modifications:\n",
        "{json.dumps(change_analysis.get('data_modifications', {}), indent=2)}\n",
        "\n",
        "Business Impact:\n",
        "{json.dumps(change_analysis.get('business_impact', {}), indent=2)}\n",
        "\n",
        "Recommendations:\n",
        "{chr(10).join([f\"• {rec}\" for rec in change_analysis.get('recommendations', [])])}\n",
        "\n",
        "Original DataFrame Shape: {original_df.shape}\n",
        "Modified DataFrame Shape: {edited_df.shape}\n",
        "{'-' * 80}\n",
        "\"\"\"\n",
        "\n",
        "            # Append to session file\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(session_entry + \"\\n\")\n",
        "\n",
        "            # Record in session data structure\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=f\"MANUAL EDIT: {description}\",\n",
        "                ai_response=session_description,\n",
        "                action_type=\"manual_data_edit\",\n",
        "                code_executed=None,\n",
        "                version_saved=version_name\n",
        "            )\n",
        "\n",
        "            # Record the dataframe version change\n",
        "            session_recorder.record_dataframe_version(\n",
        "                version_name=version_name,\n",
        "                description=description,\n",
        "                shape=list(edited_df.shape),\n",
        "                columns=list(edited_df.columns)\n",
        "            )\n",
        "\n",
        "            # Record any issues found by GPT-4\n",
        "            if change_analysis.get('data_modifications', {}).get('data_quality_impact') == 'degraded':\n",
        "                session_recorder.record_issue_found(\n",
        "                    f\"Data quality may have degraded due to manual edits: {description}\",\n",
        "                    severity=\"medium\"\n",
        "                )\n",
        "\n",
        "            logger.info(\"Manual edit recorded in copiloting session\")\n",
        "\n",
        "        # Log the changes\n",
        "        logger.info(f\"Saved edited dataframe as version {version_name}\")\n",
        "\n",
        "        # Create detailed success message\n",
        "        success_message = f\"\"\"✅ Successfully saved as version {version_name}\n",
        "\n",
        "🤖 GPT-4.1 Analysis Summary:\n",
        "{change_analysis.get('change_summary', 'Changes analyzed')}\n",
        "\n",
        "📊 Change Details:\n",
        "• Change Type: {change_analysis.get('change_type', 'Unknown')}\n",
        "• Original Shape: {original_df.shape}\n",
        "• New Shape: {edited_df.shape}\n",
        "\n",
        "📝 Session Recording: {'✅ Recorded' if session_recorder.current_session_file else '❌ No active session'}\n",
        "\"\"\"\n",
        "\n",
        "        # Add recommendations if available\n",
        "        if change_analysis.get('recommendations'):\n",
        "            success_message += f\"\\n💡 Recommendations:\\n\"\n",
        "            for rec in change_analysis['recommendations'][:3]:  # Show first 3\n",
        "                success_message += f\"• {rec}\\n\"\n",
        "\n",
        "        return success_message, gr.update(value=edited_df)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Error saving: {str(e)}\"\n",
        "        logger.error(f\"Error saving edited dataframe: {e}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        # Still try to record the error in session\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=f\"MANUAL EDIT FAILED: {description}\",\n",
        "                ai_response=error_msg,\n",
        "                action_type=\"manual_edit_error\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        return error_msg, gr.update()\n",
        "\n",
        "\n",
        "def load_latest_version_for_editing_enhanced():\n",
        "    \"\"\"Enhanced version that records when user loads data for editing\"\"\"\n",
        "    global app_state, session_recorder\n",
        "\n",
        "    if app_state is None or app_state[\"df\"] is None:\n",
        "        return None, \"No data loaded. Please upload a rent roll first.\"\n",
        "\n",
        "    try:\n",
        "        # Use the current dataframe (which is the latest)\n",
        "        df = app_state[\"df\"].copy()\n",
        "        df = df.fillna('')\n",
        "\n",
        "        # Get version info\n",
        "        if app_state[\"df_versions\"]:\n",
        "            latest_version = app_state[\"df_versions\"][-1]\n",
        "            version_info = f\"Loaded version: {latest_version['name']} - {latest_version['description']}\"\n",
        "        else:\n",
        "            version_info = \"Loaded current data (no versions saved yet)\"\n",
        "\n",
        "        # Record this action in session if active\n",
        "        if session_recorder.current_session_file:\n",
        "            session_entry = f\"\"\"\n",
        "DATA EDITING SESSION STARTED\n",
        "============================\n",
        "Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "Action: User loaded dataframe for manual editing\n",
        "Version Loaded: {latest_version['name'] if app_state[\"df_versions\"] else 'Current'}\n",
        "DataFrame Shape: {df.shape}\n",
        "DataFrame Columns: {list(df.columns)}\n",
        "{'-' * 80}\n",
        "\"\"\"\n",
        "\n",
        "            # Append to session file\n",
        "            with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(session_entry + \"\\n\")\n",
        "\n",
        "            # Record in session data\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=\"LOAD FOR EDITING: User opened data editor\",\n",
        "                ai_response=\"Dataframe loaded for manual editing\",\n",
        "                action_type=\"load_for_editing\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        logger.info(f\"Loaded dataframe for editing: {df.shape}\")\n",
        "        return df, version_info\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error loading data: {str(e)}\"\n",
        "        logger.error(f\"Error loading data for editing: {e}\")\n",
        "\n",
        "        # Record error in session\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=\"LOAD FOR EDITING FAILED\",\n",
        "                ai_response=error_msg,\n",
        "                action_type=\"load_editing_error\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        return None, error_msg\n",
        "\n",
        "\n",
        "def load_specific_version_enhanced(version_name):\n",
        "    \"\"\"Enhanced version that records version loading with GPT-4 analysis\"\"\"\n",
        "    global app_state, session_recorder\n",
        "\n",
        "    if not version_name:\n",
        "        return None, \"Please select a version to load\"\n",
        "\n",
        "    try:\n",
        "        # Extract clean version name (remove status indicators)\n",
        "        clean_version_name = version_name.split(\" (\")[0]\n",
        "\n",
        "        # Find the version file\n",
        "        versions_dir = \"rent_roll_versions\"\n",
        "        csv_filename = os.path.join(versions_dir, f\"rent_roll_{clean_version_name}.csv\")\n",
        "\n",
        "        if os.path.exists(csv_filename):\n",
        "            df = pd.read_csv(csv_filename)\n",
        "            df = df.fillna('')\n",
        "\n",
        "            # Record this action in session if active\n",
        "            if session_recorder.current_session_file:\n",
        "                session_entry = f\"\"\"\n",
        "SPECIFIC VERSION LOADED FOR EDITING\n",
        "===================================\n",
        "Timestamp: {datetime.now().strftime('%H:%M:%S')}\n",
        "Version Loaded: {clean_version_name}\n",
        "DataFrame Shape: {df.shape}\n",
        "DataFrame Columns: {list(df.columns)}\n",
        "File Path: {csv_filename}\n",
        "{'-' * 80}\n",
        "\"\"\"\n",
        "\n",
        "                # Append to session file\n",
        "                with open(session_recorder.current_session_file, 'a', encoding='utf-8') as f:\n",
        "                    f.write(session_entry + \"\\n\")\n",
        "\n",
        "                # Record in session data\n",
        "                session_recorder.record_conversation_turn(\n",
        "                    user_message=f\"LOAD SPECIFIC VERSION: {clean_version_name}\",\n",
        "                    ai_response=f\"Loaded version {clean_version_name} for editing\",\n",
        "                    action_type=\"load_specific_version\",\n",
        "                    code_executed=None,\n",
        "                    version_saved=None\n",
        "                )\n",
        "\n",
        "            logger.info(f\"Loaded version {clean_version_name} for editing\")\n",
        "            return df, f\"Loaded version: {clean_version_name}\"\n",
        "        else:\n",
        "            error_msg = f\"Version file not found: {clean_version_name}\"\n",
        "\n",
        "            # Record error in session\n",
        "            if session_recorder.current_session_file:\n",
        "                session_recorder.record_conversation_turn(\n",
        "                    user_message=f\"LOAD VERSION FAILED: {clean_version_name}\",\n",
        "                    ai_response=error_msg,\n",
        "                    action_type=\"load_version_error\",\n",
        "                    code_executed=None,\n",
        "                    version_saved=None\n",
        "                )\n",
        "\n",
        "            return None, error_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error loading version: {str(e)}\"\n",
        "        logger.error(f\"Error loading version {version_name}: {e}\")\n",
        "\n",
        "        # Record error in session\n",
        "        if session_recorder.current_session_file:\n",
        "            session_recorder.record_conversation_turn(\n",
        "                user_message=f\"LOAD VERSION ERROR: {version_name}\",\n",
        "                ai_response=error_msg,\n",
        "                action_type=\"load_version_error\",\n",
        "                code_executed=None,\n",
        "                version_saved=None\n",
        "            )\n",
        "\n",
        "        return None, error_msg\n",
        "\n",
        "\n",
        "# Update the Gradio event handlers to use enhanced functions\n",
        "def setup_enhanced_edit_data_handlers():\n",
        "    \"\"\"\n",
        "    Setup function to update Gradio event handlers for enhanced edit data functionality.\n",
        "    Add this to your Gradio interface setup.\n",
        "    \"\"\"\n",
        "\n",
        "    # Enhanced event handlers for Edit Data tab\n",
        "    refresh_versions_btn.click(\n",
        "        refresh_version_dropdown,\n",
        "        outputs=[version_dropdown]\n",
        "    )\n",
        "\n",
        "    load_version_btn.click(\n",
        "        load_specific_version_enhanced,  # Use enhanced version\n",
        "        inputs=[version_dropdown],\n",
        "        outputs=[editable_df, edit_status]\n",
        "    )\n",
        "\n",
        "    save_changes_btn.click(\n",
        "        save_edited_dataframe_enhanced,  # Use enhanced version\n",
        "        inputs=[editable_df, save_description],\n",
        "        outputs=[save_status, editable_df]\n",
        "    ).then(\n",
        "        refresh_version_dropdown,  # Refresh the dropdown after saving\n",
        "        outputs=[version_dropdown]\n",
        "    )\n",
        "\n",
        "    # You can also add a \"Load Latest\" button that uses the enhanced function\n",
        "    # load_latest_btn.click(\n",
        "    #     load_latest_version_for_editing_enhanced,\n",
        "    #     outputs=[editable_df, edit_status]\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJbOfSqqULbd"
      },
      "outputs": [],
      "source": [
        "class TemplateApplicationEngine:\n",
        "    def __init__(self):\n",
        "        self.templates_dir = \"rent_roll_templates\"\n",
        "        self.application_sessions_dir = \"template_applications\"\n",
        "        os.makedirs(self.application_sessions_dir, exist_ok=True)\n",
        "        self.current_application = None\n",
        "\n",
        "    def start_template_application(self, template_id, new_rent_roll_file, new_rent_roll_df):\n",
        "        \"\"\"Initialize a new template application session\"\"\"\n",
        "\n",
        "        # Load the template data\n",
        "        template_data, starting_template_df, final_template_df = enhanced_template_manager.load_template_dataframes(template_id)\n",
        "\n",
        "        if template_data is None:\n",
        "            return None, \"❌ Failed to load template data\"\n",
        "\n",
        "        # Create application session\n",
        "        app_session_id = f\"app_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        self.current_application = {\n",
        "            \"session_id\": app_session_id,\n",
        "            \"template_id\": template_id,\n",
        "            \"template_data\": template_data,\n",
        "            \"starting_template_df\": starting_template_df,\n",
        "            \"final_template_df\": final_template_df,\n",
        "            \"new_rent_roll_file\": new_rent_roll_file,\n",
        "            \"new_rent_roll_df\": new_rent_roll_df.copy(),\n",
        "            \"current_df\": new_rent_roll_df.copy(),\n",
        "            \"step_results\": [],\n",
        "            \"current_step\": 0,\n",
        "            \"total_steps\": len(template_data.get(\"raw_workflow_steps\", [])),\n",
        "            \"completed_steps\": [],\n",
        "            \"failed_steps\": [],\n",
        "            \"log_file\": None\n",
        "        }\n",
        "\n",
        "        # Create log file\n",
        "        log_filename = f\"{app_session_id}_application_log.txt\"\n",
        "        self.current_application[\"log_file\"] = os.path.join(self.application_sessions_dir, log_filename)\n",
        "\n",
        "        # Write initial log\n",
        "        with open(self.current_application[\"log_file\"], 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"=== TEMPLATE APPLICATION SESSION ===\\n\")\n",
        "            f.write(f\"Session ID: {app_session_id}\\n\")\n",
        "            f.write(f\"Template ID: {template_id}\\n\")\n",
        "            f.write(f\"Template Name: {template_data.get('template_name', 'Unknown')}\\n\")\n",
        "            f.write(f\"New Rent Roll File: {new_rent_roll_file}\\n\")\n",
        "            f.write(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"Total Steps to Execute: {self.current_application['total_steps']}\\n\")\n",
        "            f.write(f\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "        return app_session_id, \"✅ Template application session started successfully\"\n",
        "\n",
        "    def get_next_step_to_execute(self):\n",
        "        \"\"\"Get the next workflow step to execute\"\"\"\n",
        "        if not self.current_application:\n",
        "            return None, \"No active application session\"\n",
        "\n",
        "        current_step = self.current_application[\"current_step\"]\n",
        "        workflow_steps = self.current_application[\"template_data\"].get(\"raw_workflow_steps\", [])\n",
        "\n",
        "        if current_step >= len(workflow_steps):\n",
        "            return None, \"All steps completed\"\n",
        "\n",
        "        return workflow_steps[current_step], f\"Step {current_step + 1} of {len(workflow_steps)}\"\n",
        "\n",
        "    def execute_next_step_with_ai(self):\n",
        "        \"\"\"Execute the next template step using GPT-4.1 + Claude 3.7 with DETAILED LOGGING\"\"\"\n",
        "        if not self.current_application:\n",
        "            return \"❌ No active application session\"\n",
        "\n",
        "        # Get next step\n",
        "        next_step, step_info = self.get_next_step_to_execute()\n",
        "        if next_step is None:\n",
        "            return self._finalize_application()\n",
        "\n",
        "        current_step_num = self.current_application[\"current_step\"] + 1\n",
        "\n",
        "        try:\n",
        "            # Enhanced step start logging\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"\\n{'='*80}\\n\")\n",
        "                f.write(f\"EXECUTING STEP {current_step_num} OF {self.current_application['total_steps']}\\n\")\n",
        "                f.write(f\"{'='*80}\\n\")\n",
        "                f.write(f\"Timestamp: {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
        "                f.write(f\"Original User Query: {next_step.get('user_message', 'N/A')}\\n\")\n",
        "                f.write(f\"Original Action Type: {next_step.get('action_type', 'N/A')}\\n\")\n",
        "                f.write(f\"Original Code Executed: {next_step.get('code_executed', 'None')}\\n\")\n",
        "                f.write(f\"Original AI Response: {next_step.get('ai_response', 'N/A')[:200]}...\\n\")\n",
        "                f.write(f\"{'-'*80}\\n\\n\")\n",
        "\n",
        "            # Use GPT-4.1 to analyze step and create optimal prompt for Claude\n",
        "            print(f\"🤖 Step {current_step_num}: Analyzing with GPT-4.1...\")\n",
        "            step_analysis = self._analyze_step_with_gpt4(next_step, current_step_num)\n",
        "\n",
        "            # LOG GPT-4.1 ANALYSIS IN DETAIL\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"🤖 GPT-4.1 STEP ANALYSIS\\n\")\n",
        "                f.write(f\"{'-'*40}\\n\")\n",
        "                f.write(f\"Analysis Result:\\n\")\n",
        "                f.write(json.dumps(step_analysis, indent=2, default=str))\n",
        "                f.write(f\"\\n\\nCan Execute: {step_analysis.get('step_adaptation', {}).get('can_execute', 'Unknown')}\\n\")\n",
        "                f.write(f\"Reason: {step_analysis.get('step_adaptation', {}).get('reason', 'No reason provided')}\\n\")\n",
        "                f.write(f\"\\nColumn Mapping: {step_analysis.get('step_adaptation', {}).get('column_mapping', {})}\\n\")\n",
        "                f.write(f\"Parameter Adjustments: {step_analysis.get('step_adaptation', {}).get('parameter_adjustments', [])}\\n\")\n",
        "                f.write(f\"\\nGenerated Claude Prompt:\\n\")\n",
        "                f.write(f\"{'─'*40}\\n\")\n",
        "                f.write(f\"{step_analysis.get('claude_prompt', 'No prompt generated')}\\n\")\n",
        "                f.write(f\"{'─'*40}\\n\\n\")\n",
        "\n",
        "            # Use Claude 3.7 to execute the adapted step\n",
        "            print(f\"🧠 Step {current_step_num}: Executing with Claude 3.7...\")\n",
        "            execution_result = self._execute_step_with_claude(step_analysis, current_step_num)\n",
        "\n",
        "            # LOG CLAUDE EXECUTION IN DETAIL\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"🧠 CLAUDE 3.7 EXECUTION\\n\")\n",
        "                f.write(f\"{'-'*40}\\n\")\n",
        "                f.write(f\"Execution Success: {execution_result.get('success', False)}\\n\")\n",
        "                f.write(f\"Summary: {execution_result.get('summary', 'No summary')}\\n\\n\")\n",
        "\n",
        "                # Log Claude's full response\n",
        "                f.write(f\"Claude's Full Response:\\n\")\n",
        "                f.write(f\"{'─'*40}\\n\")\n",
        "                f.write(f\"{execution_result.get('claude_response', 'No response captured')}\\n\")\n",
        "                f.write(f\"{'─'*40}\\n\\n\")\n",
        "\n",
        "                # Log the actual code that was executed\n",
        "                if execution_result.get('executed_code'):\n",
        "                    f.write(f\"Code Generated and Executed:\\n\")\n",
        "                    f.write(f\"```python\\n\")\n",
        "                    f.write(f\"{execution_result.get('executed_code')}\\n\")\n",
        "                    f.write(f\"```\\n\\n\")\n",
        "\n",
        "                # Log execution output\n",
        "                if execution_result.get('execution_output'):\n",
        "                    f.write(f\"Code Execution Output:\\n\")\n",
        "                    f.write(f\"{'─'*40}\\n\")\n",
        "                    f.write(f\"{execution_result.get('execution_output')}\\n\")\n",
        "                    f.write(f\"{'─'*40}\\n\\n\")\n",
        "\n",
        "                # Log any errors\n",
        "                if not execution_result.get('success') and execution_result.get('error'):\n",
        "                    f.write(f\"❌ ERROR DETAILS:\\n\")\n",
        "                    f.write(f\"{execution_result.get('error')}\\n\\n\")\n",
        "\n",
        "            # Record results\n",
        "            step_result = {\n",
        "                \"step_number\": current_step_num,\n",
        "                \"original_step\": next_step,\n",
        "                \"gpt4_analysis\": step_analysis,\n",
        "                \"claude_execution\": execution_result,\n",
        "                \"success\": execution_result.get(\"success\", False),\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            self.current_application[\"step_results\"].append(step_result)\n",
        "\n",
        "            if execution_result.get(\"success\", False):\n",
        "                self.current_application[\"completed_steps\"].append(current_step_num)\n",
        "                # Update current dataframe if changes were made\n",
        "                if execution_result.get(\"updated_df\") is not None:\n",
        "                    self.current_application[\"current_df\"] = execution_result[\"updated_df\"]\n",
        "\n",
        "                    # Log dataframe changes\n",
        "                    with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                        f.write(f\"📊 DATAFRAME UPDATED\\n\")\n",
        "                        f.write(f\"New Shape: {execution_result['updated_df'].shape}\\n\")\n",
        "                        f.write(f\"New Columns: {list(execution_result['updated_df'].columns)}\\n\\n\")\n",
        "            else:\n",
        "                self.current_application[\"failed_steps\"].append(current_step_num)\n",
        "\n",
        "            # Move to next step\n",
        "            self.current_application[\"current_step\"] += 1\n",
        "\n",
        "            # Enhanced step completion logging\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"{'='*40}\\n\")\n",
        "                f.write(f\"STEP {current_step_num} {'✅ SUCCESS' if execution_result.get('success') else '❌ FAILED'}\\n\")\n",
        "                f.write(f\"Completed at: {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
        "                f.write(f\"Duration: Approximately 30-60 seconds\\n\")\n",
        "                f.write(f\"Result Summary: {execution_result.get('summary', 'No summary')}\\n\")\n",
        "\n",
        "                # Add business context if available\n",
        "                if step_analysis.get('business_context'):\n",
        "                    f.write(f\"Business Context: {step_analysis.get('business_context')}\\n\")\n",
        "\n",
        "                f.write(f\"{'='*40}\\n\\n\")\n",
        "\n",
        "            # Prepare enhanced status message\n",
        "            total_steps = self.current_application[\"total_steps\"]\n",
        "            completed = len(self.current_application[\"completed_steps\"])\n",
        "            failed = len(self.current_application[\"failed_steps\"])\n",
        "\n",
        "            status_msg = f\"\"\"🔄 Step {current_step_num}/{total_steps} {'✅ Completed' if execution_result.get('success') else '❌ Failed'}\n",
        "\n",
        "            **Step Details:**\n",
        "            • Original Query: {next_step.get('user_message', 'N/A')[:100]}...\n",
        "            • Action Type: {next_step.get('action_type', 'N/A')}\n",
        "            • GPT-4.1 Analysis: {'✅ Successful' if step_analysis.get('step_adaptation', {}).get('can_execute') else '❌ Cannot Execute'}\n",
        "            • Claude Execution: {'✅ Successful' if execution_result.get('success') else '❌ Failed'}\n",
        "\n",
        "            **AI Processing Details:**\n",
        "            • Column Mapping: {len(step_analysis.get('step_adaptation', {}).get('column_mapping', {}))} columns mapped\n",
        "            • Code Generated: {'Yes' if execution_result.get('executed_code') else 'No'}\n",
        "            • Dataframe Updated: {'Yes' if execution_result.get('updated_df') is not None else 'No'}\n",
        "\n",
        "            **Progress:**\n",
        "            • Completed: {completed}/{total_steps}\n",
        "            • Failed: {failed}/{total_steps}\n",
        "            • Remaining: {total_steps - current_step_num}\n",
        "\n",
        "            📝 Detailed logs saved to: {os.path.basename(self.current_application[\"log_file\"])}\n",
        "\n",
        "            {'🎉 All steps completed!' if current_step_num >= total_steps else '⏭️ Ready for next step'}\"\"\"\n",
        "\n",
        "            return status_msg\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"❌ Error executing step {current_step_num}: {str(e)}\"\n",
        "\n",
        "            # Enhanced error logging\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"{'='*40}\\n\")\n",
        "                f.write(f\"❌ CRITICAL ERROR in step {current_step_num}\\n\")\n",
        "                f.write(f\"{'='*40}\\n\")\n",
        "                f.write(f\"Error Time: {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
        "                f.write(f\"Error Message: {str(e)}\\n\")\n",
        "                f.write(f\"Error Type: {type(e).__name__}\\n\")\n",
        "                f.write(f\"Stack Trace:\\n\")\n",
        "                f.write(f\"{traceback.format_exc()}\\n\")\n",
        "                f.write(f\"{'='*40}\\n\\n\")\n",
        "\n",
        "            self.current_application[\"failed_steps\"].append(current_step_num)\n",
        "            self.current_application[\"current_step\"] += 1\n",
        "\n",
        "            return error_msg\n",
        "\n",
        "    def _analyze_step_with_gpt4(self, step_data, step_number):\n",
        "        \"\"\"Enhanced GPT-4.1 analysis with detailed logging\"\"\"\n",
        "\n",
        "        client = OpenAI(api_key=DEFAULT_OPENAI_API_KEY)\n",
        "\n",
        "        # Prepare enhanced context for GPT-4\n",
        "        template_context = f\"\"\"\n",
        "        CRE RENT ROLL TEMPLATE APPLICATION CONTEXT:\n",
        "        ==========================================\n",
        "\n",
        "        Current Step: {step_number}/{self.current_application['total_steps']}\n",
        "        Template Name: {self.current_application['template_data'].get('template_name', 'Unknown')}\n",
        "        Session ID: {self.current_application['session_id']}\n",
        "\n",
        "        ORIGINAL TEMPLATE DATAFRAMES:\n",
        "        - Starting Template DF Shape: {self.current_application['starting_template_df'].shape}\n",
        "        - Starting Template Columns: {list(self.current_application['starting_template_df'].columns)}\n",
        "        - Starting Template Sample Data:\n",
        "        {self.current_application['starting_template_df'].head(2).to_string()}\n",
        "\n",
        "        - Final Template DF Shape: {self.current_application['final_template_df'].shape}\n",
        "        - Final Template Columns: {list(self.current_application['final_template_df'].columns)}\n",
        "\n",
        "        NEW CRE RENT ROLL TO PROCESS:\n",
        "        - Current DF Shape: {self.current_application['current_df'].shape}\n",
        "        - Current DF Columns: {list(self.current_application['current_df'].columns)}\n",
        "        - Current DF Sample Data:\n",
        "        {self.current_application['current_df'].head(3).to_string()}\n",
        "\n",
        "        ORIGINAL STEP FROM TEMPLATE:\n",
        "        - User Query: {step_data.get('user_message', 'N/A')}\n",
        "        - Action Type: {step_data.get('action_type', 'N/A')}\n",
        "        - Original Code: {step_data.get('code_executed', 'N/A')}\n",
        "        - AI Response Preview: {step_data.get('ai_response', 'N/A')[:300]}...\n",
        "\n",
        "        PREVIOUS COMPLETED STEPS IN THIS APPLICATION:\n",
        "        {[f\"Step {i}: Success\" for i in self.current_application['completed_steps']]}\n",
        "\n",
        "        PREVIOUS FAILED STEPS:\n",
        "        {[f\"Step {i}: Failed\" for i in self.current_application['failed_steps']]}\n",
        "\n",
        "        TEMPLATE GPT-4 ANALYSIS (Original Workflow Analysis):\n",
        "        {json.dumps(self.current_application['template_data'].get('gpt4_analysis', {}), indent=2)[:1500]}...\n",
        "        \"\"\"\n",
        "\n",
        "        # Enhanced analysis prompt with more specific instructions\n",
        "        analysis_prompt = f\"\"\"\n",
        "        You are an expert at adapting commercial real estate (CRE) rent roll analysis workflows to new datasets.\n",
        "\n",
        "        Your task is to analyze the original template step and adapt it for the new CRE rent roll data, considering:\n",
        "        1. Column name differences between template and new data\n",
        "        2. Data value variations (different property types, tenant structures)\n",
        "        3. Business logic preservation for CRE analysis\n",
        "        4. Code adaptation requirements\n",
        "\n",
        "        {template_context}\n",
        "\n",
        "        Please provide a comprehensive analysis in JSON format:\n",
        "        {{\n",
        "            \"step_adaptation\": {{\n",
        "                \"can_execute\": true/false,\n",
        "                \"reason\": \"Detailed explanation of why this step can or cannot be executed\",\n",
        "                \"column_mapping\": {{\"template_column\": \"new_data_column\"}},\n",
        "                \"parameter_adjustments\": [\"Specific parameter changes needed\"],\n",
        "                \"prerequisites\": [\"What must be true before this step\"],\n",
        "                \"data_compatibility\": \"assessment of data compatibility\",\n",
        "                \"business_logic_changes\": [\"Any changes to business rules needed\"]\n",
        "            }},\n",
        "            \"claude_prompt\": \"Detailed, specific prompt for Claude 3.7 to execute this adapted CRE analysis step\",\n",
        "            \"expected_outcome\": \"Detailed description of what this step should accomplish\",\n",
        "            \"validation_criteria\": [\"Specific ways to verify the step succeeded\"],\n",
        "            \"business_context\": \"Why this step is important for CRE rent roll analysis\",\n",
        "            \"risk_assessment\": \"Any potential risks or issues with this adaptation\",\n",
        "            \"fallback_strategy\": \"Alternative approach if main execution fails\"\n",
        "        }}\n",
        "\n",
        "        CRITICAL: Focus on CRE-specific considerations:\n",
        "        1. Mapping tenant information, lease dates, rent amounts, square footage columns\n",
        "        2. Adapting rent calculations, occupancy analysis, lease expiration tracking\n",
        "        3. Handling vacant spaces, percentage rent, CAM charges appropriately\n",
        "        4. Ensuring financial calculations remain accurate for CRE analysis\n",
        "        5. Creating clear, executable instructions that Claude can follow precisely\n",
        "        \"\"\"\n",
        "\n",
        "        # Log the GPT-4 prompt being sent\n",
        "        with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"📤 SENDING TO GPT-4.1\\n\")\n",
        "            f.write(f\"{'-'*40}\\n\")\n",
        "            f.write(f\"Prompt Length: {len(analysis_prompt)} characters\\n\")\n",
        "            f.write(f\"Context Length: {len(template_context)} characters\\n\")\n",
        "            f.write(f\"Model: gpt-4o\\n\")\n",
        "            f.write(f\"Temperature: 0.2\\n\")\n",
        "            f.write(f\"Max Tokens: 3000\\n\")\n",
        "            f.write(f\"\\nFull Prompt Sent to GPT-4.1:\\n\")\n",
        "            f.write(f\"{'─'*60}\\n\")\n",
        "            f.write(f\"{analysis_prompt}\\n\")\n",
        "            f.write(f\"{'─'*60}\\n\\n\")\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4.1\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert commercial real estate data analyst who adapts CRE rent roll processing workflows to new datasets. Provide detailed, actionable analysis in JSON format that considers CRE-specific data patterns and business requirements.\"},\n",
        "                    {\"role\": \"user\", \"content\": analysis_prompt}\n",
        "                ],\n",
        "                max_tokens=3000,\n",
        "                temperature=0.2\n",
        "            )\n",
        "\n",
        "            gpt_response = response.choices[0].message.content\n",
        "\n",
        "            # Log the complete GPT-4 response\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"📥 RECEIVED FROM GPT-4.1\\n\")\n",
        "                f.write(f\"{'-'*40}\\n\")\n",
        "                f.write(f\"Response Length: {len(gpt_response)} characters\\n\")\n",
        "                f.write(f\"Tokens Used: ~{len(gpt_response.split())}\\n\")\n",
        "                f.write(f\"\\nComplete GPT-4.1 Response:\\n\")\n",
        "                f.write(f\"{'─'*60}\\n\")\n",
        "                f.write(f\"{gpt_response}\\n\")\n",
        "                f.write(f\"{'─'*60}\\n\\n\")\n",
        "\n",
        "            # Try to extract JSON\n",
        "            json_match = re.search(r'{.*}', gpt_response, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    parsed_analysis = json.loads(json_match.group(0))\n",
        "\n",
        "                    # Log successful JSON parsing\n",
        "                    with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                        f.write(f\"✅ JSON PARSING SUCCESSFUL\\n\")\n",
        "                        f.write(f\"Parsed JSON Keys: {list(parsed_analysis.keys())}\\n\\n\")\n",
        "\n",
        "                    return parsed_analysis\n",
        "                except Exception as json_error:\n",
        "                    # Log JSON parsing failure\n",
        "                    with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                        f.write(f\"❌ JSON PARSING FAILED\\n\")\n",
        "                        f.write(f\"JSON Error: {str(json_error)}\\n\")\n",
        "                        f.write(f\"Extracted JSON Text:\\n{json_match.group(0)}\\n\\n\")\n",
        "\n",
        "                    return {\"analysis\": gpt_response, \"error\": f\"JSON parsing failed: {str(json_error)}\"}\n",
        "            else:\n",
        "                # Log no JSON found\n",
        "                with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                    f.write(f\"❌ NO JSON FOUND in GPT-4.1 response\\n\\n\")\n",
        "\n",
        "                return {\"analysis\": gpt_response, \"error\": \"No JSON found in response\"}\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log API call failure\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"❌ GPT-4.1 API CALL FAILED\\n\")\n",
        "                f.write(f\"Error: {str(e)}\\n\")\n",
        "                f.write(f\"Error Type: {type(e).__name__}\\n\\n\")\n",
        "\n",
        "            return {\"error\": str(e), \"fallback\": \"GPT-4 analysis failed\"}\n",
        "    def _load_latest_dataframe_file(self, dataframes_dir, step_number):\n",
        "        \"\"\"Load the latest dataframe from saved files\"\"\"\n",
        "\n",
        "        # Look for step files\n",
        "        pattern = os.path.join(dataframes_dir, \"step_*_dataframe.csv\")\n",
        "        dataframe_files = glob.glob(pattern)\n",
        "\n",
        "        if not dataframe_files:\n",
        "            # No previous files, use original and save it\n",
        "            original_df = self.current_application[\"current_df\"]\n",
        "            original_file = os.path.join(dataframes_dir, \"step_0_original_dataframe.csv\")\n",
        "            original_df.to_csv(original_file, index=False)\n",
        "\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"📂 No previous files - saved original as: {original_file}\\n\")\n",
        "\n",
        "            return original_df\n",
        "\n",
        "        # Find the latest file by step number\n",
        "        latest_step = -1\n",
        "        latest_file = None\n",
        "\n",
        "        for file_path in dataframe_files:\n",
        "            filename = os.path.basename(file_path)\n",
        "            match = re.search(r'step_(\\d+)_', filename)\n",
        "            if match:\n",
        "                step_num = int(match.group(1))\n",
        "                if step_num > latest_step:\n",
        "                    latest_step = step_num\n",
        "                    latest_file = file_path\n",
        "\n",
        "        if latest_file is None:\n",
        "            # Fallback to original\n",
        "            return self.current_application[\"current_df\"]\n",
        "\n",
        "        # Load the latest dataframe\n",
        "        latest_df = pd.read_csv(latest_file)\n",
        "\n",
        "        with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"📂 Loaded latest dataframe from step {latest_step}: {latest_file}\\n\")\n",
        "            f.write(f\"Shape: {latest_df.shape}\\n\")\n",
        "\n",
        "        return latest_df\n",
        "    def _save_dataframe_file(self, dataframe, dataframes_dir, step_number, status):\n",
        "        \"\"\"Save dataframe to file after step execution\"\"\"\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"step_{step_number}_{status}_{timestamp}_dataframe.csv\"\n",
        "        file_path = os.path.join(dataframes_dir, filename)\n",
        "\n",
        "        # Save CSV\n",
        "        dataframe.to_csv(file_path, index=False)\n",
        "\n",
        "        # Also save Excel for verification\n",
        "        excel_path = file_path.replace('.csv', '.xlsx')\n",
        "        dataframe.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "        with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"💾 DATAFRAME SAVED:\\n\")\n",
        "            f.write(f\"CSV: {file_path}\\n\")\n",
        "            f.write(f\"Excel: {excel_path}\\n\")\n",
        "            f.write(f\"Shape: {dataframe.shape}\\n\")\n",
        "            f.write(f\"Size: {os.path.getsize(file_path)} bytes\\n\")\n",
        "\n",
        "        return file_path\n",
        "    def _finalize_application(self):\n",
        "        \"\"\"Simple finalization - load latest file and save final result\"\"\"\n",
        "        if not self.current_application:\n",
        "            return \"No active session to finalize\"\n",
        "\n",
        "        session_id = self.current_application[\"session_id\"]\n",
        "        total_steps = self.current_application[\"total_steps\"]\n",
        "        completed = len(self.current_application[\"completed_steps\"])\n",
        "        failed = len(self.current_application[\"failed_steps\"])\n",
        "\n",
        "        # Load the very latest dataframe file\n",
        "        dataframes_dir = os.path.join(self.application_sessions_dir, f\"{session_id}_dataframes\")\n",
        "        final_df = self._load_latest_dataframe_file(dataframes_dir, total_steps + 1)\n",
        "\n",
        "        # Save final result\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        final_csv = os.path.join(self.application_sessions_dir, f\"{session_id}_FINAL_RESULT_{timestamp}.csv\")\n",
        "        final_excel = os.path.join(self.application_sessions_dir, f\"{session_id}_FINAL_RESULT_{timestamp}.xlsx\")\n",
        "\n",
        "        final_df.to_csv(final_csv, index=False)\n",
        "        final_df.to_excel(final_excel, index=False, engine='openpyxl')\n",
        "\n",
        "        # Log final summary\n",
        "        with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"\\n{'='*60}\\n\")\n",
        "            f.write(\"🎉 TEMPLATE APPLICATION COMPLETED\\n\")\n",
        "            f.write(f\"{'='*60}\\n\")\n",
        "            f.write(f\"Total Steps: {total_steps}\\n\")\n",
        "            f.write(f\"Completed Successfully: {completed}\\n\")\n",
        "            f.write(f\"Failed: {failed}\\n\")\n",
        "            f.write(f\"Success Rate: {(completed/total_steps)*100:.1f}%\\n\")\n",
        "            f.write(f\"Final DataFrame Shape: {final_df.shape}\\n\")\n",
        "            f.write(f\"Final Result CSV: {final_csv}\\n\")\n",
        "            f.write(f\"Final Result Excel: {final_excel}\\n\")\n",
        "            f.write(f\"All Step Files: {dataframes_dir}\\n\")\n",
        "            f.write(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"{'='*60}\\n\")\n",
        "\n",
        "        self.current_application = None\n",
        "\n",
        "        return f\"\"\"🎉 Template Application Completed!\n",
        "        📊 Results:\n",
        "        • Total Steps: {total_steps}\n",
        "        • Successfully Completed: {completed}\n",
        "        • Failed: {failed}\n",
        "        • Success Rate: {(completed/total_steps)*100:.1f}%\n",
        "\n",
        "        📁 Final Output:\n",
        "        • CSV: {final_csv}\n",
        "        • Excel: {final_excel}\n",
        "        • Shape: {final_df.shape}\n",
        "\n",
        "        📂 All Step Files: {dataframes_dir}\n",
        "\n",
        "        ✅ File-based persistence ensures no data loss!\"\"\"\n",
        "\n",
        "    def _execute_step_with_claude(self, step_analysis, step_number):\n",
        "        \"\"\"Simple file-based dataframe persistence - save after every step\"\"\"\n",
        "\n",
        "        if not step_analysis.get(\"step_adaptation\", {}).get(\"can_execute\", False):\n",
        "            failure_reason = step_analysis.get(\"step_adaptation\", {}).get(\"reason\", \"Unknown reason\")\n",
        "\n",
        "            # Log the skip reason\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"⏭️ STEP {step_number} SKIPPED\\n\")\n",
        "                f.write(f\"Reason: {failure_reason}\\n\\n\")\n",
        "\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"summary\": f\"Step {step_number} skipped: {failure_reason}\",\n",
        "                \"error\": \"Step cannot be executed\",\n",
        "                \"claude_response\": \"Step was not executed due to GPT-4.1 analysis\"\n",
        "            }\n",
        "\n",
        "        claude_client = Anthropic(api_key=DEFAULT_ANTHROPIC_API_KEY)\n",
        "\n",
        "        # Step 1: Load the latest dataframe from file\n",
        "        session_id = self.current_application[\"session_id\"]\n",
        "        dataframes_dir = os.path.join(self.application_sessions_dir, f\"{session_id}_dataframes\")\n",
        "        os.makedirs(dataframes_dir, exist_ok=True)\n",
        "\n",
        "        latest_df = self._load_latest_dataframe_file(dataframes_dir, step_number)\n",
        "\n",
        "        # Step 2: Prepare Claude prompt\n",
        "        claude_prompt = step_analysis.get(\"claude_prompt\", \"Execute the data processing step\")\n",
        "\n",
        "        df_context = f\"\"\"\n",
        "        CURRENT DATAFRAME STATUS:\n",
        "        ========================\n",
        "        Shape: {latest_df.shape}\n",
        "        Columns: {list(latest_df.columns)}\n",
        "        Data Types: {dict(latest_df.dtypes.astype(str))}\n",
        "\n",
        "        Sample Data (First 3 rows):\n",
        "        {latest_df.head(3).to_string()}\n",
        "\n",
        "        The dataframe is already loaded as 'df' variable.\n",
        "        \"\"\"\n",
        "\n",
        "        full_claude_prompt = f\"\"\"\n",
        "        {claude_prompt}\n",
        "\n",
        "        {df_context}\n",
        "\n",
        "        EXECUTION INSTRUCTIONS:\n",
        "        1. The dataframe 'df' is already loaded and available for use\n",
        "        2. Execute the required data processing step as analyzed by GPT-4.1\n",
        "        3. Show your work step by step with clear explanations\n",
        "        4. Use proper error handling but don't suppress errors completely\n",
        "        5. Display results clearly and comprehensively\n",
        "        6. Include print statements to show what you're doing\n",
        "\n",
        "        Expected Outcome: {step_analysis.get('expected_outcome', 'Process the data as required')}\n",
        "\n",
        "        Validation Criteria:\n",
        "        {chr(10).join([f\"• {criteria}\" for criteria in step_analysis.get('validation_criteria', [])])}\n",
        "\n",
        "        Business Context: {step_analysis.get('business_context', 'CRE rent roll processing')}\n",
        "\n",
        "        Please provide your code in ```python``` blocks and explain your approach clearly.\n",
        "        \"\"\"\n",
        "\n",
        "        # Log prompt\n",
        "        with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"📤 SENDING TO CLAUDE 3.7 - STEP {step_number}\\n\")\n",
        "            f.write(f\"Input DataFrame Shape: {latest_df.shape}\\n\")\n",
        "            f.write(f\"Prompt Length: {len(full_claude_prompt)} characters\\n\\n\")\n",
        "\n",
        "        try:\n",
        "            # Execute with Claude\n",
        "            claude_response = claude_client.messages.create(\n",
        "                model=\"claude-3-7-sonnet-20250219\",\n",
        "                messages=[{\"role\": \"user\", \"content\": full_claude_prompt}],\n",
        "                max_tokens=4000,\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            response_text = claude_response.content[0].text\n",
        "\n",
        "            # Log Claude's response\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"📥 RECEIVED FROM CLAUDE 3.7 - STEP {step_number}\\n\")\n",
        "                f.write(f\"Response Length: {len(response_text)} characters\\n\\n\")\n",
        "\n",
        "            # Extract code blocks\n",
        "            code_blocks = re.findall(r'```python\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "\n",
        "            if not code_blocks:\n",
        "                # Save current dataframe even if no code\n",
        "                self._save_dataframe_file(latest_df, dataframes_dir, step_number, \"no_code\")\n",
        "\n",
        "                with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                    f.write(f\"❌ NO CODE BLOCKS FOUND - Saved unchanged dataframe\\n\\n\")\n",
        "\n",
        "                return {\n",
        "                    \"success\": False,\n",
        "                    \"summary\": f\"Step {step_number}: No executable code generated by Claude\",\n",
        "                    \"claude_response\": response_text,\n",
        "                    \"error\": \"No code blocks found in Claude's response\"\n",
        "                }\n",
        "\n",
        "            # Step 3: Execute code\n",
        "            exec_globals = {\n",
        "                \"df\": latest_df.copy(),\n",
        "                \"pd\": pd,\n",
        "                \"np\": np,\n",
        "                \"datetime\": datetime,\n",
        "                \"os\": os\n",
        "            }\n",
        "\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"⚡ CODE EXECUTION STARTING - STEP {step_number}\\n\")\n",
        "                f.write(f\"Input DataFrame Shape: {exec_globals['df'].shape}\\n\")\n",
        "                f.write(f\"Executing {len(code_blocks)} code block(s)...\\n\\n\")\n",
        "\n",
        "            execution_success = False\n",
        "            execution_output = \"\"\n",
        "            output_buffer = io.StringIO()\n",
        "\n",
        "            try:\n",
        "                with redirect_stdout(output_buffer):\n",
        "                    for i, code_block in enumerate(code_blocks, 1):\n",
        "                        print(f\"--- Executing Code Block {i} ---\")\n",
        "                        exec(code_block, exec_globals)\n",
        "                        print(f\"--- Code Block {i} Completed ---\\n\")\n",
        "\n",
        "                execution_output = output_buffer.getvalue()\n",
        "                updated_df = exec_globals[\"df\"]\n",
        "                execution_success = True\n",
        "\n",
        "            except Exception as e:\n",
        "                execution_output = f\"Execution error: {str(e)}\\n{traceback.format_exc()}\"\n",
        "                execution_success = False\n",
        "                updated_df = latest_df  # Use original if failed\n",
        "\n",
        "                with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                    f.write(f\"❌ CODE EXECUTION FAILED - STEP {step_number}\\n\")\n",
        "                    f.write(f\"Error: {str(e)}\\n\\n\")\n",
        "\n",
        "            # Step 4: ALWAYS save dataframe after execution (success or failure)\n",
        "            status = \"success\" if execution_success else \"failed\"\n",
        "            saved_file = self._save_dataframe_file(updated_df, dataframes_dir, step_number, status)\n",
        "\n",
        "            # Update memory reference for consistency\n",
        "            self.current_application[\"current_df\"] = updated_df.copy()\n",
        "\n",
        "            # Log results\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                result_status = \"✅ SUCCESS\" if execution_success else \"❌ FAILED\"\n",
        "                f.write(f\"{result_status} - STEP {step_number}\\n\")\n",
        "                f.write(f\"Input Shape: {latest_df.shape}\\n\")\n",
        "                f.write(f\"Output Shape: {updated_df.shape}\\n\")\n",
        "                f.write(f\"💾 Saved to: {saved_file}\\n\")\n",
        "                f.write(f\"Execution Output:\\n{execution_output}\\n\\n\")\n",
        "\n",
        "            return {\n",
        "                \"success\": execution_success,\n",
        "                \"summary\": f\"Step {step_number}: {'✅ Successfully executed' if execution_success else '❌ Execution failed'}\",\n",
        "                \"claude_response\": response_text,\n",
        "                \"executed_code\": \"\\n\\n# --- Next Code Block ---\\n\\n\".join(code_blocks),\n",
        "                \"execution_output\": execution_output,\n",
        "                \"updated_df\": updated_df,\n",
        "                \"saved_file\": saved_file\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Save current dataframe even on API failure\n",
        "            self._save_dataframe_file(latest_df, dataframes_dir, step_number, \"api_failure\")\n",
        "\n",
        "            with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "                f.write(f\"❌ CLAUDE API FAILED - STEP {step_number}\\n\")\n",
        "                f.write(f\"Error: {str(e)}\\n\\n\")\n",
        "\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"summary\": f\"Step {step_number}: Claude execution failed - {str(e)}\",\n",
        "                \"error\": str(e),\n",
        "                \"claude_response\": \"Failed to get response from Claude\"\n",
        "            }\n",
        "\n",
        "\n",
        "    def _finalize_application(self):\n",
        "        \"\"\"Finalize the template application session\"\"\"\n",
        "        if not self.current_application:\n",
        "            return \"No active session to finalize\"\n",
        "\n",
        "        total_steps = self.current_application[\"total_steps\"]\n",
        "        completed = len(self.current_application[\"completed_steps\"])\n",
        "        failed = len(self.current_application[\"failed_steps\"])\n",
        "\n",
        "        # Save final results\n",
        "        final_df = self.current_application[\"current_df\"]\n",
        "        session_id = self.current_application[\"session_id\"]\n",
        "\n",
        "        # Save final dataframe\n",
        "        final_df_path = os.path.join(self.application_sessions_dir, f\"{session_id}_final_result.csv\")\n",
        "        final_df.to_csv(final_df_path, index=False)\n",
        "\n",
        "        # Write final summary to log\n",
        "        with open(self.current_application[\"log_file\"], 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"\\n{'=' * 60}\\n\")\n",
        "            f.write(\"TEMPLATE APPLICATION COMPLETED\\n\")\n",
        "            f.write(f\"{'=' * 60}\\n\")\n",
        "            f.write(f\"Total Steps: {total_steps}\\n\")\n",
        "            f.write(f\"Completed Successfully: {completed}\\n\")\n",
        "            f.write(f\"Failed: {failed}\\n\")\n",
        "            f.write(f\"Success Rate: {(completed/total_steps)*100:.1f}%\\n\")\n",
        "            f.write(f\"Final Result Saved: {final_df_path}\\n\")\n",
        "            f.write(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        success_msg = f\"\"\"🎉 Template Application Completed!\n",
        "\n",
        "        📊 Final Results:\n",
        "        • Total Steps: {total_steps}\n",
        "        • Successfully Completed: {completed}\n",
        "        • Failed: {failed}\n",
        "        • Success Rate: {(completed/total_steps)*100:.1f}%\n",
        "\n",
        "        📁 Output Files:\n",
        "        • Final Processed Data: {final_df_path}\n",
        "        • Application Log: {self.current_application[\"log_file\"]}\n",
        "\n",
        "        📈 Final Dataframe:\n",
        "        • Shape: {final_df.shape}\n",
        "        • Columns: {len(final_df.columns)}\n",
        "\n",
        "        The template has been successfully applied to your new rent roll!\"\"\"\n",
        "\n",
        "        # Reset application state\n",
        "        self.current_application = None\n",
        "\n",
        "        return success_msg\n",
        "\n",
        "    def get_application_status(self):\n",
        "        \"\"\"Get current application status\"\"\"\n",
        "        if not self.current_application:\n",
        "            return \"📴 No active template application session\"\n",
        "\n",
        "        total = self.current_application[\"total_steps\"]\n",
        "        current = self.current_application[\"current_step\"]\n",
        "        completed = len(self.current_application[\"completed_steps\"])\n",
        "        failed = len(self.current_application[\"failed_steps\"])\n",
        "\n",
        "        return f\"\"\"📋 Template Application Status\n",
        "\n",
        "        🎯 Template: {self.current_application['template_data'].get('template_name', 'Unknown')}\n",
        "        📁 Processing: {self.current_application['new_rent_roll_file']}\n",
        "\n",
        "        📊 Progress:\n",
        "        • Current Step: {current}/{total}\n",
        "        • Completed: {completed}\n",
        "        • Failed: {failed}\n",
        "        • Remaining: {total - current}\n",
        "\n",
        "        🔄 Status: {'🎉 Completed' if current >= total else '⏳ In Progress'}\"\"\"\n",
        "\n",
        "# Global template application engine\n",
        "template_app_engine = TemplateApplicationEngine()\n",
        "\n",
        "# Functions for the Template Application tab\n",
        "\n",
        "def load_template_for_application(template_id):\n",
        "    \"\"\"Load template details for application\"\"\"\n",
        "    if not template_id:\n",
        "        return \"Please enter a template ID\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        template_summary = enhanced_template_manager.get_template_summary(template_id)\n",
        "\n",
        "        # Get template steps for preview\n",
        "        template_json_path = os.path.join(\"rent_roll_templates\", f\"{template_id}.json\")\n",
        "        if os.path.exists(template_json_path):\n",
        "            with open(template_json_path, 'r') as f:\n",
        "                template_data = json.load(f)\n",
        "\n",
        "            steps_preview = \"\"\n",
        "            workflow_steps = template_data.get(\"raw_workflow_steps\", [])\n",
        "            for i, step in enumerate(workflow_steps[:5], 1):  # Show first 5 steps\n",
        "                steps_preview += f\"{i}. {step.get('user_message', 'N/A')[:80]}...\\n\"\n",
        "\n",
        "            if len(workflow_steps) > 5:\n",
        "                steps_preview += f\"... and {len(workflow_steps) - 5} more steps\\n\"\n",
        "\n",
        "            return template_summary, steps_preview, f\"✅ Template loaded: {len(workflow_steps)} steps found\"\n",
        "        else:\n",
        "            return template_summary, \"\", \"❌ Template file not found\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error loading template: {str(e)}\", \"\", \"Failed to load\"\n",
        "\n",
        "def start_template_application_session(template_id, new_rent_roll_file):\n",
        "    \"\"\"Start applying template to new rent roll\"\"\"\n",
        "    if not template_id:\n",
        "        return \"❌ Please select a template first\"\n",
        "\n",
        "    if not new_rent_roll_file:\n",
        "        return \"❌ Please upload a new rent roll file\"\n",
        "\n",
        "    try:\n",
        "        # Load the new rent roll\n",
        "        new_df = pd.read_excel(new_rent_roll_file.name)\n",
        "\n",
        "        # Start application session\n",
        "        session_id, status = template_app_engine.start_template_application(\n",
        "            template_id=template_id,\n",
        "            new_rent_roll_file=new_rent_roll_file.name,\n",
        "            new_rent_roll_df=new_df\n",
        "        )\n",
        "\n",
        "        if session_id:\n",
        "            return f\"✅ Session started: {session_id}\\n\\n{status}\\n\\n📊 New Rent Roll Info:\\n• Shape: {new_df.shape}\\n• Columns: {list(new_df.columns)}\\n\\n🎯 Ready to execute {template_app_engine.current_application['total_steps']} template steps!\"\n",
        "        else:\n",
        "            return status\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error starting application: {str(e)}\"\n",
        "\n",
        "def execute_next_template_step():\n",
        "    \"\"\"Execute the next step in template application\"\"\"\n",
        "    try:\n",
        "        result = template_app_engine.execute_next_step_with_ai()\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error executing step: {str(e)}\"\n",
        "\n",
        "def get_template_application_status():\n",
        "    \"\"\"Get current application status\"\"\"\n",
        "    return template_app_engine.get_application_status()\n",
        "\n",
        "def execute_all_remaining_steps():\n",
        "    \"\"\"Execute all remaining steps in sequence with guaranteed finalization\"\"\"\n",
        "    if not template_app_engine.current_application:\n",
        "        return \"❌ No active application session\"\n",
        "\n",
        "    print(\"🚀 Starting batch execution of all remaining steps...\")\n",
        "\n",
        "    results = []\n",
        "    step_count = 0\n",
        "    max_steps = 20  # Prevent infinite loops\n",
        "\n",
        "    initial_total_steps = template_app_engine.current_application[\"total_steps\"]\n",
        "    initial_current_step = template_app_engine.current_application[\"current_step\"]\n",
        "\n",
        "    print(f\"📊 Will execute steps {initial_current_step + 1} through {initial_total_steps}\")\n",
        "\n",
        "    # Execute all remaining steps\n",
        "    while (template_app_engine.current_application and\n",
        "           template_app_engine.current_application[\"current_step\"] < template_app_engine.current_application[\"total_steps\"] and\n",
        "           step_count < max_steps):\n",
        "\n",
        "        current_step_before = template_app_engine.current_application[\"current_step\"]\n",
        "        step_result = template_app_engine.execute_next_step_with_ai()\n",
        "        current_step_after = template_app_engine.current_application[\"current_step\"]\n",
        "\n",
        "        print(f\"🔄 Executed step {current_step_after}/{initial_total_steps}\")\n",
        "        results.append(f\"Step {current_step_after}: {step_result[:100]}...\")\n",
        "        step_count += 1\n",
        "\n",
        "        # Safety check: if step didn't advance, break to avoid infinite loop\n",
        "        if current_step_before == current_step_after:\n",
        "            print(\"⚠️ Step didn't advance, breaking loop\")\n",
        "            break\n",
        "\n",
        "    # ✅ GUARANTEED FINALIZATION: Always check if we need to finalize\n",
        "    if template_app_engine.current_application:\n",
        "        current_step = template_app_engine.current_application[\"current_step\"]\n",
        "        total_steps = template_app_engine.current_application[\"total_steps\"]\n",
        "\n",
        "        print(f\"🔍 Final check - Current step: {current_step}, Total steps: {total_steps}\")\n",
        "\n",
        "        if current_step >= total_steps:\n",
        "            print(\"✅ All steps completed, triggering finalization...\")\n",
        "            try:\n",
        "                finalization_result = template_app_engine._finalize_application()\n",
        "                results.append(f\"✅ FINALIZATION SUCCESS: {finalization_result[:200]}...\")\n",
        "                print(\"🎉 Finalization completed successfully!\")\n",
        "            except Exception as e:\n",
        "                error_msg = f\"❌ Finalization failed: {str(e)}\"\n",
        "                results.append(error_msg)\n",
        "                print(f\"💥 Finalization error: {e}\")\n",
        "        else:\n",
        "            incomplete_msg = f\"⚠️ Not all steps completed: {current_step}/{total_steps}\"\n",
        "            results.append(incomplete_msg)\n",
        "            print(incomplete_msg)\n",
        "    else:\n",
        "        results.append(\"⚠️ Application session ended unexpectedly\")\n",
        "        print(\"💥 Application session is None - may have been finalized already\")\n",
        "\n",
        "    # Create final summary\n",
        "    final_summary = \"\\n\".join(results[-4:])  # Show last 4 results\n",
        "\n",
        "    if step_count >= max_steps:\n",
        "        final_summary += f\"\\n\\n⚠️ Stopped after {max_steps} steps to prevent timeout\"\n",
        "\n",
        "    # Add file location information\n",
        "    final_summary += f\"\\n\\n📁 Files should be saved in:\"\n",
        "    final_summary += f\"\\n  • template_applications/{template_app_engine.current_application['session_id'] if template_app_engine.current_application else 'unknown'}_final_result.csv\"\n",
        "    final_summary += f\"\\n  • rent_roll_versions/rent_roll_v_*_template_result.csv\"\n",
        "\n",
        "    print(\"🏁 Batch execution completed\")\n",
        "    return final_summary\n",
        "\n",
        "# Add this new tab to your Gradio interface - place this BEFORE the run section\n",
        "\n",
        "def add_template_application_tab():\n",
        "    \"\"\"Add the Template Application tab to the Gradio interface\"\"\"\n",
        "\n",
        "    with gr.Tab(\"Apply Template\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 🎯 Apply Saved Templates to New Rent Rolls\n",
        "\n",
        "        Use your saved templates to automatically process similar rent roll files.\n",
        "        The system will adapt the template steps to work with your new data.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"#### 1. Select Template\")\n",
        "                template_id_select = gr.Textbox(\n",
        "                    label=\"Template ID\",\n",
        "                    placeholder=\"e.g., template_20250526_143022\",\n",
        "                    lines=1\n",
        "                )\n",
        "                load_template_btn = gr.Button(\"📂 Load Template\", variant=\"secondary\")\n",
        "\n",
        "                gr.Markdown(\"#### 2. Upload New Rent Roll\")\n",
        "                new_rent_roll_file = gr.File(\n",
        "                    label=\"New Rent Roll File (.xlsx, .xls)\",\n",
        "                    file_types=[\".xlsx\", \".xls\"]\n",
        "                )\n",
        "\n",
        "                start_application_btn = gr.Button(\"🚀 Start Application\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### Template Details\")\n",
        "                template_details_display = gr.HTML(label=\"Template Information\")\n",
        "\n",
        "                gr.Markdown(\"#### Workflow Steps Preview\")\n",
        "                template_steps_preview = gr.Textbox(\n",
        "                    label=\"Steps to Execute\",\n",
        "                    lines=8,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### 3. Execute Template Steps\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    execute_next_btn = gr.Button(\"▶️ Execute Next Step\", variant=\"primary\")\n",
        "                    execute_all_btn = gr.Button(\"⏭️ Execute All Steps\", variant=\"secondary\")\n",
        "                    status_btn = gr.Button(\"📊 Check Status\")\n",
        "\n",
        "                application_status = gr.Textbox(\n",
        "                    label=\"Application Status & Results\",\n",
        "                    lines=15,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"#### Progress Tracking\")\n",
        "\n",
        "                progress_info = gr.HTML(\n",
        "                    label=\"Current Progress\",\n",
        "                    value=\"<p>No active session</p>\"\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"\"\"\n",
        "                #### 💡 How It Works:\n",
        "                1. **Select Template**: Choose a saved template\n",
        "                2. **Upload File**: New rent roll to process\n",
        "                3. **Auto-Adaptation**: GPT-4.1 adapts each step\n",
        "                4. **Claude Execution**: Claude 3.7 runs the code\n",
        "                5. **Step-by-Step**: Execute one or all steps\n",
        "                6. **Results**: Get processed rent roll\n",
        "\n",
        "                #### ⚙️ AI Workflow:\n",
        "                - **GPT-4.1**: Analyzes & adapts template steps\n",
        "                - **Claude 3.7**: Generates & executes code\n",
        "                - **Auto-Mapping**: Matches columns intelligently\n",
        "                - **Error Recovery**: Handles step failures gracefully\n",
        "                \"\"\")\n",
        "\n",
        "        # Event handlers for Template Application tab\n",
        "        load_template_btn.click(\n",
        "            load_template_for_application,\n",
        "            inputs=[template_id_select],\n",
        "            outputs=[template_details_display, template_steps_preview, application_status]\n",
        "        )\n",
        "\n",
        "        start_application_btn.click(\n",
        "            start_template_application_session,\n",
        "            inputs=[template_id_select, new_rent_roll_file],\n",
        "            outputs=[application_status]\n",
        "        ).then(\n",
        "            get_template_application_status,\n",
        "            outputs=[progress_info]\n",
        "        )\n",
        "\n",
        "        execute_next_btn.click(\n",
        "            execute_next_template_step,\n",
        "            outputs=[application_status]\n",
        "        ).then(\n",
        "            get_template_application_status,\n",
        "            outputs=[progress_info]\n",
        "        )\n",
        "\n",
        "        execute_all_btn.click(\n",
        "            execute_all_remaining_steps,\n",
        "            outputs=[application_status]\n",
        "        ).then(\n",
        "            get_template_application_status,\n",
        "            outputs=[progress_info]\n",
        "        )\n",
        "\n",
        "        status_btn.click(\n",
        "            get_template_application_status,\n",
        "            outputs=[application_status]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYiRx9fvejLP",
        "outputId": "2ddb9302-0bb6-44c1-ecc4-f4468db65c76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-da48583de568>:58: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Agentic Rent Roll Analysis Chat\", height=500, type=\"tuples\")\n"
          ]
        }
      ],
      "source": [
        "# Initialize the global agent state\n",
        "agent_state = None\n",
        "custom_css = \"\"\"\n",
        ".chatbot-container .message-wrap .message.bot pre {\n",
        "    white-space: pre !important;\n",
        "    overflow-x: auto !important;\n",
        "    max-width: 100% !important;\n",
        "}\n",
        ".chatbot-container .message-wrap .message.bot code {\n",
        "    white-space: pre !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\"), css=custom_css) as demo:\n",
        "    gr.Markdown(\"# Agentic Commercial Real Estate Rent Roll Analyzer\")\n",
        "    gr.Markdown(\"## Hybrid AI System: GPT-4 for Decision Making & Claude for Code Generation\")\n",
        "\n",
        "    with gr.Tab(\"Setup\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                file_input = gr.File(label=\"Upload Rent Roll Excel File (.xlsx, .xls)\")\n",
        "\n",
        "                # Add separate API key inputs for OpenAI and Anthropic\n",
        "                anthropic_api_key = gr.Textbox(\n",
        "                    label=\"Anthropic API Key (Optional - for code generation)\",\n",
        "                    placeholder=\"Leave blank to use the default API key\",\n",
        "                    type=\"password\"\n",
        "                )\n",
        "\n",
        "                openai_api_key = gr.Textbox(\n",
        "                    label=\"OpenAI API Key (Optional - for decision making and text responses)\",\n",
        "                    placeholder=\"Leave blank to use the default API key\",\n",
        "                    type=\"password\"\n",
        "                )\n",
        "\n",
        "                # Updated auto-analyze checkbox\n",
        "                auto_analyze = gr.Checkbox(\n",
        "                    label=\"Automatically analyze for issues using GPT-4\",\n",
        "                    value=True,\n",
        "                    info=\"When checked, GPT-4 will automatically identify issues in your rent roll\"\n",
        "                )\n",
        "\n",
        "                upload_button = gr.Button(\"Load Rent Roll & Start Chat\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                result = gr.Textbox(label=\"Status\")\n",
        "                preview = gr.HTML(label=\"Data Preview\")\n",
        "\n",
        "    with gr.Tab(\"Chat\"):\n",
        "        # Session management buttons\n",
        "        with gr.Row():\n",
        "            view_versions_btn = gr.Button(\"View Version History\")\n",
        "            create_template_btn = gr.Button(\"🎯 Create Template from Session\", variant=\"primary\")\n",
        "            end_session_btn = gr.Button(\"🔚 End Current Session\")\n",
        "            session_status_btn = gr.Button(\"📊 Session Status\")\n",
        "\n",
        "        data_view = gr.HTML()\n",
        "        chatbot = gr.Chatbot(label=\"Agentic Rent Roll Analysis Chat\", height=500, type=\"tuples\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                msg = gr.Textbox(label=\"Your question\", placeholder=\"Ask about the rent roll...\", lines=2)\n",
        "            with gr.Column(scale=1):\n",
        "                send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "\n",
        "        clear_btn = gr.Button(\"Clear Chat History\")\n",
        "\n",
        "        # Template creation input\n",
        "        with gr.Accordion(\"Template Creation\", open=False):\n",
        "            template_name_input = gr.Textbox(\n",
        "                label=\"Template Name (Optional)\",\n",
        "                placeholder=\"e.g., 'Monthly Rent Roll Cleanup Process'\",\n",
        "                lines=1\n",
        "            )\n",
        "            template_status = gr.Textbox(label=\"Template Creation Status\", interactive=False, lines=5)\n",
        "\n",
        "        # Set up event handlers with proper return values for Gradio chatbot\n",
        "        msg.submit(\n",
        "            chat,\n",
        "            inputs=[msg, chatbot],\n",
        "            outputs=[chatbot]\n",
        "        ).then(\n",
        "            lambda: \"\", None, msg  # Clear the message box after sending\n",
        "        )\n",
        "\n",
        "        send_btn.click(\n",
        "            chat,\n",
        "            inputs=[msg, chatbot],\n",
        "            outputs=[chatbot]\n",
        "        ).then(\n",
        "            lambda: \"\", None, msg  # Clear the message box after sending\n",
        "        )\n",
        "\n",
        "        clear_btn.click(clear_chat, None, chatbot)\n",
        "\n",
        "        # Enhanced event handlers for session management\n",
        "        view_versions_btn.click(view_dataframe_versions, None, data_view)\n",
        "\n",
        "        create_template_btn.click(\n",
        "            lambda template_name: create_template_from_current_session(template_name),\n",
        "            inputs=[template_name_input],\n",
        "            outputs=[template_status]\n",
        "        )\n",
        "\n",
        "        end_session_btn.click(\n",
        "            end_current_session,\n",
        "            outputs=[template_status]\n",
        "        )\n",
        "\n",
        "        session_status_btn.click(\n",
        "            get_current_session_status,\n",
        "            outputs=[template_status]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Edit Data\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 📝 Edit Rent Roll Data\n",
        "\n",
        "        You can directly edit cells in the table below, just like in Excel.\n",
        "        - Click on any cell to edit it\n",
        "        - Use Tab or arrow keys to navigate\n",
        "        - Changes are analyzed by GPT-4.1 and recorded in your session\n",
        "        - All changes are automatically saved to session history\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=3):\n",
        "                # Version selector\n",
        "                version_dropdown = gr.Dropdown(\n",
        "                    label=\"Select Version to Edit\",\n",
        "                    choices=get_version_choices(),\n",
        "                    value=None,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                refresh_versions_btn = gr.Button(\"🔄 Refresh Versions\", size=\"sm\")\n",
        "                with gr.Row():\n",
        "                    load_latest_btn = gr.Button(\"📂 Load Latest\", variant=\"secondary\", size=\"sm\")\n",
        "                    load_version_btn = gr.Button(\"📂 Load Selected\", variant=\"primary\", size=\"sm\")\n",
        "\n",
        "        # Status display\n",
        "        edit_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "        # The editable dataframe\n",
        "        editable_df = gr.Dataframe(\n",
        "            label=\"Editable Data (Click any cell to edit) - Changes tracked by AI\",\n",
        "            interactive=True,\n",
        "            wrap=True,\n",
        "            max_height=500,\n",
        "            column_widths=[\"100px\"] * 20,\n",
        "        )\n",
        "\n",
        "        # Save controls\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=3):\n",
        "                save_description = gr.Textbox(\n",
        "                    label=\"Description of Changes (GPT-4.1 will analyze if left blank)\",\n",
        "                    placeholder=\"e.g., 'Updated rent for units 101-105' or leave blank for AI analysis\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                save_changes_btn = gr.Button(\"💾 Save & Analyze Changes\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        save_status = gr.Textbox(label=\"Save Status & AI Analysis\", interactive=False, lines=8)\n",
        "\n",
        "        # Quick actions section\n",
        "        with gr.Accordion(\"Quick Actions\", open=False):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### Bulk Operations\n",
        "            Use these buttons for common bulk edits:\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                # Add quick action buttons here in future\n",
        "                gr.Button(\"🧹 Clean Empty Rows\", size=\"sm\", interactive=False)\n",
        "                gr.Button(\"💵 Round All Currency\", size=\"sm\", interactive=False)\n",
        "                gr.Button(\"📅 Fix Date Formats\", size=\"sm\", interactive=False)\n",
        "                gr.Button(\"🔢 Recalculate Totals\", size=\"sm\", interactive=False)\n",
        "\n",
        "        # Enhanced session tracking notice\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 🤖 AI-Powered Change Tracking\n",
        "        - **GPT-4.1 Analysis**: Every edit is analyzed for business impact\n",
        "        - **Session Recording**: All changes saved to copiloting session\n",
        "        - **Template Ready**: Manual edits become part of reusable workflows\n",
        "        - **Quality Assurance**: AI detects data quality improvements/issues\n",
        "        \"\"\")\n",
        "\n",
        "        # Event handlers for Edit Data tab with enhanced functions\n",
        "        refresh_versions_btn.click(\n",
        "            refresh_version_dropdown,\n",
        "            outputs=[version_dropdown]\n",
        "        )\n",
        "\n",
        "        load_latest_btn.click(\n",
        "            load_latest_version_for_editing_enhanced,  # ← Enhanced function\n",
        "            outputs=[editable_df, edit_status]\n",
        "        )\n",
        "\n",
        "        load_version_btn.click(\n",
        "            load_specific_version_enhanced,  # ← Enhanced function\n",
        "            inputs=[version_dropdown],\n",
        "            outputs=[editable_df, edit_status]\n",
        "        )\n",
        "\n",
        "        save_changes_btn.click(\n",
        "            save_edited_dataframe_enhanced,  # ← Enhanced function\n",
        "            inputs=[editable_df, save_description],\n",
        "            outputs=[save_status, editable_df]\n",
        "        ).then(\n",
        "            refresh_version_dropdown,  # Refresh the dropdown after saving\n",
        "            outputs=[version_dropdown]\n",
        "        )\n",
        "\n",
        "        # Instructions\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### 💡 How to Use Enhanced Edit Data:\n",
        "        1. **Load Data**: Click \"Load Latest\" or select a specific version\n",
        "        2. **Edit Cells**: Click on any cell and type to edit (just like Excel!)\n",
        "        3. **Navigate**: Use Tab, Enter, or arrow keys to move between cells\n",
        "        4. **Save Changes**: Enter a description (optional) and click \"Save & Analyze Changes\"\n",
        "        5. **AI Analysis**: GPT-4.1 will analyze your changes and provide insights\n",
        "\n",
        "        ### ⚠️ Enhanced Features:\n",
        "        - **Automatic Analysis**: AI understands what you changed and why\n",
        "        - **Business Impact**: Get insights on how changes affect rent calculations\n",
        "        - **Session Integration**: All edits become part of your copiloting history\n",
        "        - **Template Building**: Manual edits are included in reusable templates\n",
        "        - **Quality Checks**: AI warns if changes might impact data quality\n",
        "        \"\"\")\n",
        "\n",
        "    # NEW TAB: Apply Template\n",
        "    with gr.Tab(\"Apply Template\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 🎯 Apply Saved Templates to New Rent Rolls\n",
        "\n",
        "        Use your saved templates to automatically process similar rent roll files.\n",
        "        The system will adapt the template steps to work with your new data.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"#### 1. Select Template\")\n",
        "                template_id_select = gr.Textbox(\n",
        "                    label=\"Template ID\",\n",
        "                    placeholder=\"e.g., template_20250526_143022\",\n",
        "                    lines=1\n",
        "                )\n",
        "                load_template_btn = gr.Button(\"📂 Load Template\", variant=\"secondary\")\n",
        "\n",
        "                gr.Markdown(\"#### 2. Upload New Rent Roll\")\n",
        "                new_rent_roll_file = gr.File(\n",
        "                    label=\"New Rent Roll File (.xlsx, .xls)\",\n",
        "                    file_types=[\".xlsx\", \".xls\"]\n",
        "                )\n",
        "\n",
        "                start_application_btn = gr.Button(\"🚀 Start Application\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### Template Details\")\n",
        "                template_details_display = gr.HTML(label=\"Template Information\")\n",
        "\n",
        "                gr.Markdown(\"#### Workflow Steps Preview\")\n",
        "                template_steps_preview = gr.Textbox(\n",
        "                    label=\"Steps to Execute\",\n",
        "                    lines=8,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### 3. Execute Template Steps\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    execute_next_btn = gr.Button(\"▶️ Execute Next Step\", variant=\"primary\")\n",
        "                    execute_all_btn = gr.Button(\"⏭️ Execute All Steps\", variant=\"secondary\")\n",
        "                    status_btn = gr.Button(\"📊 Check Status\")\n",
        "\n",
        "                application_status = gr.Textbox(\n",
        "                    label=\"Application Status & Results\",\n",
        "                    lines=15,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"#### Progress Tracking\")\n",
        "\n",
        "                progress_info = gr.HTML(\n",
        "                    label=\"Current Progress\",\n",
        "                    value=\"<p>No active session</p>\"\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"\"\"\n",
        "                #### 💡 How It Works:\n",
        "                1. **Select Template**: Choose a saved template\n",
        "                2. **Upload File**: New rent roll to process\n",
        "                3. **Auto-Adaptation**: GPT-4.1 adapts each step\n",
        "                4. **Claude Execution**: Claude 3.7 runs the code\n",
        "                5. **Step-by-Step**: Execute one or all steps\n",
        "                6. **Results**: Get processed rent roll\n",
        "\n",
        "                #### ⚙️ AI Workflow:\n",
        "                - **GPT-4.1**: Analyzes & adapts template steps\n",
        "                - **Claude 3.7**: Generates & executes code\n",
        "                - **Auto-Mapping**: Matches columns intelligently\n",
        "                - **Error Recovery**: Handles step failures gracefully\n",
        "                \"\"\")\n",
        "\n",
        "        # Event handlers for Template Application tab\n",
        "        load_template_btn.click(\n",
        "            load_template_for_application,\n",
        "            inputs=[template_id_select],\n",
        "            outputs=[template_details_display, template_steps_preview, application_status]\n",
        "        )\n",
        "\n",
        "        start_application_btn.click(\n",
        "            start_template_application_session,\n",
        "            inputs=[template_id_select, new_rent_roll_file],\n",
        "            outputs=[application_status]\n",
        "        ).then(\n",
        "            get_template_application_status,\n",
        "            outputs=[progress_info]\n",
        "        )\n",
        "\n",
        "        execute_next_btn.click(\n",
        "            execute_next_template_step,\n",
        "            outputs=[application_status]\n",
        "        ).then(\n",
        "            get_template_application_status,\n",
        "            outputs=[progress_info]\n",
        "        )\n",
        "\n",
        "        execute_all_btn.click(\n",
        "            execute_all_remaining_steps,\n",
        "            outputs=[application_status]\n",
        "        ).then(\n",
        "            get_template_application_status,\n",
        "            outputs=[progress_info]\n",
        "        )\n",
        "\n",
        "        status_btn.click(\n",
        "            get_template_application_status,\n",
        "            outputs=[application_status]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Template Manager\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### 📋 Template Management System\n",
        "\n",
        "        Create, view, and apply reusable rent roll processing templates.\n",
        "        Templates capture your complete workflow including conversations, code, and manual edits.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### Available Templates\")\n",
        "                template_list = gr.HTML(label=\"Template List\")\n",
        "                refresh_templates_btn = gr.Button(\"🔄 Refresh Template List\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"#### Template Details\")\n",
        "                template_details = gr.HTML(label=\"Template Summary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            template_id_input = gr.Textbox(\n",
        "                label=\"Template ID\",\n",
        "                placeholder=\"e.g., template_20250526_143022\"\n",
        "            )\n",
        "            with gr.Column():\n",
        "                view_template_btn = gr.Button(\"👁️ View Template\", variant=\"secondary\")\n",
        "                delete_template_btn = gr.Button(\"🗑️ Delete Template\", variant=\"stop\")\n",
        "\n",
        "        template_action_status = gr.Textbox(label=\"Status\", interactive=False, lines=3)\n",
        "\n",
        "        # Template management event handlers\n",
        "        refresh_templates_btn.click(\n",
        "            lambda: list_available_templates(),\n",
        "            outputs=[template_list]\n",
        "        )\n",
        "\n",
        "        view_template_btn.click(\n",
        "            lambda template_id: enhanced_template_manager.get_template_summary(template_id) if template_id else \"Please enter a template ID\",\n",
        "            inputs=[template_id_input],\n",
        "            outputs=[template_details]\n",
        "        )\n",
        "\n",
        "        delete_template_btn.click(\n",
        "            lambda template_id: enhanced_template_manager.delete_template(template_id) if template_id else \"Please enter a template ID\",\n",
        "            inputs=[template_id_input],\n",
        "            outputs=[template_action_status]\n",
        "        ).then(\n",
        "            lambda: list_available_templates(),  # Refresh list after deletion\n",
        "            outputs=[template_list]\n",
        "        )\n",
        "\n",
        "    # Initially hide the chat interface\n",
        "    chatbot.visible = False\n",
        "\n",
        "    # Updated upload button event with both API keys and version dropdown\n",
        "    upload_button.click(\n",
        "        upload_rent_roll,\n",
        "        inputs=[file_input, anthropic_api_key, openai_api_key, auto_analyze],\n",
        "        outputs=[result, preview, chatbot, version_dropdown]\n",
        "    )\n",
        "\n",
        "    # Updated style and help info\n",
        "    gr.Markdown(\"\"\"\n",
        "    ## How to use this Enhanced Agentic Rent Roll Analyzer:\n",
        "\n",
        "    ### 🚀 **NEW: Template Application System**\n",
        "\n",
        "    #### **Workflow Overview:**\n",
        "    1. **Create Templates** (Chat Tab): Work through your rent roll analysis normally\n",
        "    2. **Save Templates**: Click \"Create Template from Session\" to save your workflow\n",
        "    3. **Apply Templates** (Apply Template Tab): Use saved templates on new rent roll files\n",
        "    4. **Automated Processing**: GPT-4.1 + Claude 3.7 adapt and execute each step\n",
        "\n",
        "    ### 📋 **Step-by-Step Guide:**\n",
        "\n",
        "    #### **Phase 1: Create Your First Template**\n",
        "    1. **Setup Tab**: Upload your rent roll Excel file\n",
        "    2. **Chat Tab**: Interact normally - ask questions, get analysis, make changes\n",
        "    3. **Edit Data Tab**: Make any manual edits (tracked by AI)\n",
        "    4. **Create Template**: Click \"🎯 Create Template from Session\"\n",
        "\n",
        "    #### **Phase 2: Apply Template to New Files**\n",
        "    1. **Apply Template Tab**: Select your saved template ID\n",
        "    2. **Upload New File**: Choose a similar rent roll file\n",
        "    3. **Start Application**: Click \"🚀 Start Application\"\n",
        "    4. **Execute Steps**: Run \"▶️ Execute Next Step\" or \"⏭️ Execute All Steps\"\n",
        "\n",
        "    ### 🤖 **AI Workflow in Template Application:**\n",
        "\n",
        "    **For Each Template Step:**\n",
        "    1. **GPT-4.1 Analysis**:\n",
        "       - Analyzes original template step\n",
        "       - Maps columns from template to new file\n",
        "       - Adapts parameters and business rules\n",
        "       - Creates optimized prompt for Claude\n",
        "\n",
        "    2. **Claude 3.7 Execution**:\n",
        "       - Receives adapted instructions\n",
        "       - Generates appropriate Python code\n",
        "       - Executes data processing\n",
        "       - Returns results and updates dataframe\n",
        "\n",
        "    3. **Validation & Progress**:\n",
        "       - Validates step completion\n",
        "       - Records success/failure\n",
        "       - Logs detailed results\n",
        "       - Moves to next step\n",
        "\n",
        "    ### 🔧 **Key Features:**\n",
        "\n",
        "    - **Intelligent Adaptation**: Automatically maps different column names\n",
        "    - **Business Logic Preservation**: Maintains the intent of original analysis\n",
        "    - **Error Recovery**: Handles failures gracefully and continues\n",
        "    - **Progress Tracking**: Real-time status of template application\n",
        "    - **Complete Logging**: Detailed logs of every step and decision\n",
        "\n",
        "    ### 💡 **Use Cases:**\n",
        "\n",
        "    - **Monthly Processing**: Apply same cleanup to each month's rent roll\n",
        "    - **Property Portfolios**: Use one template across multiple properties\n",
        "    - **Team Workflows**: Share proven analysis methods\n",
        "    - **Quality Assurance**: Ensure consistent processing standards\n",
        "    - **Time Savings**: Automate repetitive analysis tasks\n",
        "\n",
        "    ### ⚡ **Quick Start:**\n",
        "\n",
        "    1. Upload rent roll → Chat about analysis → Create template\n",
        "    2. Get template ID from Template Manager\n",
        "    3. Go to Apply Template → Enter template ID → Upload new file → Execute!\n",
        "\n",
        "    The system transforms your one-time analysis into reusable, intelligent automation!\n",
        "    \"\"\")\n",
        "\n",
        "# Additional helper functions for Template Manager tab (keeping existing)\n",
        "def list_available_templates():\n",
        "    \"\"\"Generate HTML list of available templates\"\"\"\n",
        "    try:\n",
        "        templates = enhanced_template_manager.list_templates()\n",
        "\n",
        "        if not templates:\n",
        "            return \"<p>No templates available yet. Create your first template by using the 'Create Template from Session' button in the Chat tab.</p>\"\n",
        "\n",
        "        html = \"<div style='max-height: 400px; overflow-y: auto;'>\"\n",
        "\n",
        "        for template in templates:\n",
        "            gpt_status = \"🤖 GPT-4 Analysis\" if template.get('gpt4_analysis_available') else \"📝 Basic Info\"\n",
        "\n",
        "            html += f\"\"\"\n",
        "            <div style='border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 8px; background-color: #f9f9f9;'>\n",
        "                <h4 style='margin: 0 0 10px 0; color: #333;'>{template['template_name']}</h4>\n",
        "                <p style='margin: 5px 0; color: #666;'><strong>ID:</strong> <code>{template['template_id']}</code></p>\n",
        "                <p style='margin: 5px 0; color: #666;'><strong>Created:</strong> {template['created_date'][:10]}</p>\n",
        "                <p style='margin: 5px 0; color: #666;'><strong>Source:</strong> {template['source_file']}</p>\n",
        "                <p style='margin: 5px 0; color: #666;'><strong>Steps:</strong> {template['steps_count']} workflow steps</p>\n",
        "                <p style='margin: 5px 0;'><span style='background-color: #e3f2fd; padding: 2px 6px; border-radius: 4px; font-size: 12px;'>{gpt_status}</span></p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        html += \"</div>\"\n",
        "        return html\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"<p>Error loading templates: {str(e)}</p>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TYFJGIGney3s",
        "outputId": "1a82e9d6-8745-4525-b958-133cc1ffcbcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://14d57cac9a50ee73f5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://14d57cac9a50ee73f5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:rent_roll_analyzer:Could not find header row with 'Current' marker. Falling back to standard loading.\n",
            "WARNING:rent_roll_analyzer:Could not find header row with 'Current' marker. Falling back to standard loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved dataframe version v_20250606_002003: Initial upload - original dataset\n",
            "  - CSV: rent_roll_versions/rent_roll_v_20250606_002003.csv\n",
            "  - Excel: rent_roll_versions/rent_roll_v_20250606_002003.xlsx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-3df7e6be88cc>:135: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  {rent_roll_df.head(5).fillna('').to_html(index=False)}\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1965: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Started session recording: session_20250606_002115\n",
            "\n",
            "==== STARTING CODE GENERATION ====\n",
            "User query: for the given table add a new column Residents that classifies the tenants as \"Occupied\" if the corresponding lease start date is not empty or \"Vacant\" if the lease start date is empty.\n",
            "Dataframe has 30 rows and 9 columns\n",
            "Sending FIRST 50 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH SAMPLE OF 50 ROWS) ====\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE ====\n",
            "You are an expert Python analyst working with a rent roll dataframe that is already loaded as the variable df. The dataframe contains ALL 30 rows of real data, as described in the provided sample (see above for structure and sample content). You do NOT need to load or preview the data; df is ready for analysis.\n",
            "\n",
            "Your task:  \n",
            "Add a new column named Residents to df, classifying each row as follows:\n",
            "- If the 'Lease Start Date' is NOT empty (i.e., not null/NaT), set 'Residents' to \"Occupied\".\n",
            "- If t...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE ====\n",
            "# Adding Residents Column to Rent Roll Dataframe\n",
            "\n",
            "## Approach\n",
            "To add the \"Residents\" column to the dataframe, I'll follow these steps:\n",
            "\n",
            "1. Check for null/NaT values in the 'Lease Start Date' column\n",
            "2. Create a new column 'Residents' with:\n",
            "   - \"Occupied\" when 'Lease Start Date' is not null/NaT\n",
            "   - \"Vacant\" when 'Lease Start Date' is null/NaT\n",
            "3. Display the updated dataframe\n",
            "4. Save the new version of the dataframe\n",
            "\n",
            "## Implementation\n",
            "\n",
            "```python\n",
            "# Add the Residents column based on Lease Start Dat...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 1 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Add the Residents column based on Lease Start Date\n",
            "df['Residents'] = df['Lease Start Date'].apply(lambda x: \"Occupied\" if pd.notna(x) else \"Vacant\")\n",
            "\n",
            "# Display the updated dataframe with all rows\n",
            "pr...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE WITH RETRIES ====\n",
            "\n",
            "Executing code...\n",
            "Execution successful! Output length: 5368 characters\n",
            "\n",
            "Rent Roll with Residents Classification\n",
            "================================================================================\n",
            "   Tenant                                             Floor   Lease Start Date...\n",
            "\n",
            "==== FINAL RESPONSE GENERATED ====\n",
            "Original response length: 9991 characters\n",
            "Retry attempts: 0\n",
            "Execution successful: True\n",
            "\n",
            "==== CODE GENERATION COMPLETE ====\n",
            "\n",
            "==== STARTING CODE GENERATION ====\n",
            "User query: use the table name v_20250606_002228 and corresponding to all of the Vacant tenant names add zero values to the rent columns of \"Annual Rent\", \"Monthly Rent\" and \"Rent PSF\".\n",
            "Dataframe has 30 rows and 10 columns\n",
            "Sending FIRST 50 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH SAMPLE OF 50 ROWS) ====\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE ====\n",
            "You are an expert Python analyst working with a rent roll dataframe that is already loaded as df and contains ALL 30 rows of real data. The dataframe structure and sample content are provided below for your reference.\n",
            "\n",
            "**Key Instructions for Analysis and Coding:**\n",
            "\n",
            "1. The dataframe df is ALREADY LOADED and contains ALL 30 rows. Do NOT say \"I need to see the data first.\"\n",
            "2. When displaying tables, ALWAYS show ALL rows (no truncation or row limits).\n",
            "3. Do NOT clean or modify the data unless specif...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE ====\n",
            "# Adding Zero Values to Rent Columns for Vacant Tenants\n",
            "\n",
            "## Approach\n",
            "I'll follow these steps to update the rent roll data:\n",
            "\n",
            "1. Load the specified version of the dataframe (v_20250606_002228)\n",
            "2. Identify rows where the 'Residents' column is 'Vacant'\n",
            "3. For these vacant rows, replace NaN values in 'Annual Rent', 'Monthly Rent', and 'Rent PSF' with 0\n",
            "4. Display the updated dataframe\n",
            "5. Save the new version of the dataframe\n",
            "\n",
            "## Implementation\n",
            "\n",
            "```python\n",
            "# Load the specified version\n",
            "import pandas as ...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 1 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Load the specified version\n",
            "import pandas as pd\n",
            "import os\n",
            "version_name = \"v_20250606_002228\"\n",
            "file_path = os.path.join(\"rent_roll_versions\", f\"rent_roll_{version_name}.csv\")\n",
            "df = pd.read_csv(file_path...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE WITH RETRIES ====\n",
            "\n",
            "Executing code...\n",
            "Execution successful! Output length: 10562 characters\n",
            "Loaded version: v_20250606_002228\n",
            "Shape: (30, 10)\n",
            "\n",
            "Original Rent Roll Data\n",
            "================================================================================\n",
            "   Tenant                                   ...\n",
            "\n",
            "==== FINAL RESPONSE GENERATED ====\n",
            "Original response length: 15792 characters\n",
            "Retry attempts: 0\n",
            "Execution successful: True\n",
            "\n",
            "==== CODE GENERATION COMPLETE ====\n",
            "\n",
            "==== STARTING CODE GENERATION ====\n",
            "User query: use the v_20250606_002503, deleted rows that are empty or rows that do not belong to any particular tenant name or rows that just summarizes above rows or calculates total values\n",
            "Dataframe has 30 rows and 10 columns\n",
            "Sending FIRST 50 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH SAMPLE OF 50 ROWS) ====\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE ====\n",
            "You are an expert Python assistant working with a rent roll dataframe that is already loaded as df and contains ALL 30 rows of real data. The dataframe structure and content are as described in the sample above. You do NOT need to load or inspect the data—df is ready for analysis.\n",
            "\n",
            "**Critical Instructions:**\n",
            "\n",
            "- The dataframe variable df is already defined and contains all 30 rows of the latest rent roll data.\n",
            "- Do NOT say \"I need to see the data first\" or attempt to reload the dataframe.\n",
            "- When ...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE ====\n",
            "# Cleaning Rent Roll Data by Removing Summary and Empty Rows\n",
            "\n",
            "## Approach\n",
            "I'll follow these steps to clean the rent roll data:\n",
            "\n",
            "1. Load the specified version of the dataframe (v_20250606_002503)\n",
            "2. Identify and remove rows that:\n",
            "   - Are completely empty (all NaN values)\n",
            "   - Contain summary information (like \"Total Monthly & Annual Base Rent\")\n",
            "   - Don't belong to specific tenants (like informational rows)\n",
            "3. Display the original and cleaned dataframes\n",
            "4. Save the new version of the dataframe\n",
            "\n",
            "...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 1 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Load the specified version\n",
            "import pandas as pd\n",
            "import os\n",
            "import numpy as np\n",
            "\n",
            "version_name = \"v_20250606_002503\"\n",
            "file_path = os.path.join(\"rent_roll_versions\", f\"rent_roll_{version_name}.csv\")\n",
            "df = p...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE WITH RETRIES ====\n",
            "\n",
            "Executing code...\n",
            "Execution successful! Output length: 5685 characters\n",
            "Loaded version: v_20250606_002503\n",
            "Original shape: (30, 10)\n",
            "\n",
            "Original Rent Roll Data\n",
            "================================================================================\n",
            "   Tenant                          ...\n",
            "\n",
            "==== FINAL RESPONSE GENERATED ====\n",
            "Original response length: 15931 characters\n",
            "Retry attempts: 0\n",
            "Execution successful: True\n",
            "\n",
            "==== CODE GENERATION COMPLETE ====\n",
            "\n",
            "==== STARTING CODE GENERATION ====\n",
            "User query: use the table v_20250606_002503 and delete several types of rows to isolate individual tenant information: rows without any tenant name or identifier is to be removed, including completely empty entries and those with null or blank tenant fields. Rows that served only to summarize preceding entries or calculate total values, such as those containing 'Total Monthly & Annual Base Rent', 'Total Building Sq ft', are also to be excised. Additionally, general explanatory notes or disclaimers, including 'The annual rent does not include the increases during the year' and 'Available Vacant Space', should be removed. Incomplete tenant records lacking essential lease information should also be deleted - specifically those missing both lease start dates and square footage data (RSF or RSF%), ensuring the remaining dataset focuses specifically on complete tenant-level details with valid lease and space information.\n",
            "Dataframe has 30 rows and 10 columns\n",
            "Sending FIRST 50 ROWS to GPT-4.1 (sample instead of full dataset)\n",
            "\n",
            "==== STEP 1: GENERATING PROMPT WITH GPT-4 (WITH SAMPLE OF 50 ROWS) ====\n",
            "\n",
            "==== GPT-4 GENERATED PROMPT FOR CLAUDE ====\n",
            "You are an expert Python assistant working with a rent roll dataframe that is already loaded as df. The dataframe contains all 30 rows of real data (not just a sample), and you have access to the following columns: ['Tenant', 'Floor', 'Lease Start Date', 'Lease Expiration', 'RSF', 'RSF%', 'Annual Rent', 'Monthly Rent', 'Rent PSF', 'Residents'].\n",
            "\n",
            "Here are your instructions for any rent roll analysis or code generation:\n",
            "\n",
            "- The dataframe df is already loaded and contains all 30 rows. Do not say \"I ...\n",
            "==== END OF PROMPT (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== STEP 2: SENDING TO CLAUDE FOR CODE GENERATION ====\n",
            "\n",
            "==== CLAUDE'S RESPONSE ====\n",
            "# Cleaning Rent Roll Data by Removing Non-Tenant and Summary Rows\n",
            "\n",
            "## Approach\n",
            "I'll follow these steps to clean the rent roll data:\n",
            "\n",
            "1. Load the specified version of the dataframe (v_20250606_002503)\n",
            "2. Create a comprehensive filtering strategy to remove:\n",
            "   - Rows without tenant names or identifiers\n",
            "   - Summary rows (like \"Total Monthly & Annual Base Rent\", \"Total Building Sq ft\")\n",
            "   - Explanatory notes (like \"The annual rent does not include...\")\n",
            "   - Incomplete tenant records lacking essenti...\n",
            "==== END OF CLAUDE RESPONSE (TRUNCATED) ====\n",
            "\n",
            "\n",
            "==== EXTRACTED 1 CODE BLOCKS ====\n",
            "\n",
            "-- Code Block 1 --\n",
            "# Load the specified version\n",
            "import pandas as pd\n",
            "import os\n",
            "import numpy as np\n",
            "\n",
            "version_name = \"v_20250606_002503\"\n",
            "file_path = os.path.join(\"rent_roll_versions\", f\"rent_roll_{version_name}.csv\")\n",
            "df = p...\n",
            "\n",
            "==== STEP 3: EXECUTING CODE WITH RETRIES ====\n",
            "\n",
            "Executing code...\n",
            "Execution successful! Output length: 8672 characters\n",
            "Loaded version: v_20250606_002503\n",
            "Original shape: (30, 10)\n",
            "\n",
            "Original Rent Roll Data\n",
            "================================================================================\n",
            "   Tenant                          ...\n",
            "\n",
            "==== FINAL RESPONSE GENERATED ====\n",
            "Original response length: 16148 characters\n",
            "Retry attempts: 0\n",
            "Execution successful: True\n",
            "\n",
            "==== CODE GENERATION COMPLETE ====\n"
          ]
        }
      ],
      "source": [
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Agentic Rent Roll Analyzer application\")\n",
        "    demo.launch(debug=True)\n",
        "    logger.info(\"Application shutdown\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2xWsOChRZ8Z"
      },
      "source": [
        "Summarize the key user-guided instructions and solutions from this rent roll copiloting session.\n",
        "\n",
        "For each step, briefly describe:\n",
        "\n",
        "What the user wanted to accomplish (without quoting them directly).\n",
        "\n",
        "How the request was addressed or solved by the copilot.\n",
        "\n",
        "Avoid excessive detail, don’t repeat the user’s exact instructions, and keep each summary concise and clear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz22BvOOezGk"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/rent_roll_versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnYLYeTzfXe-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS-CestArIDR"
      },
      "source": [
        "Copying any files generated during the session to the google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jetTVWA97W6l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\")\n",
        "\n",
        "# Define the source folders (excluding sample_data)\n",
        "folders_to_save = [\n",
        "    'copiloting_sessions',\n",
        "    'rent_roll_templates',\n",
        "    'rent_roll_versions',\n",
        "    'template_applications'\n",
        "]\n",
        "\n",
        "# Define the destination path in Google Drive\n",
        "# You can change this path to wherever you want to save in your Drive\n",
        "gdrive_destination = '/content/drive/MyDrive/CRE_AI_agent/'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(gdrive_destination, exist_ok=True)\n",
        "print(f\"Created destination directory: {gdrive_destination}\")\n",
        "\n",
        "# Copy each folder to Google Drive\n",
        "for folder in folders_to_save:\n",
        "    source_path = f'/content/{folder}'\n",
        "    destination_path = os.path.join(gdrive_destination, folder)\n",
        "\n",
        "    if os.path.exists(source_path):\n",
        "        print(f\"Copying {folder}...\")\n",
        "\n",
        "        # Remove destination folder if it already exists\n",
        "        if os.path.exists(destination_path):\n",
        "            shutil.rmtree(destination_path)\n",
        "            print(f\"  Removed existing {folder} folder\")\n",
        "\n",
        "        # Copy the folder\n",
        "        shutil.copytree(source_path, destination_path)\n",
        "        print(f\"  ✓ Successfully copied {folder} to Google Drive\")\n",
        "    else:\n",
        "        print(f\"  ⚠️  Warning: {folder} not found in /content/\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COPY OPERATION COMPLETED!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"All folders have been saved to: {gdrive_destination}\")\n",
        "print(\"\\nFolders copied:\")\n",
        "for folder in folders_to_save:\n",
        "    destination_path = os.path.join(gdrive_destination, folder)\n",
        "    if os.path.exists(destination_path):\n",
        "        print(f\"  ✓ {folder}\")\n",
        "    else:\n",
        "        print(f\"  ✗ {folder} (failed)\")\n",
        "\n",
        "# Optional: List the contents of the destination folder\n",
        "print(f\"\\nContents of {gdrive_destination}:\")\n",
        "try:\n",
        "    contents = os.listdir(gdrive_destination)\n",
        "    for item in contents:\n",
        "        print(f\"  📁 {item}\")\n",
        "except:\n",
        "    print(\"  Unable to list contents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc-75XeTtyKI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
